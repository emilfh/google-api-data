{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f24eb0a0-83c0-49cd-b1aa-8019b7f95db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script started: 2023-06-28_17:31:17\n",
      "\n",
      "\n",
      "Last date saved: 2022-12-29\n",
      "\n",
      "\n",
      "Fetching new data, starting: 2022-12-30\n",
      "up to and including: 2023-01-02\n",
      "\n",
      "\n",
      "(178, 22)\n",
      "(180, 22)\n",
      "(186, 22)\n",
      "(181, 22)\n",
      "(185, 22)\n",
      "(181, 22)\n",
      "(187, 22)\n",
      "(190, 22)\n",
      "(177, 22)\n",
      "(191, 22)\n",
      "(177, 22)\n",
      "(182, 22)\n",
      "(178, 22)\n",
      "(183, 22)\n",
      "(597, 22)\n",
      "(226, 22)\n",
      "(183, 22)\n",
      "(194, 22)\n",
      "(225, 22)\n",
      "(0, 20)\n",
      "(0, 20)\n",
      "(0, 19)\n",
      "(1, 19)\n",
      "(405, 6)\n",
      "(475, 6)\n",
      "(521, 6)\n",
      "(430, 6)\n",
      "(552, 6)\n",
      "(530, 6)\n",
      "(685, 6)\n",
      "(769, 6)\n",
      "(352, 6)\n",
      "(827, 6)\n",
      "(417, 6)\n",
      "(508, 6)\n",
      "(418, 6)\n",
      "(494, 6)\n",
      "(1445, 6)\n",
      "(1042, 6)\n",
      "(409, 6)\n",
      "(834, 6)\n",
      "(848, 6)\n",
      "(0, 6)\n",
      "(0, 6)\n",
      "(0, 5)\n",
      "(1, 5)\n",
      "(1054, 6)\n",
      "(1328, 6)\n",
      "(1359, 6)\n",
      "(1102, 6)\n",
      "(1510, 6)\n",
      "(1559, 6)\n",
      "(2170, 6)\n",
      "(2282, 6)\n",
      "(810, 6)\n",
      "(2695, 6)\n",
      "(1040, 6)\n",
      "(1261, 6)\n",
      "(1046, 6)\n",
      "(1271, 6)\n",
      "(3571, 6)\n",
      "(3278, 6)\n",
      "(1064, 6)\n",
      "(2642, 6)\n",
      "(2502, 6)\n",
      "(0, 6)\n",
      "(0, 6)\n",
      "(0, 5)\n",
      "(1, 5)\n",
      "output/csv/metrics/language/language_metrics_2022_12.csv\n",
      "output/csv/metrics/language/language_metrics_2023_1.csv\n",
      "output/csv/metrics/languageCode/languageCode_metrics_2022_12.csv\n",
      "output/csv/metrics/languageCode/languageCode_metrics_2023_1.csv\n",
      "output/csv/metrics/browser/browser_metrics_2022_12.csv\n",
      "output/csv/metrics/browser/browser_metrics_2023_1.csv\n",
      "output/csv/metrics/deviceCategory/deviceCategory_metrics_2022_12.csv\n",
      "output/csv/metrics/deviceCategory/deviceCategory_metrics_2023_1.csv\n",
      "output/csv/metrics/mobileDeviceBranding/mobileDeviceBranding_metrics_2022_12.csv\n",
      "output/csv/metrics/mobileDeviceBranding/mobileDeviceBranding_metrics_2023_1.csv\n",
      "output/csv/metrics/mobileDeviceMarketingName/mobileDeviceMarketingName_metrics_2022_12.csv\n",
      "output/csv/metrics/mobileDeviceMarketingName/mobileDeviceMarketingName_metrics_2023_1.csv\n",
      "output/csv/metrics/mobileDeviceModel/mobileDeviceModel_metrics_2022_12.csv\n",
      "output/csv/metrics/mobileDeviceModel/mobileDeviceModel_metrics_2023_1.csv\n",
      "output/csv/metrics/operatingSystemWithVersion/operatingSystemWithVersion_metrics_2022_12.csv\n",
      "output/csv/metrics/operatingSystemWithVersion/operatingSystemWithVersion_metrics_2023_1.csv\n",
      "output/csv/metrics/platform/platform_metrics_2022_12.csv\n",
      "output/csv/metrics/platform/platform_metrics_2023_1.csv\n",
      "output/csv/metrics/screenResolution/screenResolution_metrics_2022_12.csv\n",
      "output/csv/metrics/screenResolution/screenResolution_metrics_2023_1.csv\n",
      "output/csv/metrics/firstUserDefaultChannelGroup/firstUserDefaultChannelGroup_metrics_2022_12.csv\n",
      "output/csv/metrics/firstUserDefaultChannelGroup/firstUserDefaultChannelGroup_metrics_2023_1.csv\n",
      "output/csv/metrics/firstUserSource/firstUserSource_metrics_2022_12.csv\n",
      "output/csv/metrics/firstUserSource/firstUserSource_metrics_2023_1.csv\n",
      "output/csv/metrics/sessionDefaultChannelGroup/sessionDefaultChannelGroup_metrics_2022_12.csv\n",
      "output/csv/metrics/sessionDefaultChannelGroup/sessionDefaultChannelGroup_metrics_2023_1.csv\n",
      "output/csv/metrics/sessionSource/sessionSource_metrics_2022_12.csv\n",
      "output/csv/metrics/sessionSource/sessionSource_metrics_2023_1.csv\n",
      "output/csv/metrics/eventName/eventName_metrics_2022_12.csv\n",
      "output/csv/metrics/eventName/eventName_metrics_2023_1.csv\n",
      "output/csv/metrics/pagePath/pagePath_metrics_2022_12.csv\n",
      "output/csv/metrics/pagePath/pagePath_metrics_2023_1.csv\n",
      "output/csv/metrics/linkUrl/linkUrl_metrics_2022_12.csv\n",
      "output/csv/metrics/linkUrl/linkUrl_metrics_2023_1.csv\n",
      "output/csv/metrics/landingPage/landingPage_metrics_2022_12.csv\n",
      "output/csv/metrics/landingPage/landingPage_metrics_2023_1.csv\n",
      "output/csv/metrics/pageReferrer/pageReferrer_metrics_2022_12.csv\n",
      "output/csv/metrics/pageReferrer/pageReferrer_metrics_2023_1.csv\n",
      "output/csv/loyalty/language/language_loyalty_2022_12.csv\n",
      "output/csv/loyalty/language/language_loyalty_2023_1.csv\n",
      "output/csv/loyalty/languageCode/languageCode_loyalty_2022_12.csv\n",
      "output/csv/loyalty/languageCode/languageCode_loyalty_2023_1.csv\n",
      "output/csv/loyalty/browser/browser_loyalty_2022_12.csv\n",
      "output/csv/loyalty/browser/browser_loyalty_2023_1.csv\n",
      "output/csv/loyalty/deviceCategory/deviceCategory_loyalty_2022_12.csv\n",
      "output/csv/loyalty/deviceCategory/deviceCategory_loyalty_2023_1.csv\n",
      "output/csv/loyalty/mobileDeviceBranding/mobileDeviceBranding_loyalty_2022_12.csv\n",
      "output/csv/loyalty/mobileDeviceBranding/mobileDeviceBranding_loyalty_2023_1.csv\n",
      "output/csv/loyalty/mobileDeviceMarketingName/mobileDeviceMarketingName_loyalty_2022_12.csv\n",
      "output/csv/loyalty/mobileDeviceMarketingName/mobileDeviceMarketingName_loyalty_2023_1.csv\n",
      "output/csv/loyalty/mobileDeviceModel/mobileDeviceModel_loyalty_2022_12.csv\n",
      "output/csv/loyalty/mobileDeviceModel/mobileDeviceModel_loyalty_2023_1.csv\n",
      "output/csv/loyalty/operatingSystemWithVersion/operatingSystemWithVersion_loyalty_2022_12.csv\n",
      "output/csv/loyalty/operatingSystemWithVersion/operatingSystemWithVersion_loyalty_2023_1.csv\n",
      "output/csv/loyalty/platform/platform_loyalty_2022_12.csv\n",
      "output/csv/loyalty/platform/platform_loyalty_2023_1.csv\n",
      "output/csv/loyalty/screenResolution/screenResolution_loyalty_2022_12.csv\n",
      "output/csv/loyalty/screenResolution/screenResolution_loyalty_2023_1.csv\n",
      "output/csv/loyalty/firstUserDefaultChannelGroup/firstUserDefaultChannelGroup_loyalty_2022_12.csv\n",
      "output/csv/loyalty/firstUserDefaultChannelGroup/firstUserDefaultChannelGroup_loyalty_2023_1.csv\n",
      "output/csv/loyalty/firstUserSource/firstUserSource_loyalty_2022_12.csv\n",
      "output/csv/loyalty/firstUserSource/firstUserSource_loyalty_2023_1.csv\n",
      "output/csv/loyalty/sessionDefaultChannelGroup/sessionDefaultChannelGroup_loyalty_2022_12.csv\n",
      "output/csv/loyalty/sessionDefaultChannelGroup/sessionDefaultChannelGroup_loyalty_2023_1.csv\n",
      "output/csv/loyalty/sessionSource/sessionSource_loyalty_2022_12.csv\n",
      "output/csv/loyalty/sessionSource/sessionSource_loyalty_2023_1.csv\n",
      "output/csv/loyalty/eventName/eventName_loyalty_2022_12.csv\n",
      "output/csv/loyalty/eventName/eventName_loyalty_2023_1.csv\n",
      "output/csv/loyalty/pagePath/pagePath_loyalty_2022_12.csv\n",
      "output/csv/loyalty/pagePath/pagePath_loyalty_2023_1.csv\n",
      "output/csv/loyalty/linkUrl/linkUrl_loyalty_2022_12.csv\n",
      "output/csv/loyalty/linkUrl/linkUrl_loyalty_2023_1.csv\n",
      "output/csv/loyalty/landingPage/landingPage_loyalty_2022_12.csv\n",
      "output/csv/loyalty/landingPage/landingPage_loyalty_2023_1.csv\n",
      "output/csv/loyalty/pageReferrer/pageReferrer_loyalty_2022_12.csv\n",
      "output/csv/loyalty/pageReferrer/pageReferrer_loyalty_2023_1.csv\n",
      "output/csv/activity/language/language_activity_2022_12.csv\n",
      "output/csv/activity/language/language_activity_2023_1.csv\n",
      "output/csv/activity/languageCode/languageCode_activity_2022_12.csv\n",
      "output/csv/activity/languageCode/languageCode_activity_2023_1.csv\n",
      "output/csv/activity/browser/browser_activity_2022_12.csv\n",
      "output/csv/activity/browser/browser_activity_2023_1.csv\n",
      "output/csv/activity/deviceCategory/deviceCategory_activity_2022_12.csv\n",
      "output/csv/activity/deviceCategory/deviceCategory_activity_2023_1.csv\n",
      "output/csv/activity/mobileDeviceBranding/mobileDeviceBranding_activity_2022_12.csv\n",
      "output/csv/activity/mobileDeviceBranding/mobileDeviceBranding_activity_2023_1.csv\n",
      "output/csv/activity/mobileDeviceMarketingName/mobileDeviceMarketingName_activity_2022_12.csv\n",
      "output/csv/activity/mobileDeviceMarketingName/mobileDeviceMarketingName_activity_2023_1.csv\n",
      "output/csv/activity/mobileDeviceModel/mobileDeviceModel_activity_2022_12.csv\n",
      "output/csv/activity/mobileDeviceModel/mobileDeviceModel_activity_2023_1.csv\n",
      "output/csv/activity/operatingSystemWithVersion/operatingSystemWithVersion_activity_2022_12.csv\n",
      "output/csv/activity/operatingSystemWithVersion/operatingSystemWithVersion_activity_2023_1.csv\n",
      "output/csv/activity/platform/platform_activity_2022_12.csv\n",
      "output/csv/activity/platform/platform_activity_2023_1.csv\n",
      "output/csv/activity/screenResolution/screenResolution_activity_2022_12.csv\n",
      "output/csv/activity/screenResolution/screenResolution_activity_2023_1.csv\n",
      "output/csv/activity/firstUserDefaultChannelGroup/firstUserDefaultChannelGroup_activity_2022_12.csv\n",
      "output/csv/activity/firstUserDefaultChannelGroup/firstUserDefaultChannelGroup_activity_2023_1.csv\n",
      "output/csv/activity/firstUserSource/firstUserSource_activity_2022_12.csv\n",
      "output/csv/activity/firstUserSource/firstUserSource_activity_2023_1.csv\n",
      "output/csv/activity/sessionDefaultChannelGroup/sessionDefaultChannelGroup_activity_2022_12.csv\n",
      "output/csv/activity/sessionDefaultChannelGroup/sessionDefaultChannelGroup_activity_2023_1.csv\n",
      "output/csv/activity/sessionSource/sessionSource_activity_2022_12.csv\n",
      "output/csv/activity/sessionSource/sessionSource_activity_2023_1.csv\n",
      "output/csv/activity/eventName/eventName_activity_2022_12.csv\n",
      "output/csv/activity/eventName/eventName_activity_2023_1.csv\n",
      "output/csv/activity/pagePath/pagePath_activity_2022_12.csv\n",
      "output/csv/activity/pagePath/pagePath_activity_2023_1.csv\n",
      "output/csv/activity/linkUrl/linkUrl_activity_2022_12.csv\n",
      "output/csv/activity/linkUrl/linkUrl_activity_2023_1.csv\n",
      "output/csv/activity/landingPage/landingPage_activity_2022_12.csv\n",
      "output/csv/activity/landingPage/landingPage_activity_2023_1.csv\n",
      "output/csv/activity/pageReferrer/pageReferrer_activity_2022_12.csv\n",
      "output/csv/activity/pageReferrer/pageReferrer_activity_2023_1.csv\n",
      "output/csv/metrics/userAgeBracket/userAgeBracket_metrics_countryId.csv\n",
      "output/csv/loyalty/userAgeBracket/userAgeBracket_loyalty_countryId.csv\n",
      "output/csv/activity/userAgeBracket/userAgeBracket_activity_countryId.csv\n",
      "(81, 7)\n",
      "(75, 11)\n",
      "output/csv/reference/geographyInfo.csv\n",
      "output/csv/reference/dateInfo.csv\n",
      "\n",
      "\n",
      "Fetched data until 2023-01-02\n",
      "Written to .csv files\n",
      "\n",
      "\n",
      "The following License applies to gapandas4 ONLY:\n",
      "\n",
      "MIT License\n",
      "Copyright (c) 2018\n",
      "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
      "of this software and associated documentation files (the \"Software\"), to deal\n",
      "in the Software without restriction, including without limitation the rights\n",
      "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
      "copies of the Software, and to permit persons to whom the Software is\n",
      "furnished to do so, subject to the following conditions:\n",
      "The above copyright notice and this permission notice shall be included in all\n",
      "copies or substantial portions of the Software.\n",
      "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
      "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
      "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
      "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
      "SOFTWARE.\n",
      "\n",
      "\n",
      "Script finished: 2023-06-28_17:33:04\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "CPU times: user 14.1 s, sys: 665 ms, total: 14.8 s\n",
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Import Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gapandas4 as gp # See Footnote 1 bottom of page\n",
    "import os\n",
    "import datetime\n",
    "from datetime import date\n",
    "# Written functions\n",
    "\n",
    "import itertools\n",
    "import sys\n",
    "# adding Notebooksfolder to the system path\n",
    "sys.path.insert(0, '/Users/emil/miniforge3/envs/googleapi/Notebooks')\n",
    "\n",
    "import importlib\n",
    "import dates_funcs\n",
    "importlib.reload(dates_funcs)\n",
    "from dates_funcs import appendDFToCSV\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Now to print to log when program running \n",
    "nowDT = datetime.datetime.now()\n",
    "now = datetime.datetime.strftime(nowDT,'%Y-%m-%d_%H:%M:%S')\n",
    "print(\"Script started: \"+now)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Check of Dates\n",
    "f = open(\"latestDate.txt\")\n",
    "maxSavedDate = f.read()\n",
    "f.close()\n",
    "#print(\"Previously fetched data up to and including: \"+ maxSavedDate)\n",
    "maxSavedDateDT = datetime.datetime.strptime(maxSavedDate,'%Y-%m-%d').date()\n",
    "print(\"Last date saved: \"+maxSavedDate)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Todays date\n",
    "todayDT = date.today()\n",
    "today = datetime.datetime.strftime(todayDT,'%Y-%m-%d')\n",
    "\n",
    "# start_date as the next day as maxSavedDate\n",
    "start_dateDT = maxSavedDateDT + datetime.timedelta(days=1)\n",
    "start_date = datetime.datetime.strftime(start_dateDT,'%Y-%m-%d')\n",
    "print(\"Fetching new data, starting: \"+start_date)\n",
    "\n",
    "# # end_date as the same day as start_date, to get data for one days\n",
    "end_dateDT = start_dateDT + datetime.timedelta(days=3)\n",
    "#end_dateDT = start_dateDT \n",
    "end_date = datetime.datetime.strftime(end_dateDT,'%Y-%m-%d')\n",
    "print(\"up to and including: \"+end_date)\n",
    "print(\"\\n\")\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#MASTER IF\n",
    "\n",
    "if start_dateDT > maxSavedDateDT and todayDT > end_dateDT and todayDT > maxSavedDateDT:\n",
    "    \n",
    "    # START BACKUP or previous .csv files to an archive\n",
    "    # DISABLED because taking up lots of space, will enable manually once in a while instead\n",
    "    '''\n",
    "    import pathlib\n",
    "    import zipfile\n",
    "    from zipfile import ZipFile, ZIP_LZMA\n",
    "    \n",
    "    directory = pathlib.Path(\"output/\")\n",
    "\n",
    "    try:\n",
    "        with ZipFile(\"backup_date:_\"+maxSavedDate+\"_written:_\"+now+\".zip\", mode=\"w\",compression=ZIP_LZMA, allowZip64=True) as archive:\n",
    "            for file_path in directory.rglob(\"*\"):\n",
    "                archive.write(\n",
    "                    file_path,\n",
    "                    arcname=file_path.relative_to(directory)\n",
    "                )\n",
    "            print(\"Previous .csv-files backed up to: backup_until:_\"+maxSavedDate+\"_written:_\"+now+\".zip\")\n",
    "    except BadZipFile as error:\n",
    "        print(error)\n",
    "    '''\n",
    "    # END BACKUP    \n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # CREDENTIALS\n",
    "    property_id = \"298727788\"\n",
    "    credentials_json_path=\"/Users/emil/miniforge3/envs/googleapi/Notebooks/Analytics/client_secrets.json\"\n",
    "    service_account = credentials_json_path\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # LISTS DIMENSIONS AND METRICS\n",
    "    # Dimensions\n",
    "    dimensions = [\"language\",\"languageCode\",\"browser\",\"deviceCategory\",\"mobileDeviceBranding\",\n",
    "                  \"mobileDeviceMarketingName\",\"mobileDeviceModel\",\"operatingSystemWithVersion\",\n",
    "                  \"platform\",\"screenResolution\",\"firstUserDefaultChannelGroup\",\"firstUserSource\",\n",
    "                  \"sessionDefaultChannelGroup\",\"sessionSource\",\"eventName\",\"pagePath\",\"linkUrl\",\n",
    "                  \"landingPage\",\"pageReferrer\"]\n",
    "    \n",
    "    # Does not work with cityId, only countryId\n",
    "    # does not work with dateHour only date.\n",
    "    dimensionsSpecial = [\"userGender\",\"brandingInterest\"]\n",
    "    \n",
    "    # Does not work with cityId, only countryId,\n",
    "    # does not work with dateHour only date:\n",
    "    # also cant request cityId and date in same request.\n",
    "    dimensionsSpecialSpecial = [\"userAgeBracket\"]\n",
    "    \n",
    "    dateHourList = [\"dateHour\"]\n",
    "    dateList = [\"date\"]\n",
    "    cityIdList = [\"cityId\"]\n",
    "    countryIdList = [\"countryId\"]\n",
    "    \n",
    "    # Metrics with empty first place and repeat last to count 1-18\n",
    "    metricList = [\"\",\"totalUsers\",\"newUsers\",\"activeUsers\",\"userEngagementDuration\",\"scrolledUsers\",\n",
    "                  \"averageSessionDuration\",\"bounceRate\",\"engagedSessions\",\"engagementRate\",\"sessions\",\n",
    "                  \"sessionsPerUser\",\"eventCount\",\"eventCountPerUser\",\"eventsPerSession\",\"screenPageViews\",\n",
    "                  \"screenPageViewsPerSession\",\"screenPageViewsPerUser\",\"totalUsers\"]\n",
    "        \n",
    "    loyaltyList = [\"wauPerMau\",\"dauPerMau\",\"dauPerWau\"]\n",
    "        \n",
    "    activityList = [\"active28DayUsers\",\"active7DayUsers\",\"activeUsers\"]\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # REQUESTS\n",
    "    \n",
    "    ### dimensions\n",
    "    \n",
    "    #### metricList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs = [[],[],[],]\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensions)):\n",
    "        for y, z in zip([1,7,13],[0,1,2]): \n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=dateHourList[0]),\n",
    "                    gp.Dimension(name=cityIdList[0]),\n",
    "                    gp.Dimension(name=dimensions[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=metricList[y]),\n",
    "                    gp.Metric(name=metricList[y+1]),\n",
    "                    gp.Metric(name=metricList[y+2]),\n",
    "                    gp.Metric(name=metricList[y+3]),\n",
    "                    gp.Metric(name=metricList[y+4]),\n",
    "                    gp.Metric(name=metricList[y+5]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs[z].append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "            \n",
    "    #### loyaltyList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_l = []\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensions)):\n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=dateList[0]),\n",
    "                    gp.Dimension(name=cityIdList[0]),\n",
    "                    gp.Dimension(name=dimensions[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=loyaltyList[0]),\n",
    "                    gp.Metric(name=loyaltyList[1]),\n",
    "                    gp.Metric(name=loyaltyList[2]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_l.append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "        \n",
    "    #### activityList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_a = []\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensions)):\n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=dateList[0]),\n",
    "                    gp.Dimension(name=cityIdList[0]),\n",
    "                    gp.Dimension(name=dimensions[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=activityList[0]),\n",
    "                    gp.Metric(name=activityList[1]),\n",
    "                    gp.Metric(name=activityList[2]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_a.append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "    \n",
    "        \n",
    "    ### dimensionsSpecial\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_s = [[],[],[],]\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensionsSpecial)):\n",
    "        for y, z in zip([1,7,13],[0,1,2]): \n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=dateList[0]),\n",
    "                    gp.Dimension(name=countryIdList[0]),\n",
    "                    gp.Dimension(name=dimensionsSpecial[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=metricList[y]),\n",
    "                    gp.Metric(name=metricList[y+1]),\n",
    "                    gp.Metric(name=metricList[y+2]),\n",
    "                    gp.Metric(name=metricList[y+3]),\n",
    "                    gp.Metric(name=metricList[y+4]),\n",
    "                    gp.Metric(name=metricList[y+5]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_s[z].append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "            \n",
    "    #### loyaltyList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_s_l = []\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensionsSpecial)):\n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=dateList[0]),\n",
    "                    gp.Dimension(name=countryIdList[0]),\n",
    "                    gp.Dimension(name=dimensionsSpecial[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=loyaltyList[0]),\n",
    "                    gp.Metric(name=loyaltyList[1]),\n",
    "                    gp.Metric(name=loyaltyList[2]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_s_l.append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "        \n",
    "    #### activityList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_s_a = []\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensionsSpecial)):\n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=dateList[0]),\n",
    "                    gp.Dimension(name=countryIdList[0]),\n",
    "                    gp.Dimension(name=dimensionsSpecial[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=activityList[0]),\n",
    "                    gp.Metric(name=activityList[1]),\n",
    "                    gp.Metric(name=activityList[2]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_s_a.append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "    \n",
    "        \n",
    "    ### dimensionsSpecialSpecial date\n",
    "    \n",
    "    #### metricList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_ssd = [[],[],[],]\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        for y, z in zip([1,7,13],[0,1,2]): \n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=dateList[0]),\n",
    "                    gp.Dimension(name=dimensionsSpecialSpecial[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=metricList[y]),\n",
    "                    gp.Metric(name=metricList[y+1]),\n",
    "                    gp.Metric(name=metricList[y+2]),\n",
    "                    gp.Metric(name=metricList[y+3]),\n",
    "                    gp.Metric(name=metricList[y+4]),\n",
    "                    gp.Metric(name=metricList[y+5]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_ssd[z].append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "            \n",
    "    #### loyaltyList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_ssd_l = []\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=dateList[0]),\n",
    "                    gp.Dimension(name=dimensionsSpecialSpecial[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=loyaltyList[0]),\n",
    "                    gp.Metric(name=loyaltyList[1]),\n",
    "                    gp.Metric(name=loyaltyList[2]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_ssd_l.append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "        \n",
    "    #### activityList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_ssd_a = []\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=dateList[0]),\n",
    "                    gp.Dimension(name=dimensionsSpecialSpecial[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=activityList[0]),\n",
    "                    gp.Metric(name=activityList[1]),\n",
    "                    gp.Metric(name=activityList[2]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_ssd_a.append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "    \n",
    "        \n",
    "    ### dimensionsSpecialSpecial countryId\n",
    "    \n",
    "    #### metricList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_ssc = [[],[],[],]\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        for y, z in zip([1,7,13],[0,1,2]): \n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=countryIdList[0]),\n",
    "                    gp.Dimension(name=dimensionsSpecialSpecial[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=metricList[y]),\n",
    "                    gp.Metric(name=metricList[y+1]),\n",
    "                    gp.Metric(name=metricList[y+2]),\n",
    "                    gp.Metric(name=metricList[y+3]),\n",
    "                    gp.Metric(name=metricList[y+4]),\n",
    "                    gp.Metric(name=metricList[y+5]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_ssc[z].append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "            \n",
    "    #### loyaltyList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_ssc_l = []\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=countryIdList[0]),\n",
    "                    gp.Dimension(name=dimensionsSpecialSpecial[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=loyaltyList[0]),\n",
    "                    gp.Metric(name=loyaltyList[1]),\n",
    "                    gp.Metric(name=loyaltyList[2]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_ssc_l.append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "        \n",
    "    #### activityList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_ssc_a = []\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=countryIdList[0]),\n",
    "                    gp.Dimension(name=dimensionsSpecialSpecial[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=activityList[0]),\n",
    "                    gp.Metric(name=activityList[1]),\n",
    "                    gp.Metric(name=activityList[2]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_ssc_a.append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "      \n",
    "    ## Formatting\n",
    "    \n",
    "    #### metricList\n",
    "    \n",
    "    # Drop the extra Totalusers column, since it exists twice in the tables to merge\n",
    "    # since metricList contained it twice.\n",
    "    \n",
    "    for x in range(len(dimensions)):\n",
    "        for z in [2]:\n",
    "            dfs[z][x].drop(columns=['totalUsers'],inplace=True)\n",
    "    \n",
    "    for x in range(len(dimensionsSpecial)):\n",
    "        for z in [2]:\n",
    "            dfs_s[z][x].drop(columns=['totalUsers'],inplace=True)\n",
    "    \n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        for z in [2]:\n",
    "            dfs_ssd[z][x].drop(columns=['totalUsers'],inplace=True)\n",
    "    \n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        for z in [2]:\n",
    "            dfs_ssc[z][x].drop(columns=['totalUsers'],inplace=True)\n",
    "            \n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "    ## Merge\n",
    "    \n",
    "    dfs_merged = list(range(len(dimensions)))\n",
    "    \n",
    "    for x in range(len(dimensions)):\n",
    "        mergeOn = ['dateHour','cityId'] \n",
    "        mergeDim = [dimensions[x]]\n",
    "        mergeOn =  mergeOn + mergeDim\n",
    "        dfs_merged[x] = dfs[0][x].merge(dfs[1][x], how='outer', on=mergeOn)\n",
    "    \n",
    "    for n in range(len(dimensions)):\n",
    "        mergeOn = ['dateHour','cityId'] \n",
    "        mergeDim = [dimensions[n]]\n",
    "        mergeOn =  mergeOn + mergeDim\n",
    "        dfs_merged[n] = dfs_merged[n].merge(dfs[2][n], how='outer', on=mergeOn)\n",
    "    \n",
    "    \n",
    "    dfs_s_merged = list(range(len(dimensionsSpecial)))\n",
    "    \n",
    "    for x in range(len(dimensionsSpecial)):\n",
    "        mergeOn = ['date','countryId'] \n",
    "        mergeDim = [dimensionsSpecial[x]]\n",
    "        mergeOn =  mergeOn + mergeDim\n",
    "        dfs_s_merged[x] = dfs_s[0][x].merge(dfs_s[1][x], how='outer', on=mergeOn)\n",
    "    \n",
    "    for n in range(len(dimensionsSpecial)):\n",
    "        mergeOn = ['date','countryId'] \n",
    "        mergeDim = [dimensionsSpecial[n]]\n",
    "        mergeOn =  mergeOn + mergeDim\n",
    "        dfs_s_merged[n] = dfs_s_merged[n].merge(dfs_s[2][n], how='outer', on=mergeOn)\n",
    "    \n",
    "    \n",
    "    dfs_ssd_merged = list(range(len(dimensionsSpecialSpecial)))\n",
    "    \n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        mergeOn = ['date'] \n",
    "        mergeDim = [dimensionsSpecialSpecial[x]]\n",
    "        mergeOn =  mergeOn + mergeDim\n",
    "        dfs_ssd_merged[x] = dfs_ssd[0][x].merge(dfs_ssd[1][x], how='outer', on=mergeOn)\n",
    "    \n",
    "    for n in range(len(dimensionsSpecialSpecial)):\n",
    "        mergeOn = ['date'] \n",
    "        mergeDim = [dimensionsSpecialSpecial[n]]\n",
    "        mergeOn =  mergeOn + mergeDim\n",
    "        dfs_ssd_merged[n] = dfs_ssd_merged[n].merge(dfs_ssd[2][n], how='outer', on=mergeOn)\n",
    "    \n",
    "    \n",
    "    dfs_ssc_merged = list(range(len(dimensionsSpecialSpecial)))\n",
    "    \n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        mergeOn = ['countryId'] \n",
    "        mergeDim = [dimensionsSpecialSpecial[x]]\n",
    "        mergeOn =  mergeOn + mergeDim\n",
    "        dfs_ssc_merged[x] = dfs_ssc[0][x].merge(dfs_ssc[1][x], how='outer', on=mergeOn)\n",
    "    \n",
    "    for n in range(len(dimensionsSpecialSpecial)):\n",
    "        mergeOn = ['countryId'] \n",
    "        mergeDim = [dimensionsSpecialSpecial[n]]\n",
    "        mergeOn =  mergeOn + mergeDim\n",
    "        dfs_ssc_merged[n] = dfs_ssc_merged[n].merge(dfs_ssc[2][n], how='outer', on=mergeOn)\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Sort out dates and sort columns.\n",
    "        \n",
    "    #### metricList\n",
    "    \n",
    "    for x in range(len(dimensions)):\n",
    "        if not dfs_merged[x].empty:\n",
    "            dfs_merged[x] = dates_funcs.sortOut_dateHour_short(dfs_merged[x])\n",
    "    \n",
    "    for x in range(len(dimensionsSpecial)):\n",
    "        if not  dfs_s_merged[x].empty:\n",
    "            dfs_s_merged[x] = dates_funcs.sortOut_date_short(dfs_s_merged[x])\n",
    "            \n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        if not dfs_ssd_merged[x].empty:\n",
    "            dfs_ssd_merged[x] = dates_funcs.sortOut_date_short(dfs_ssd_merged[x])\n",
    "    \n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        if not dfs_ssc_merged[x].empty:\n",
    "            sortOrder = True\n",
    "            dfs_ssc_merged[x] = dfs_ssc_merged[x].sort_values(list(dfs_ssc_merged[x].columns.values), ascending=sortOrder)\n",
    "    \n",
    "    #### loyaltyList\n",
    "    \n",
    "    for x in range(len(dimensions)):\n",
    "        if not dfs_l[x].empty:\n",
    "            dfs_l[x] = dates_funcs.sortOut_date_short(dfs_l[x])\n",
    "    \n",
    "    for x in range(len(dimensionsSpecial)):\n",
    "        if not dfs_s_l[x].empty:\n",
    "            dfs_s_l[x] = dates_funcs.sortOut_date_short(dfs_s_l[x])\n",
    "            \n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        if not dfs_ssd_l[x].empty:\n",
    "            dfs_ssd_l[x] = dates_funcs.sortOut_date_short(dfs_ssd_l[x])\n",
    "\n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        if not dfs_ssc_l[x].empty:\n",
    "            sortOrder = True\n",
    "            dfs_ssc_l[x] = dfs_ssc_l[x].sort_values(list(dfs_ssc_l[x].columns.values), ascending=sortOrder)\n",
    "    \n",
    "    #### activityList\n",
    "    \n",
    "    for x in range(len(dimensions)):\n",
    "        if not dfs_a[x].empty:\n",
    "            dfs_a[x] = dates_funcs.sortOut_date_short(dfs_a[x])\n",
    "    \n",
    "    for x in range(len(dimensionsSpecial)):\n",
    "        if not dfs_s_a[x].empty:\n",
    "            dfs_s_a[x] = dates_funcs.sortOut_date_short(dfs_s_a[x])\n",
    "            \n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        if not dfs_ssd_a[x].empty:\n",
    "            dfs_ssd_a[x] = dates_funcs.sortOut_date_short(dfs_ssd_a[x])\n",
    "    \n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        if not dfs_ssc_a[x].empty:\n",
    "            sortOrder = True\n",
    "            dfs_ssc_a[x] = dfs_ssc_a[x].sort_values(list(dfs_ssc_a[x].columns.values), ascending=sortOrder)\n",
    "        \n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Add baseURL advokatfamiljforsvar.se to fullUrl\n",
    "\n",
    "    pos_landingPage = dimensions.index(\"landingPage\") \n",
    "    mainAdress = 'https://advokatfamiljforsvar.se'  \n",
    "            \n",
    "    dfs_merged[pos_landingPage]['landingPage'] = dfs_merged[pos_landingPage]['landingPage'].where(dfs_merged[pos_landingPage]['landingPage'].values == '(not set)', mainAdress + dfs_merged[pos_landingPage]['landingPage'])\n",
    "\n",
    "    dfs_l[pos_landingPage]['landingPage'] = dfs_l[pos_landingPage]['landingPage'].where(dfs_l[pos_landingPage]['landingPage'].values == '(not set)', mainAdress + dfs_l[pos_landingPage]['landingPage'])\n",
    "\n",
    "    dfs_a[pos_landingPage]['landingPage'] = dfs_a[pos_landingPage]['landingPage'].where(dfs_a[pos_landingPage]['landingPage'].values == '(not set)', mainAdress + dfs_a[pos_landingPage]['landingPage'])\n",
    "    \n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    ## Sort out NA\n",
    "    \n",
    "    #### metricList\n",
    "    \n",
    "    dfs_merged_fillNA = list(range(len(dimensions)))\n",
    "    \n",
    "    for n in range(len(dimensions)):\n",
    "        dfs_merged_fillNA[n] = dfs_merged[n].replace('', np.nan)\n",
    "        dfs_merged_fillNA[n] = dfs_merged_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_merged_fillNA[n].shape)\n",
    "    \n",
    "    \n",
    "    dfs_s_merged_fillNA = list(range(len(dimensionsSpecial)))\n",
    "    \n",
    "    for n in range(len(dimensionsSpecial)):\n",
    "        dfs_s_merged_fillNA[n] = dfs_s_merged[n].replace('', np.nan)\n",
    "        dfs_s_merged_fillNA[n] = dfs_s_merged_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_s_merged_fillNA[n].shape)\n",
    "    \n",
    "    \n",
    "    dfs_ssd_merged_fillNA = list(range(len(dimensionsSpecialSpecial)))\n",
    "    \n",
    "    for n in range(len(dimensionsSpecialSpecial)):\n",
    "        dfs_ssd_merged_fillNA[n] = dfs_ssd_merged[n].replace('', np.nan)\n",
    "        dfs_ssd_merged_fillNA[n] = dfs_ssd_merged_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_ssd_merged_fillNA[n].shape)\n",
    "    \n",
    "    \n",
    "    dfs_ssc_merged_fillNA = list(range(len(dimensionsSpecialSpecial)))\n",
    "    \n",
    "    for n in range(len(dimensionsSpecialSpecial)):\n",
    "        dfs_ssc_merged_fillNA[n] = dfs_ssc_merged[n].replace('', np.nan)\n",
    "        dfs_ssc_merged_fillNA[n] = dfs_ssc_merged_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_ssc_merged_fillNA[n].shape)\n",
    "    \n",
    "    #### loyaltyList\n",
    "    \n",
    "    dfs_l_fillNA = list(range(len(dimensions)))\n",
    "    \n",
    "    for n in range(len(dimensions)):\n",
    "        dfs_l_fillNA[n] = dfs_l[n].replace('', np.nan)\n",
    "        dfs_l_fillNA[n] = dfs_l_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_l_fillNA[n].shape)\n",
    "    \n",
    "    \n",
    "    dfs_s_l_fillNA = list(range(len(dimensionsSpecial)))\n",
    "    \n",
    "    for n in range(len(dimensionsSpecial)):\n",
    "        dfs_s_l_fillNA[n] = dfs_s_l[n].replace('', np.nan)\n",
    "        dfs_s_l_fillNA[n] = dfs_s_l_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_s_l_fillNA[n].shape)\n",
    "    \n",
    "    \n",
    "    dfs_ssd_l_fillNA = list(range(len(dimensionsSpecialSpecial)))\n",
    "    \n",
    "    for n in range(len(dimensionsSpecialSpecial)):\n",
    "        dfs_ssd_l_fillNA[n] = dfs_ssd_l[n].replace('', np.nan)\n",
    "        dfs_ssd_l_fillNA[n] = dfs_ssd_l_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_ssd_l_fillNA[n].shape)\n",
    "    \n",
    "    \n",
    "    dfs_ssc_l_fillNA = list(range(len(dimensionsSpecialSpecial)))\n",
    "    \n",
    "    for n in range(len(dimensionsSpecialSpecial)):\n",
    "        dfs_ssc_l_fillNA[n] = dfs_ssc_l[n].replace('', np.nan)\n",
    "        dfs_ssc_l_fillNA[n] = dfs_ssc_l_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_ssc_l_fillNA[n].shape)\n",
    "    \n",
    "    #### activityList\n",
    "    \n",
    "    \n",
    "    dfs_a_fillNA = list(range(len(dimensions)))\n",
    "    \n",
    "    for n in range(len(dimensions)):\n",
    "        dfs_a_fillNA[n] = dfs_a[n].replace('', np.nan)\n",
    "        dfs_a_fillNA[n] = dfs_a_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_a_fillNA[n].shape)\n",
    "    \n",
    "    \n",
    "    dfs_s_a_fillNA = list(range(len(dimensionsSpecial)))\n",
    "    \n",
    "    for n in range(len(dimensionsSpecial)):\n",
    "        dfs_s_a_fillNA[n] = dfs_s_a[n].replace('', np.nan)\n",
    "        dfs_s_a_fillNA[n] = dfs_s_a_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_s_a_fillNA[n].shape)\n",
    "    \n",
    "    \n",
    "    dfs_ssd_a_fillNA = list(range(len(dimensionsSpecialSpecial)))\n",
    "    \n",
    "    for n in range(len(dimensionsSpecialSpecial)):\n",
    "        dfs_ssd_a_fillNA[n] = dfs_ssd_a[n].replace('', np.nan)\n",
    "        dfs_ssd_a_fillNA[n] = dfs_ssd_a_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_ssd_a_fillNA[n].shape)\n",
    "    \n",
    "    \n",
    "    dfs_ssc_a_fillNA = list(range(len(dimensionsSpecialSpecial)))\n",
    "    \n",
    "    for n in range(len(dimensionsSpecialSpecial)):\n",
    "        dfs_ssc_a_fillNA[n] = dfs_ssc_a[n].replace('', np.nan)\n",
    "        dfs_ssc_a_fillNA[n] = dfs_ssc_a_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_ssc_a_fillNA[n].shape)\n",
    "    \n",
    "        \n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # SPLIT DATA BY YEAR AND THEN MONTH print to .csv, all dfs which contains dateinfo\n",
    "\n",
    "    dfs_out = pd.DataFrame()\n",
    "\n",
    "    # Empty dataframes for years\n",
    "    dfs_merged_fillNA_by_Year = pd.DataFrame()\n",
    "    dfs_s_merged_fillNA_by_Year = pd.DataFrame()\n",
    "    dfs_ssd_merged_fillNA_by_Year = pd.DataFrame()\n",
    "    #dfs_ssc_merged_fillNA_by_Year = pd.DataFrame()\n",
    "    dfs_l_fillNA_by_Year = pd.DataFrame()\n",
    "    dfs_s_l_fillNA_by_Year = pd.DataFrame()\n",
    "    dfs_ssd_l_fillNA_by_Year = pd.DataFrame()\n",
    "    #dfs_ssc_l_fillNA_by_Year = pd.DataFrame()\n",
    "    dfs_a_fillNA_by_Year = pd.DataFrame()\n",
    "    dfs_s_a_fillNA_by_Year = pd.DataFrame()\n",
    "    dfs_ssd_a_fillNA_by_Year = pd.DataFrame()\n",
    "    #dfs_ssc_a_fillNA_by_Year = pd.DataFrame()\n",
    "\n",
    "    # Empty dataframes for years\n",
    "    dfs_merged_fillNA_by_Month = pd.DataFrame()\n",
    "    dfs_s_merged_fillNA_by_Month = pd.DataFrame()\n",
    "    dfs_ssd_merged_fillNA_by_Month = pd.DataFrame()\n",
    "    #dfs_ssc_merged_fillNA_by_Month = pd.DataFrame()\n",
    "    dfs_l_fillNA_by_Month = pd.DataFrame()\n",
    "    dfs_s_l_fillNA_by_Month = pd.DataFrame()\n",
    "    dfs_ssd_l_fillNA_by_Month = pd.DataFrame()\n",
    "    #dfs_ssc_l_fillNA_by_Month = pd.DataFrame()\n",
    "    dfs_a_fillNA_by_Month = pd.DataFrame()\n",
    "    dfs_s_a_fillNA_by_Month = pd.DataFrame()\n",
    "    dfs_ssd_a_fillNA_by_Month = pd.DataFrame()\n",
    "    #dfs_ssc_a_fillNA_by_Month = pd.DataFrame()\n",
    "\n",
    "    # All lists to loop over using zip:\n",
    "    list_of_dimensions = [dimensions, dimensionsSpecial, dimensionsSpecialSpecial,\n",
    "                          dimensions, dimensionsSpecial, dimensionsSpecialSpecial,\n",
    "                          dimensions, dimensionsSpecial, dimensionsSpecialSpecial]\n",
    "\n",
    "    list_of_input = [dfs_merged_fillNA, dfs_s_merged_fillNA, dfs_ssd_merged_fillNA,\n",
    "                     dfs_l_fillNA, dfs_s_l_fillNA, dfs_ssd_l_fillNA,\n",
    "                     dfs_a_fillNA, dfs_s_a_fillNA, dfs_ssd_a_fillNA]\n",
    "\n",
    "    list_of_type = ['metrics', 'metrics', 'metrics',\n",
    "                    'loyalty', 'loyalty', 'loyalty',\n",
    "                    'activity', 'activity', 'activity']\n",
    "\n",
    "    list_of_df_for_year = [dfs_merged_fillNA_by_Year, dfs_s_merged_fillNA_by_Year, dfs_ssd_merged_fillNA_by_Year,\n",
    "                           dfs_l_fillNA_by_Year, dfs_s_l_fillNA_by_Year, dfs_ssd_l_fillNA_by_Year,\n",
    "                           dfs_a_fillNA_by_Year, dfs_s_a_fillNA_by_Year, dfs_ssd_a_fillNA_by_Year]\n",
    "\n",
    "    list_of_df_for_month = [dfs_merged_fillNA_by_Month, dfs_s_merged_fillNA_by_Month, dfs_ssd_merged_fillNA_by_Month,\n",
    "                           dfs_l_fillNA_by_Month, dfs_s_l_fillNA_by_Month, dfs_ssd_l_fillNA_by_Month, \n",
    "                           dfs_a_fillNA_by_Month, dfs_s_a_fillNA_by_Month, dfs_ssd_a_fillNA_by_Month]\n",
    "\n",
    "    pd.options.mode.chained_assignment = None \n",
    "\n",
    "\n",
    "    \n",
    "    # Looping over all data\n",
    "    for l_dim, l_input, l_type, l_year, l_month in zip(list_of_dimensions, list_of_input, list_of_type, list_of_df_for_year, list_of_df_for_month):\n",
    "\n",
    "        if l_dim and l_input and l_type:\n",
    "            \n",
    "            # Split by years\n",
    "            for n in range(len(l_dim)):\n",
    "                if not l_input[n].empty:\n",
    "                    l_year[n] = dates_funcs.split_years(l_input[n])\n",
    "        \n",
    "            # add for earch year add list to monthlist\n",
    "            for n in range(len(l_dim)):\n",
    "                if not l_input[n].empty:\n",
    "                    if not l_year[n].empty:\n",
    "                        # List of lists\n",
    "                        l_month[n] = [[] for _ in range(len(l_year[n]))]    \n",
    "        \n",
    "            # Split by month\n",
    "            for n in range(len(l_dim)):\n",
    "                for x in range(len(l_year)):\n",
    "                    if not l_input[n].empty:\n",
    "                        if not l_year[n][x].empty:\n",
    "                            l_month[n][x] = dates_funcs.split_months(l_year[n][x])\n",
    "            \n",
    "            # Flatten df_by_months[x][y] (two levels) structure to array of dataframes (one level)\n",
    "            for n in range(len(l_dim)):\n",
    "                if not l_input[n].empty:\n",
    "                    if not l_month[n].empty:\n",
    "                        dfs_out[n] = list(itertools.chain.from_iterable(l_month[n]))\n",
    "          \n",
    "\n",
    "            #print to .csv, create folder if needed\n",
    "            for n in range(len(l_dim)):\n",
    "                for x in range(len(dfs_out)):\n",
    "                #for x, y in zip(range(len(dfs_out[n])),fileNameList[n]):\n",
    "                    if not l_input[n].empty:\n",
    "                        if not dfs_out[n][x].empty:\n",
    "                            csvFilePath = \"output/csv/\"+l_type+\"/\"+l_dim[n]+\"/\"\n",
    "                            if not os.path.exists(csvFilePath):\n",
    "                                os.makedirs(csvFilePath)\n",
    "                            #dfs_out[n][x] = dfs_out[n][x].astype(str)\n",
    "                            #dfs_out[n][x]['year'].min()+'_'+dfs_out[n][x]['month'].min()\n",
    "                            fileName = l_dim[n]+\"_\"+l_type+\"_\"+dfs_out[n][x]['year'].min().astype(str)+'_'+dfs_out[n][x]['month'].min().astype(str)+\".csv\"\n",
    "                            dfs_out[n][x].drop(columns=['year'],inplace=True)\n",
    "                            dfs_out[n][x].drop(columns=['month'],inplace=True)\n",
    "                            appendDFToCSV(dfs_out[n][x], csvFilePath, fileName, ',')       \n",
    "    # End Loop\n",
    "        \n",
    "    pd.options.mode.chained_assignment = 'warn' \n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "    ## Dataframes which does not contain dateFull to .csv\n",
    "\n",
    "    '''\n",
    "    for n in range(len(dfs_merged_fillNA)):\n",
    "        if not dfs_merged_fillNA[n].empty:\n",
    "            csvFilePath = \"output/csv/metrics/\"+dimensions[n]+\"/\"\n",
    "            if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "            fileName = dimensions[n]+\"_dateHour\"+\"_cityId\"+\".csv\" \n",
    "            appendDFToCSV(dfs_merged_fillNA[n], csvFilePath, fileName, sep)\n",
    "    \n",
    "    for n in range(len(dfs_s_merged_fillNA)):\n",
    "        if not dfs_s_merged_fillNA[n].empty:\n",
    "            csvFilePath = \"output/csv/metrics/\"+dimensionsSpecial[n]+\"/\"\n",
    "            if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "            fileName = dimensionsSpecial[n]+\"_date\"+\"_countryId\"+\".csv\"\n",
    "            appendDFToCSV(dfs_s_merged_fillNA[n], csvFilePath, fileName, sep)\n",
    "    \n",
    "    for n in range(len(dfs_ssd_merged_fillNA)):\n",
    "        if not dfs_ssd_merged_fillNA[n].empty:\n",
    "            csvFilePath = \"output/csv/metrics/\"+dimensionsSpecialSpecial[n]+\"/\"\n",
    "            if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "            fileName = dimensionsSpecialSpecial[n]+\"_date\"+\".csv\"\n",
    "            appendDFToCSV(dfs_ssd_merged_fillNA[n], csvFilePath, fileName, sep)\n",
    "    '''\n",
    "    for n in range(len(dfs_ssc_merged_fillNA)):\n",
    "        if not dfs_ssc_merged_fillNA[n].empty:\n",
    "            csvFilePath = \"output/csv/metrics/\"+dimensionsSpecialSpecial[n]+\"/\"\n",
    "            if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "            fileName = dimensionsSpecialSpecial[n]+\"_metrics_countryId.csv\"\n",
    "            appendDFToCSV(dfs_ssc_merged_fillNA[n], csvFilePath, fileName, ',')\n",
    "    '''\n",
    "    for n in range(len(dfs_l_fillNA)):\n",
    "        if not dfs_l_fillNA[n].empty:\n",
    "            csvFilePath = \"output/csv/loyalty/\"+dimensions[n]+\"/\"\n",
    "            if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "            fileName = dimensions[n]+\"_dateHour\"+\"_cityId\"+\"_loyalty\"+\".csv\"\n",
    "            appendDFToCSV(dfs_l_fillNA[n], csvFilePath, fileName, sep)\n",
    "    \n",
    "    for n in range(len(dfs_s_l_fillNA)):\n",
    "        if not dfs_s_l_fillNA[n].empty:\n",
    "            csvFilePath = \"output/csv/loyalty/\"+dimensionsSpecial[n]+\"/\"\n",
    "            if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "            fileName = dimensionsSpecial[n]+\"_date\"+\"_countryId\"+\"_loyalty\"+\".csv\"\n",
    "            appendDFToCSV(dfs_s_l_fillNA[n], csvFilePath, fileName, sep)\n",
    "    \n",
    "    for n in range(len(dfs_ssd_l_fillNA)):\n",
    "        if not dfs_ssd_l_fillNA[n].empty:\n",
    "            csvFilePath = \"output/csv/loyalty/\"+dimensionsSpecialSpecial[n]+\"/\"\n",
    "            if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "            fileName = dimensionsSpecialSpecial[n]+\"_date\"+\"_loyalty\"+\".csv\"\n",
    "            appendDFToCSV(dfs_ssd_l_fillNA[n], csvFilePath, fileName, sep)\n",
    "    '''\n",
    "    for n in range(len(dfs_ssc_l_fillNA)):\n",
    "        if not dfs_ssc_l_fillNA[n].empty:\n",
    "            csvFilePath = \"output/csv/loyalty/\"+dimensionsSpecialSpecial[n]+\"/\"\n",
    "            if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "            fileName = dimensionsSpecialSpecial[n]+\"_loyalty_countryId.csv\"\n",
    "            appendDFToCSV(dfs_ssc_l_fillNA[n], csvFilePath, fileName, ',')\n",
    "    '''\n",
    "    for n in range(len(dfs_a_fillNA)):\n",
    "        if not dfs_a_fillNA[n].empty:\n",
    "            csvFilePath = \"output/csv/activity/\"+dimensions[n]+\"/\"\n",
    "            if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "            fileName = dimensions[n]+\"_dateHour\"+\"_cityId\"+\"_activity\"+\".csv\"\n",
    "            appendDFToCSV(dfs_a_fillNA[n], csvFilePath, fileName, sep)\n",
    "    \n",
    "    for n in range(len(dfs_s_a_fillNA)):\n",
    "        if not dfs_s_a_fillNA[n].empty:\n",
    "            csvFilePath = \"output/csv/activity/\"+dimensionsSpecial[n]+\"/\"\n",
    "            if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "            fileName = dimensionsSpecial[n]+\"_date\"+\"_countryId\"+\"_activity\"+\".csv\"\n",
    "            appendDFToCSV(dfs_s_a_fillNA[n], csvFilePath, fileName, sep)\n",
    "    \n",
    "    for n in range(len(dfs_ssd_a_fillNA)):\n",
    "        if not dfs_ssd_a_fillNA[n].empty:\n",
    "            csvFilePath = \"output/csv/activity/\"+dimensionsSpecialSpecial[n]+\"/\"\n",
    "            if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "            fileName = dimensionsSpecialSpecial[n]+\"_date\"+\"_activity\"+\".csv\"\n",
    "            appendDFToCSV(dfs_ssd_a_fillNA[n], csvFilePath, fileName, sep)\n",
    "    '''\n",
    "    for n in range(len(dfs_ssc_a_fillNA)):\n",
    "        if not dfs_ssc_a_fillNA[n].empty:\n",
    "            csvFilePath = \"output/csv/activity/\"+dimensionsSpecialSpecial[n]+\"/\"\n",
    "            if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "            fileName = dimensionsSpecialSpecial[n]+\"_activity_countryId.csv\"\n",
    "            appendDFToCSV(dfs_ssc_a_fillNA[n], csvFilePath, fileName, ',')\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "    ## Date and Geography tables\n",
    "    \n",
    "    # Geography List\n",
    "    geoList = ['continentId','continent','countryId','country','region','cityId','city']\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    \n",
    "    report_request = gp.RunReportRequest(\n",
    "        property=f\"properties/{property_id}\",\n",
    "         dimensions=[\n",
    "             gp.Dimension(name=geoList[0]),\n",
    "             gp.Dimension(name=geoList[1]),\n",
    "             gp.Dimension(name=geoList[2]),\n",
    "             gp.Dimension(name=geoList[3]),\n",
    "             gp.Dimension(name=geoList[4]),\n",
    "             gp.Dimension(name=geoList[5]),\n",
    "             gp.Dimension(name=geoList[6]),\n",
    "           ],\n",
    "           metrics=[\n",
    "            ],\n",
    "            date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "        )\n",
    "        \n",
    "    # Perform query and append to list\n",
    "    df_geo = gp.query(service_account, report_request, report_type=\"report\")\n",
    "\n",
    "    # Sort columns\n",
    "    sortOrder = True\n",
    "    df_geo = df_geo.sort_values(list(df_geo.columns.values), ascending=sortOrder)\n",
    "\n",
    "    \n",
    "    # Date List\n",
    "    dateList = [\"dateHour\"]\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    \n",
    "    report_request = gp.RunReportRequest(\n",
    "        property=f\"properties/{property_id}\",\n",
    "         dimensions=[\n",
    "             gp.Dimension(name=dateList[0]),\n",
    "           ],\n",
    "           metrics=[\n",
    "            ],\n",
    "            date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "        )\n",
    "    \n",
    "    # Perform query and append to list\n",
    "    df_dateHour = gp.query(service_account, report_request, report_type=\"report\")\n",
    "\n",
    "    # Sort out dates function\n",
    "    df_dateHour = dates_funcs.sortOut_dateHour(df_dateHour)\n",
    "\n",
    "    \n",
    "    df_geoTest = df_geo.replace('', np.nan)\n",
    "    df_geoTest = df_geoTest.replace('(not set)', np.nan)\n",
    "    print(df_geoTest.shape)\n",
    "    \n",
    "    df_dateHourTest = df_dateHour.replace('', np.nan)\n",
    "    df_dateHourTest = df_dateHourTest.replace('(not set)', np.nan)\n",
    "    print(df_dateHourTest.shape)\n",
    "\n",
    "\n",
    "    csvFilePath = \"output/csv/reference/\"\n",
    "    if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "    fileName = \"geographyInfo.csv\"\n",
    "    if not df_geoTest.empty:\n",
    "        appendDFToCSV(df_geoTest, csvFilePath, fileName, ',')\n",
    "    \n",
    "    csvFilePath = \"output/csv/reference/\"\n",
    "    if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "    fileName = \"dateInfo.csv\"\n",
    "    if not df_dateHourTest.empty:\n",
    "        appendDFToCSV(df_dateHourTest, csvFilePath, fileName, ',')\n",
    "\n",
    "    # -------------------------------------------\n",
    "    \n",
    "    # Read in and merge geographyInfo with geographyCountryCodes \n",
    "    pathReferenceGeography=\"/Users/emil/miniforge3/envs/googleapi/Notebooks/Analytics/output/csv/reference/\"\n",
    "    #https://github.com/stefangabos/world_countries/blob/master/data/countries/en/world.csv\n",
    "    fileNameGeoMaster = \"countryCodes.csv\"\n",
    "    fileNameGeoInfo = \"geographyInfo.csv\"\n",
    "    \n",
    "    countryCodes = pathReferenceGeography+fileNameGeoMaster\n",
    "    geoInfo = pathReferenceGeography+fileNameGeoInfo\n",
    "    \n",
    "    dfCountryCodes = pd.read_csv(countryCodes, dtype='object')\n",
    "    dfGeoInfo = pd.read_csv(geoInfo, dtype='object')\n",
    "    \n",
    "    dfCountryCodes['alpha2'] = dfCountryCodes['alpha2'].str.upper()\n",
    "    dfCountryCodes['alpha3'] = dfCountryCodes['alpha3'].str.upper()\n",
    "    \n",
    "    dfCountryCodes.rename(columns={'alpha2': 'countryId'}, inplace=True)\n",
    "    dfCountryCodes.rename(columns={'alpha3': 'countryIdLong'}, inplace=True)\n",
    "    dfCountryCodes.rename(columns={'name': 'countryRef'}, inplace=True)\n",
    "    \n",
    "    dfCountryCodes = dfCountryCodes.drop(columns=['id'])\n",
    "    \n",
    "    dfMerged = dfCountryCodes.merge(dfGeoInfo, how='outer', on='countryId')\n",
    "    \n",
    "    # Rearrange columns\n",
    "    dfMerged = dfMerged[dfMerged.columns[[3,4,0,1,2,5,6,7,8]]]\n",
    "    \n",
    "    # Replace with NaN\n",
    "    dfMerged = dfMerged.replace('', np.nan)\n",
    "    dfMerged = dfMerged.replace('(not set)', np.nan)\n",
    "    \n",
    "    # Sort columns\n",
    "    sortOrder = True\n",
    "    dfSorted = dfMerged.sort_values(list(dfMerged.columns.values), ascending=sortOrder)\n",
    "    \n",
    "    \n",
    "    # Write to and overwrite file\n",
    "    sep = ','\n",
    "    csvFilePath = \"output/csv/reference/\"\n",
    "    if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "    fileName = \"geographyReference.csv\"\n",
    "    fileNameFull = csvFilePath + fileName\n",
    "    \n",
    "    if not dfSorted.empty:\n",
    "        dfSorted.to_csv(fileNameFull, mode='w', index=0, sep=',')        \n",
    "    \n",
    "    # -------------------------------------------\n",
    "    \n",
    "    ## Table with all dates, not just those with fetched data, as reference\n",
    "    \n",
    "    # Date range\n",
    "    dates = pd.date_range('2022-01-01', '2030-01-01', freq=\"H\",inclusive='left')\n",
    "    \n",
    "    # List to Dataframe\n",
    "    dateReference = pd.DataFrame(dates, columns=['dateHour'])\n",
    "    \n",
    "    # Running function sortOut_dateHour\n",
    "    dateReference = dates_funcs.sortOut_dateHour(dateReference)\n",
    "    \n",
    "    csvFilePath = \"output/csv/reference/\"\n",
    "    fileName = \"dateReference.csv\"\n",
    "    fileNameFull = csvFilePath + fileName\n",
    "    \n",
    "    # If file does not alreay exist, write to file\n",
    "    if not os.path.isfile(fileNameFull):\n",
    "        dateReference.to_csv(fileNameFull, mode='w', index=0, sep=',')\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    ## Max time to latestDatefile\n",
    "    \n",
    "    # Find largest date in dataframes, looking at df eventName, which should have all dates. (?)\n",
    "    maxTimestamp = dfs_merged_fillNA[14][\"dateFull\"].max()\n",
    "    maxTimestampString = datetime.datetime.strftime(maxTimestamp,'%Y-%m-%d')\n",
    "    maxDate = datetime.datetime.strptime(maxTimestampString,'%Y-%m-%d').date()\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"Fetched data until \"+maxTimestampString)\n",
    "    \n",
    "    # If date is larger than priviously max date.\n",
    "    if maxDate > maxSavedDateDT:\n",
    "    # Write largest date to file\n",
    "        f = open(\"latestDate.txt\", 'w')\n",
    "        f.write(maxTimestampString)\n",
    "        f.close()\n",
    "    \n",
    "    print(\"Written to .csv files\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR\")\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Footnote 1\n",
    "print(\n",
    "'''The following License applies to gapandas4 ONLY:\n",
    "\n",
    "MIT License\n",
    "Copyright (c) 2018\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \\\"Software\\\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \\\"AS IS\\\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "\n",
    "'''\n",
    ")\n",
    "# Now to print to log when script completed\n",
    "\n",
    "nowDT = datetime.datetime.now()\n",
    "now = datetime.datetime.strftime(nowDT,'%Y-%m-%d_%H:%M:%S')\n",
    "print(\"Script finished: \"+now)\n",
    "print(\"\\n\")\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb15cff-943f-4fe8-ad47-49faa86baa6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

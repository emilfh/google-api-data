{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f24eb0a0-83c0-49cd-b1aa-8019b7f95db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script started: 2023-06-27_01:10:41\n",
      "\n",
      "\n",
      "Last date saved: 2022-12-30\n",
      "\n",
      "\n",
      "Fetching new data, starting: 2022-12-31\n",
      "up to and including: 2023-01-03\n",
      "\n",
      "\n",
      "(215, 22)\n",
      "(217, 22)\n",
      "(222, 22)\n",
      "(212, 22)\n",
      "(221, 22)\n",
      "(212, 22)\n",
      "(226, 22)\n",
      "(226, 22)\n",
      "(206, 22)\n",
      "(230, 22)\n",
      "(209, 22)\n",
      "(214, 22)\n",
      "(210, 22)\n",
      "(218, 22)\n",
      "(696, 22)\n",
      "(278, 22)\n",
      "(214, 22)\n",
      "(234, 22)\n",
      "(274, 22)\n",
      "(1, 20)\n",
      "(2, 20)\n",
      "(0, 19)\n",
      "(2, 19)\n",
      "(432, 6)\n",
      "(502, 6)\n",
      "(553, 6)\n",
      "(462, 6)\n",
      "(582, 6)\n",
      "(553, 6)\n",
      "(716, 6)\n",
      "(805, 6)\n",
      "(380, 6)\n",
      "(871, 6)\n",
      "(443, 6)\n",
      "(539, 6)\n",
      "(445, 6)\n",
      "(528, 6)\n",
      "(1550, 6)\n",
      "(1098, 6)\n",
      "(436, 6)\n",
      "(889, 6)\n",
      "(899, 6)\n",
      "(1, 6)\n",
      "(2, 6)\n",
      "(0, 5)\n",
      "(2, 5)\n",
      "(1076, 6)\n",
      "(1352, 6)\n",
      "(1388, 6)\n",
      "(1128, 6)\n",
      "(1548, 6)\n",
      "(1592, 6)\n",
      "(2217, 6)\n",
      "(2339, 6)\n",
      "(831, 6)\n",
      "(2752, 6)\n",
      "(1065, 6)\n",
      "(1291, 6)\n",
      "(1073, 6)\n",
      "(1303, 6)\n",
      "(3651, 6)\n",
      "(3343, 6)\n",
      "(1088, 6)\n",
      "(2711, 6)\n",
      "(2550, 6)\n",
      "(1, 6)\n",
      "(2, 6)\n",
      "(0, 5)\n",
      "(2, 5)\n",
      "output/csv/metrics/language/language_dateHour_cityId.csv\n",
      "output/csv/metrics/languageCode/languageCode_dateHour_cityId.csv\n",
      "output/csv/metrics/browser/browser_dateHour_cityId.csv\n",
      "output/csv/metrics/deviceCategory/deviceCategory_dateHour_cityId.csv\n",
      "output/csv/metrics/mobileDeviceBranding/mobileDeviceBranding_dateHour_cityId.csv\n",
      "output/csv/metrics/mobileDeviceMarketingName/mobileDeviceMarketingName_dateHour_cityId.csv\n",
      "output/csv/metrics/mobileDeviceModel/mobileDeviceModel_dateHour_cityId.csv\n",
      "output/csv/metrics/operatingSystemWithVersion/operatingSystemWithVersion_dateHour_cityId.csv\n",
      "output/csv/metrics/platform/platform_dateHour_cityId.csv\n",
      "output/csv/metrics/screenResolution/screenResolution_dateHour_cityId.csv\n",
      "output/csv/metrics/firstUserDefaultChannelGroup/firstUserDefaultChannelGroup_dateHour_cityId.csv\n",
      "output/csv/metrics/firstUserSource/firstUserSource_dateHour_cityId.csv\n",
      "output/csv/metrics/sessionDefaultChannelGroup/sessionDefaultChannelGroup_dateHour_cityId.csv\n",
      "output/csv/metrics/sessionSource/sessionSource_dateHour_cityId.csv\n",
      "output/csv/metrics/eventName/eventName_dateHour_cityId.csv\n",
      "output/csv/metrics/pagePath/pagePath_dateHour_cityId.csv\n",
      "output/csv/metrics/linkUrl/linkUrl_dateHour_cityId.csv\n",
      "output/csv/metrics/landingPage/landingPage_dateHour_cityId.csv\n",
      "output/csv/metrics/pageReferrer/pageReferrer_dateHour_cityId.csv\n",
      "output/csv/metrics/userGender/userGender_date_countryId.csv\n",
      "output/csv/metrics/brandingInterest/brandingInterest_date_countryId.csv\n",
      "output/csv/metrics/userAgeBracket/userAgeBracket_countryId.csv\n",
      "output/csv/loyalty/language/language_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/languageCode/languageCode_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/browser/browser_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/deviceCategory/deviceCategory_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/mobileDeviceBranding/mobileDeviceBranding_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/mobileDeviceMarketingName/mobileDeviceMarketingName_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/mobileDeviceModel/mobileDeviceModel_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/operatingSystemWithVersion/operatingSystemWithVersion_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/platform/platform_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/screenResolution/screenResolution_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/firstUserDefaultChannelGroup/firstUserDefaultChannelGroup_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/firstUserSource/firstUserSource_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/sessionDefaultChannelGroup/sessionDefaultChannelGroup_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/sessionSource/sessionSource_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/eventName/eventName_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/pagePath/pagePath_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/linkUrl/linkUrl_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/landingPage/landingPage_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/pageReferrer/pageReferrer_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/userGender/userGender_date_countryId_loyalty.csv\n",
      "output/csv/loyalty/brandingInterest/brandingInterest_date_countryId_loyalty.csv\n",
      "output/csv/loyalty/userAgeBracket/userAgeBracket_countryId_loyalty.csv\n",
      "output/csv/activity/language/language_dateHour_cityId_activity.csv\n",
      "output/csv/activity/languageCode/languageCode_dateHour_cityId_activity.csv\n",
      "output/csv/activity/browser/browser_dateHour_cityId_activity.csv\n",
      "output/csv/activity/deviceCategory/deviceCategory_dateHour_cityId_activity.csv\n",
      "output/csv/activity/mobileDeviceBranding/mobileDeviceBranding_dateHour_cityId_activity.csv\n",
      "output/csv/activity/mobileDeviceMarketingName/mobileDeviceMarketingName_dateHour_cityId_activity.csv\n",
      "output/csv/activity/mobileDeviceModel/mobileDeviceModel_dateHour_cityId_activity.csv\n",
      "output/csv/activity/operatingSystemWithVersion/operatingSystemWithVersion_dateHour_cityId_activity.csv\n",
      "output/csv/activity/platform/platform_dateHour_cityId_activity.csv\n",
      "output/csv/activity/screenResolution/screenResolution_dateHour_cityId_activity.csv\n",
      "output/csv/activity/firstUserDefaultChannelGroup/firstUserDefaultChannelGroup_dateHour_cityId_activity.csv\n",
      "output/csv/activity/firstUserSource/firstUserSource_dateHour_cityId_activity.csv\n",
      "output/csv/activity/sessionDefaultChannelGroup/sessionDefaultChannelGroup_dateHour_cityId_activity.csv\n",
      "output/csv/activity/sessionSource/sessionSource_dateHour_cityId_activity.csv\n",
      "output/csv/activity/eventName/eventName_dateHour_cityId_activity.csv\n",
      "output/csv/activity/pagePath/pagePath_dateHour_cityId_activity.csv\n",
      "output/csv/activity/linkUrl/linkUrl_dateHour_cityId_activity.csv\n",
      "output/csv/activity/landingPage/landingPage_dateHour_cityId_activity.csv\n",
      "output/csv/activity/pageReferrer/pageReferrer_dateHour_cityId_activity.csv\n",
      "output/csv/activity/userGender/userGender_date_countryId_activity.csv\n",
      "output/csv/activity/brandingInterest/brandingInterest_date_countryId_activity.csv\n",
      "output/csv/activity/userAgeBracket/userAgeBracket_countryId_activity.csv\n",
      "(92, 7)\n",
      "(79, 11)\n",
      "output/csv/reference/geographyInfo.csv\n",
      "output/csv/reference/dateInfo.csv\n",
      "\n",
      "\n",
      "Fetched data until 2023-01-03\n",
      "Written to .csv files\n",
      "\n",
      "\n",
      "The following License applies to gapandas4 ONLY:\n",
      "\n",
      "MIT License\n",
      "Copyright (c) 2018\n",
      "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
      "of this software and associated documentation files (the \"Software\"), to deal\n",
      "in the Software without restriction, including without limitation the rights\n",
      "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
      "copies of the Software, and to permit persons to whom the Software is\n",
      "furnished to do so, subject to the following conditions:\n",
      "The above copyright notice and this permission notice shall be included in all\n",
      "copies or substantial portions of the Software.\n",
      "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
      "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
      "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
      "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
      "SOFTWARE.\n",
      "\n",
      "\n",
      "Script finished: 2023-06-27_01:12:39\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "CPU times: user 11.5 s, sys: 569 ms, total: 12.1 s\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Import Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gapandas4 as gp # See Footnote 1 bottom of page\n",
    "import os\n",
    "import datetime\n",
    "from datetime import date\n",
    "# Written functions\n",
    "\n",
    "import itertools\n",
    "import sys\n",
    "# adding Notebooksfolder to the system path\n",
    "sys.path.insert(0, '/Users/emil/miniforge3/envs/googleapi/Notebooks')\n",
    "\n",
    "import importlib\n",
    "import dates_funcs\n",
    "importlib.reload(dates_funcs)\n",
    "from dates_funcs import appendDFToCSV\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Now to print to log when program running \n",
    "nowDT = datetime.datetime.now()\n",
    "now = datetime.datetime.strftime(nowDT,'%Y-%m-%d_%H:%M:%S')\n",
    "print(\"Script started: \"+now)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Check of Dates\n",
    "f = open(\"latestDate.txt\")\n",
    "maxSavedDate = f.read()\n",
    "f.close()\n",
    "#print(\"Previously fetched data up to and including: \"+ maxSavedDate)\n",
    "maxSavedDateDT = datetime.datetime.strptime(maxSavedDate,'%Y-%m-%d').date()\n",
    "print(\"Last date saved: \"+maxSavedDate)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Todays date\n",
    "todayDT = date.today()\n",
    "today = datetime.datetime.strftime(todayDT,'%Y-%m-%d')\n",
    "\n",
    "# start_date as the next day as maxSavedDate\n",
    "start_dateDT = maxSavedDateDT + datetime.timedelta(days=1)\n",
    "start_date = datetime.datetime.strftime(start_dateDT,'%Y-%m-%d')\n",
    "print(\"Fetching new data, starting: \"+start_date)\n",
    "\n",
    "# # end_date as the same day as start_date, to get data for one days\n",
    "end_dateDT = start_dateDT + datetime.timedelta(days=3)\n",
    "#end_dateDT = start_dateDT \n",
    "end_date = datetime.datetime.strftime(end_dateDT,'%Y-%m-%d')\n",
    "print(\"up to and including: \"+end_date)\n",
    "print(\"\\n\")\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#MASTER IF\n",
    "\n",
    "if start_dateDT > maxSavedDateDT and todayDT > end_dateDT and todayDT > maxSavedDateDT:\n",
    "    \n",
    "    # START BACKUP or previous .csv files to an archive\n",
    "    # DISABLED because taking up lots of space, will enable manually once in a while instead\n",
    "    '''\n",
    "    import pathlib\n",
    "    import zipfile\n",
    "    from zipfile import ZipFile, ZIP_LZMA\n",
    "    \n",
    "    directory = pathlib.Path(\"output/\")\n",
    "\n",
    "    try:\n",
    "        with ZipFile(\"backup_date:_\"+maxSavedDate+\"_written:_\"+now+\".zip\", mode=\"w\",compression=ZIP_LZMA, allowZip64=True) as archive:\n",
    "            for file_path in directory.rglob(\"*\"):\n",
    "                archive.write(\n",
    "                    file_path,\n",
    "                    arcname=file_path.relative_to(directory)\n",
    "                )\n",
    "            print(\"Previous .csv-files backed up to: backup_until:_\"+maxSavedDate+\"_written:_\"+now+\".zip\")\n",
    "    except BadZipFile as error:\n",
    "        print(error)\n",
    "    '''\n",
    "    # END BACKUP    \n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # CREDENTIALS\n",
    "    property_id = \"298727788\"\n",
    "    credentials_json_path=\"/Users/emil/miniforge3/envs/googleapi/Notebooks/Analytics/client_secrets.json\"\n",
    "    service_account = credentials_json_path\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # LISTS DIMENSIONS AND METRICS\n",
    "    # Dimensions\n",
    "    dimensions = [\"language\",\"languageCode\",\"browser\",\"deviceCategory\",\"mobileDeviceBranding\",\n",
    "                  \"mobileDeviceMarketingName\",\"mobileDeviceModel\",\"operatingSystemWithVersion\",\n",
    "                  \"platform\",\"screenResolution\",\"firstUserDefaultChannelGroup\",\"firstUserSource\",\n",
    "                  \"sessionDefaultChannelGroup\",\"sessionSource\",\"eventName\",\"pagePath\",\"linkUrl\",\n",
    "                  \"landingPage\",\"pageReferrer\"]\n",
    "    \n",
    "    # Does not work with cityId, only countryId\n",
    "    # does not work with dateHour only date.\n",
    "    dimensionsSpecial = [\"userGender\",\"brandingInterest\"]\n",
    "    \n",
    "    # Does not work with cityId, only countryId,\n",
    "    # does not work with dateHour only date:\n",
    "    # also cant request cityId and date in same request.\n",
    "    dimensionsSpecialSpecial = [\"userAgeBracket\"]\n",
    "    \n",
    "    dateHourList = [\"dateHour\"]\n",
    "    dateList = [\"date\"]\n",
    "    cityIdList = [\"cityId\"]\n",
    "    countryIdList = [\"countryId\"]\n",
    "    \n",
    "    # Metrics with empty first place and repeat last to count 1-18\n",
    "    metricList = [\"\",\"totalUsers\",\"newUsers\",\"activeUsers\",\"userEngagementDuration\",\"scrolledUsers\",\n",
    "                  \"averageSessionDuration\",\"bounceRate\",\"engagedSessions\",\"engagementRate\",\"sessions\",\n",
    "                  \"sessionsPerUser\",\"eventCount\",\"eventCountPerUser\",\"eventsPerSession\",\"screenPageViews\",\n",
    "                  \"screenPageViewsPerSession\",\"screenPageViewsPerUser\",\"totalUsers\"]\n",
    "        \n",
    "    loyaltyList = [\"wauPerMau\",\"dauPerMau\",\"dauPerWau\"]\n",
    "        \n",
    "    activityList = [\"active28DayUsers\",\"active7DayUsers\",\"activeUsers\"]\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # REQUESTS\n",
    "    \n",
    "    ### dimensions\n",
    "    \n",
    "    #### metricList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs = [[],[],[],]\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensions)):\n",
    "        for y, z in zip([1,7,13],[0,1,2]): \n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=dateHourList[0]),\n",
    "                    gp.Dimension(name=cityIdList[0]),\n",
    "                    gp.Dimension(name=dimensions[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=metricList[y]),\n",
    "                    gp.Metric(name=metricList[y+1]),\n",
    "                    gp.Metric(name=metricList[y+2]),\n",
    "                    gp.Metric(name=metricList[y+3]),\n",
    "                    gp.Metric(name=metricList[y+4]),\n",
    "                    gp.Metric(name=metricList[y+5]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs[z].append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "            \n",
    "    #### loyaltyList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_l = []\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensions)):\n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=dateList[0]),\n",
    "                    gp.Dimension(name=cityIdList[0]),\n",
    "                    gp.Dimension(name=dimensions[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=loyaltyList[0]),\n",
    "                    gp.Metric(name=loyaltyList[1]),\n",
    "                    gp.Metric(name=loyaltyList[2]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_l.append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "        \n",
    "    #### activityList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_a = []\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensions)):\n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=dateList[0]),\n",
    "                    gp.Dimension(name=cityIdList[0]),\n",
    "                    gp.Dimension(name=dimensions[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=activityList[0]),\n",
    "                    gp.Metric(name=activityList[1]),\n",
    "                    gp.Metric(name=activityList[2]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_a.append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "    \n",
    "        \n",
    "    ### dimensionsSpecial\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_s = [[],[],[],]\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensionsSpecial)):\n",
    "        for y, z in zip([1,7,13],[0,1,2]): \n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=dateList[0]),\n",
    "                    gp.Dimension(name=countryIdList[0]),\n",
    "                    gp.Dimension(name=dimensionsSpecial[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=metricList[y]),\n",
    "                    gp.Metric(name=metricList[y+1]),\n",
    "                    gp.Metric(name=metricList[y+2]),\n",
    "                    gp.Metric(name=metricList[y+3]),\n",
    "                    gp.Metric(name=metricList[y+4]),\n",
    "                    gp.Metric(name=metricList[y+5]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_s[z].append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "            \n",
    "    #### loyaltyList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_s_l = []\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensionsSpecial)):\n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=dateList[0]),\n",
    "                    gp.Dimension(name=countryIdList[0]),\n",
    "                    gp.Dimension(name=dimensionsSpecial[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=loyaltyList[0]),\n",
    "                    gp.Metric(name=loyaltyList[1]),\n",
    "                    gp.Metric(name=loyaltyList[2]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_s_l.append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "        \n",
    "    #### activityList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_s_a = []\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensionsSpecial)):\n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=dateList[0]),\n",
    "                    gp.Dimension(name=countryIdList[0]),\n",
    "                    gp.Dimension(name=dimensionsSpecial[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=activityList[0]),\n",
    "                    gp.Metric(name=activityList[1]),\n",
    "                    gp.Metric(name=activityList[2]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_s_a.append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "    \n",
    "        \n",
    "    ### dimensionsSpecialSpecial date\n",
    "    \n",
    "    #### metricList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_ssd = [[],[],[],]\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        for y, z in zip([1,7,13],[0,1,2]): \n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=dateList[0]),\n",
    "                    gp.Dimension(name=dimensionsSpecialSpecial[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=metricList[y]),\n",
    "                    gp.Metric(name=metricList[y+1]),\n",
    "                    gp.Metric(name=metricList[y+2]),\n",
    "                    gp.Metric(name=metricList[y+3]),\n",
    "                    gp.Metric(name=metricList[y+4]),\n",
    "                    gp.Metric(name=metricList[y+5]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_ssd[z].append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "            \n",
    "    #### loyaltyList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_ssd_l = []\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=dateList[0]),\n",
    "                    gp.Dimension(name=dimensionsSpecialSpecial[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=loyaltyList[0]),\n",
    "                    gp.Metric(name=loyaltyList[1]),\n",
    "                    gp.Metric(name=loyaltyList[2]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_ssd_l.append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "        \n",
    "    #### activityList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_ssd_a = []\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=dateList[0]),\n",
    "                    gp.Dimension(name=dimensionsSpecialSpecial[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=activityList[0]),\n",
    "                    gp.Metric(name=activityList[1]),\n",
    "                    gp.Metric(name=activityList[2]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_ssd_a.append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "    \n",
    "        \n",
    "    ### dimensionsSpecialSpecial countryId\n",
    "    \n",
    "    #### metricList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_ssc = [[],[],[],]\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        for y, z in zip([1,7,13],[0,1,2]): \n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=countryIdList[0]),\n",
    "                    gp.Dimension(name=dimensionsSpecialSpecial[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=metricList[y]),\n",
    "                    gp.Metric(name=metricList[y+1]),\n",
    "                    gp.Metric(name=metricList[y+2]),\n",
    "                    gp.Metric(name=metricList[y+3]),\n",
    "                    gp.Metric(name=metricList[y+4]),\n",
    "                    gp.Metric(name=metricList[y+5]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_ssc[z].append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "            \n",
    "    #### loyaltyList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_ssc_l = []\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=countryIdList[0]),\n",
    "                    gp.Dimension(name=dimensionsSpecialSpecial[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=loyaltyList[0]),\n",
    "                    gp.Metric(name=loyaltyList[1]),\n",
    "                    gp.Metric(name=loyaltyList[2]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_ssc_l.append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "        \n",
    "    #### activityList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_ssc_a = []\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=countryIdList[0]),\n",
    "                    gp.Dimension(name=dimensionsSpecialSpecial[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=activityList[0]),\n",
    "                    gp.Metric(name=activityList[1]),\n",
    "                    gp.Metric(name=activityList[2]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_ssc_a.append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "    ## Formatting\n",
    "    \n",
    "    #### metricList\n",
    "    \n",
    "    # Drop the extra Totalusers column, since it exists twice in the tables to merge\n",
    "    # since metricList contained it twice.\n",
    "    \n",
    "    for x in range(len(dimensions)):\n",
    "        for z in [2]:\n",
    "            dfs[z][x].drop(columns=['totalUsers'],inplace=True)\n",
    "    \n",
    "    for x in range(len(dimensionsSpecial)):\n",
    "        for z in [2]:\n",
    "            dfs_s[z][x].drop(columns=['totalUsers'],inplace=True)\n",
    "    \n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        for z in [2]:\n",
    "            dfs_ssd[z][x].drop(columns=['totalUsers'],inplace=True)\n",
    "    \n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        for z in [2]:\n",
    "            dfs_ssc[z][x].drop(columns=['totalUsers'],inplace=True)\n",
    "            \n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "    ## Merge\n",
    "    \n",
    "    dfs_merged = list(range(len(dimensions)))\n",
    "    \n",
    "    for x in range(len(dimensions)):\n",
    "        mergeOn = ['dateHour','cityId'] \n",
    "        mergeDim = [dimensions[x]]\n",
    "        mergeOn =  mergeOn + mergeDim\n",
    "        dfs_merged[x] = dfs[0][x].merge(dfs[1][x], how='outer', on=mergeOn)\n",
    "    \n",
    "    for n in range(len(dimensions)):\n",
    "        mergeOn = ['dateHour','cityId'] \n",
    "        mergeDim = [dimensions[n]]\n",
    "        mergeOn =  mergeOn + mergeDim\n",
    "        dfs_merged[n] = dfs_merged[n].merge(dfs[2][n], how='outer', on=mergeOn)\n",
    "    \n",
    "    \n",
    "    dfs_s_merged = list(range(len(dimensionsSpecial)))\n",
    "    \n",
    "    for x in range(len(dimensionsSpecial)):\n",
    "        mergeOn = ['date','countryId'] \n",
    "        mergeDim = [dimensionsSpecial[x]]\n",
    "        mergeOn =  mergeOn + mergeDim\n",
    "        dfs_s_merged[x] = dfs_s[0][x].merge(dfs_s[1][x], how='outer', on=mergeOn)\n",
    "    \n",
    "    for n in range(len(dimensionsSpecial)):\n",
    "        mergeOn = ['date','countryId'] \n",
    "        mergeDim = [dimensionsSpecial[n]]\n",
    "        mergeOn =  mergeOn + mergeDim\n",
    "        dfs_s_merged[n] = dfs_s_merged[n].merge(dfs_s[2][n], how='outer', on=mergeOn)\n",
    "    \n",
    "    \n",
    "    dfs_ssd_merged = list(range(len(dimensionsSpecialSpecial)))\n",
    "    \n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        mergeOn = ['date'] \n",
    "        mergeDim = [dimensionsSpecialSpecial[x]]\n",
    "        mergeOn =  mergeOn + mergeDim\n",
    "        dfs_ssd_merged[x] = dfs_ssd[0][x].merge(dfs_ssd[1][x], how='outer', on=mergeOn)\n",
    "    \n",
    "    for n in range(len(dimensionsSpecialSpecial)):\n",
    "        mergeOn = ['date'] \n",
    "        mergeDim = [dimensionsSpecialSpecial[n]]\n",
    "        mergeOn =  mergeOn + mergeDim\n",
    "        dfs_ssd_merged[n] = dfs_ssd_merged[n].merge(dfs_ssd[2][n], how='outer', on=mergeOn)\n",
    "    \n",
    "    \n",
    "    dfs_ssc_merged = list(range(len(dimensionsSpecialSpecial)))\n",
    "    \n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        mergeOn = ['countryId'] \n",
    "        mergeDim = [dimensionsSpecialSpecial[x]]\n",
    "        mergeOn =  mergeOn + mergeDim\n",
    "        dfs_ssc_merged[x] = dfs_ssc[0][x].merge(dfs_ssc[1][x], how='outer', on=mergeOn)\n",
    "    \n",
    "    for n in range(len(dimensionsSpecialSpecial)):\n",
    "        mergeOn = ['countryId'] \n",
    "        mergeDim = [dimensionsSpecialSpecial[n]]\n",
    "        mergeOn =  mergeOn + mergeDim\n",
    "        dfs_ssc_merged[n] = dfs_ssc_merged[n].merge(dfs_ssc[2][n], how='outer', on=mergeOn)\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Sort out dates and sort columns.\n",
    "        \n",
    "    #### metricList\n",
    "    \n",
    "    for x in range(len(dimensions)):\n",
    "        if not dfs_merged[x].empty:\n",
    "            dfs_merged[x] = dates_funcs.sortOut_dateHour_short(dfs_merged[x])\n",
    "    \n",
    "    for x in range(len(dimensionsSpecial)):\n",
    "        if not  dfs_s_merged[x].empty:\n",
    "            dfs_s_merged[x] = dates_funcs.sortOut_date_short(dfs_s_merged[x])\n",
    "            \n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        if not dfs_ssd_merged[x].empty:\n",
    "            dfs_ssd_merged[x] = dates_funcs.sortOut_date_short(dfs_ssd_merged[x])\n",
    "    \n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        if not dfs_ssc_merged[x].empty:\n",
    "            sortOrder = True\n",
    "            dfs_ssc_merged[x] = dfs_ssc_merged[x].sort_values(list(dfs_ssc_merged[x].columns.values), ascending=sortOrder)\n",
    "    \n",
    "    #### loyaltyList\n",
    "    \n",
    "    for x in range(len(dimensions)):\n",
    "        if not dfs_l[x].empty:\n",
    "            dfs_l[x] = dates_funcs.sortOut_date_short(dfs_l[x])\n",
    "    \n",
    "    for x in range(len(dimensionsSpecial)):\n",
    "        if not dfs_s_l[x].empty:\n",
    "            dfs_s_l[x] = dates_funcs.sortOut_date_short(dfs_s_l[x])\n",
    "            \n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        if not dfs_ssd_l[x].empty:\n",
    "            dfs_ssd_l[x] = dates_funcs.sortOut_date_short(dfs_ssd_l[x])\n",
    "\n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        if not dfs_ssc_l[x].empty:\n",
    "            sortOrder = True\n",
    "            dfs_ssc_l[x] = dfs_ssc_l[x].sort_values(list(dfs_ssc_l[x].columns.values), ascending=sortOrder)\n",
    "    \n",
    "    #### activityList\n",
    "    \n",
    "    for x in range(len(dimensions)):\n",
    "        if not dfs_a[x].empty:\n",
    "            dfs_a[x] = dates_funcs.sortOut_date_short(dfs_a[x])\n",
    "    \n",
    "    for x in range(len(dimensionsSpecial)):\n",
    "        if not dfs_s_a[x].empty:\n",
    "            dfs_s_a[x] = dates_funcs.sortOut_date_short(dfs_s_a[x])\n",
    "            \n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        if not dfs_ssd_a[x].empty:\n",
    "            dfs_ssd_a[x] = dates_funcs.sortOut_date_short(dfs_ssd_a[x])\n",
    "    \n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        if not dfs_ssc_a[x].empty:\n",
    "            sortOrder = True\n",
    "            dfs_ssc_a[x] = dfs_ssc_a[x].sort_values(list(dfs_ssc_a[x].columns.values), ascending=sortOrder)\n",
    "        \n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Add baseURL advokatfamiljforsvar.se to fullUrl\n",
    "\n",
    "    pos_landingPage = dimensions.index(\"landingPage\") \n",
    "    mainAdress = 'https://advokatfamiljforsvar.se'  \n",
    "            \n",
    "    dfs_merged[pos_landingPage]['landingPage'] = dfs_merged[pos_landingPage]['landingPage'].where(dfs_merged[pos_landingPage]['landingPage'].values == '(not set)', mainAdress + dfs_merged[pos_landingPage]['landingPage'])\n",
    "\n",
    "    dfs_l[pos_landingPage]['landingPage'] = dfs_l[pos_landingPage]['landingPage'].where(dfs_l[pos_landingPage]['landingPage'].values == '(not set)', mainAdress + dfs_l[pos_landingPage]['landingPage'])\n",
    "\n",
    "    dfs_a[pos_landingPage]['landingPage'] = dfs_a[pos_landingPage]['landingPage'].where(dfs_a[pos_landingPage]['landingPage'].values == '(not set)', mainAdress + dfs_a[pos_landingPage]['landingPage'])\n",
    "    \n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    ## Sort out NA\n",
    "    \n",
    "    #### metricList\n",
    "    \n",
    "    dfs_merged_fillNA = list(range(len(dimensions)))\n",
    "    \n",
    "    for n in range(len(dimensions)):\n",
    "        dfs_merged_fillNA[n] = dfs_merged[n].replace('', np.nan)\n",
    "        dfs_merged_fillNA[n] = dfs_merged_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_merged_fillNA[n].shape)\n",
    "    \n",
    "    \n",
    "    dfs_s_merged_fillNA = list(range(len(dimensionsSpecial)))\n",
    "    \n",
    "    for n in range(len(dimensionsSpecial)):\n",
    "        dfs_s_merged_fillNA[n] = dfs_s_merged[n].replace('', np.nan)\n",
    "        dfs_s_merged_fillNA[n] = dfs_s_merged_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_s_merged_fillNA[n].shape)\n",
    "    \n",
    "    \n",
    "    dfs_ssd_merged_fillNA = list(range(len(dimensionsSpecialSpecial)))\n",
    "    \n",
    "    for n in range(len(dimensionsSpecialSpecial)):\n",
    "        dfs_ssd_merged_fillNA[n] = dfs_ssd_merged[n].replace('', np.nan)\n",
    "        dfs_ssd_merged_fillNA[n] = dfs_ssd_merged_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_ssd_merged_fillNA[n].shape)\n",
    "    \n",
    "    \n",
    "    dfs_ssc_merged_fillNA = list(range(len(dimensionsSpecialSpecial)))\n",
    "    \n",
    "    for n in range(len(dimensionsSpecialSpecial)):\n",
    "        dfs_ssc_merged_fillNA[n] = dfs_ssc_merged[n].replace('', np.nan)\n",
    "        dfs_ssc_merged_fillNA[n] = dfs_ssc_merged_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_ssc_merged_fillNA[n].shape)\n",
    "    \n",
    "    #### loyaltyList\n",
    "    \n",
    "    dfs_l_fillNA = list(range(len(dimensions)))\n",
    "    \n",
    "    for n in range(len(dimensions)):\n",
    "        dfs_l_fillNA[n] = dfs_l[n].replace('', np.nan)\n",
    "        dfs_l_fillNA[n] = dfs_l_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_l_fillNA[n].shape)\n",
    "    \n",
    "    \n",
    "    dfs_s_l_fillNA = list(range(len(dimensionsSpecial)))\n",
    "    \n",
    "    for n in range(len(dimensionsSpecial)):\n",
    "        dfs_s_l_fillNA[n] = dfs_s_l[n].replace('', np.nan)\n",
    "        dfs_s_l_fillNA[n] = dfs_s_l_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_s_l_fillNA[n].shape)\n",
    "    \n",
    "    \n",
    "    dfs_ssd_l_fillNA = list(range(len(dimensionsSpecialSpecial)))\n",
    "    \n",
    "    for n in range(len(dimensionsSpecialSpecial)):\n",
    "        dfs_ssd_l_fillNA[n] = dfs_ssd_l[n].replace('', np.nan)\n",
    "        dfs_ssd_l_fillNA[n] = dfs_ssd_l_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_ssd_l_fillNA[n].shape)\n",
    "    \n",
    "    \n",
    "    dfs_ssc_l_fillNA = list(range(len(dimensionsSpecialSpecial)))\n",
    "    \n",
    "    for n in range(len(dimensionsSpecialSpecial)):\n",
    "        dfs_ssc_l_fillNA[n] = dfs_ssc_l[n].replace('', np.nan)\n",
    "        dfs_ssc_l_fillNA[n] = dfs_ssc_l_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_ssc_l_fillNA[n].shape)\n",
    "    \n",
    "    #### activityList\n",
    "    \n",
    "    \n",
    "    dfs_a_fillNA = list(range(len(dimensions)))\n",
    "    \n",
    "    for n in range(len(dimensions)):\n",
    "        dfs_a_fillNA[n] = dfs_a[n].replace('', np.nan)\n",
    "        dfs_a_fillNA[n] = dfs_a_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_a_fillNA[n].shape)\n",
    "    \n",
    "    \n",
    "    dfs_s_a_fillNA = list(range(len(dimensionsSpecial)))\n",
    "    \n",
    "    for n in range(len(dimensionsSpecial)):\n",
    "        dfs_s_a_fillNA[n] = dfs_s_a[n].replace('', np.nan)\n",
    "        dfs_s_a_fillNA[n] = dfs_s_a_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_s_a_fillNA[n].shape)\n",
    "    \n",
    "    \n",
    "    dfs_ssd_a_fillNA = list(range(len(dimensionsSpecialSpecial)))\n",
    "    \n",
    "    for n in range(len(dimensionsSpecialSpecial)):\n",
    "        dfs_ssd_a_fillNA[n] = dfs_ssd_a[n].replace('', np.nan)\n",
    "        dfs_ssd_a_fillNA[n] = dfs_ssd_a_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_ssd_a_fillNA[n].shape)\n",
    "    \n",
    "    \n",
    "    dfs_ssc_a_fillNA = list(range(len(dimensionsSpecialSpecial)))\n",
    "    \n",
    "    for n in range(len(dimensionsSpecialSpecial)):\n",
    "        dfs_ssc_a_fillNA[n] = dfs_ssc_a[n].replace('', np.nan)\n",
    "        dfs_ssc_a_fillNA[n] = dfs_ssc_a_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_ssc_a_fillNA[n].shape)\n",
    "    \n",
    "        \n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "    ## Dataframes to .csv\n",
    "    \n",
    "    csvFilePath = \"output/csv/\"\n",
    "    fileName = dimensions[n]\n",
    "    sep = \",\"\n",
    "    \n",
    "    for n in range(len(dfs_merged_fillNA)):\n",
    "        if not dfs_merged_fillNA[n].empty:\n",
    "            csvFilePath = \"output/csv/metrics/\"+dimensions[n]+\"/\"\n",
    "            if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "            fileName = dimensions[n]+\"_dateHour\"+\"_cityId\"+\".csv\" \n",
    "            appendDFToCSV(dfs_merged_fillNA[n], csvFilePath, fileName, sep)\n",
    "    \n",
    "    for n in range(len(dfs_s_merged_fillNA)):\n",
    "        if not dfs_s_merged_fillNA[n].empty:\n",
    "            csvFilePath = \"output/csv/metrics/\"+dimensionsSpecial[n]+\"/\"\n",
    "            if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "            fileName = dimensionsSpecial[n]+\"_date\"+\"_countryId\"+\".csv\"\n",
    "            appendDFToCSV(dfs_s_merged_fillNA[n], csvFilePath, fileName, sep)\n",
    "    \n",
    "    for n in range(len(dfs_ssd_merged_fillNA)):\n",
    "        if not dfs_ssd_merged_fillNA[n].empty:\n",
    "            csvFilePath = \"output/csv/metrics/\"+dimensionsSpecialSpecial[n]+\"/\"\n",
    "            if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "            fileName = dimensionsSpecialSpecial[n]+\"_date\"+\".csv\"\n",
    "            appendDFToCSV(dfs_ssd_merged_fillNA[n], csvFilePath, fileName, sep)\n",
    "    \n",
    "    for n in range(len(dfs_ssc_merged_fillNA)):\n",
    "        if not dfs_ssc_merged_fillNA[n].empty:\n",
    "            csvFilePath = \"output/csv/metrics/\"+dimensionsSpecialSpecial[n]+\"/\"\n",
    "            if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "            fileName = dimensionsSpecialSpecial[n]+\"_countryId\"+\".csv\"\n",
    "            appendDFToCSV(dfs_ssc_merged_fillNA[n], csvFilePath, fileName, sep)\n",
    "    \n",
    "    for n in range(len(dfs_l_fillNA)):\n",
    "        if not dfs_l_fillNA[n].empty:\n",
    "            csvFilePath = \"output/csv/loyalty/\"+dimensions[n]+\"/\"\n",
    "            if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "            fileName = dimensions[n]+\"_dateHour\"+\"_cityId\"+\"_loyalty\"+\".csv\"\n",
    "            appendDFToCSV(dfs_l_fillNA[n], csvFilePath, fileName, sep)\n",
    "    \n",
    "    for n in range(len(dfs_s_l_fillNA)):\n",
    "        if not dfs_s_l_fillNA[n].empty:\n",
    "            csvFilePath = \"output/csv/loyalty/\"+dimensionsSpecial[n]+\"/\"\n",
    "            if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "            fileName = dimensionsSpecial[n]+\"_date\"+\"_countryId\"+\"_loyalty\"+\".csv\"\n",
    "            appendDFToCSV(dfs_s_l_fillNA[n], csvFilePath, fileName, sep)\n",
    "    \n",
    "    for n in range(len(dfs_ssd_l_fillNA)):\n",
    "        if not dfs_ssd_l_fillNA[n].empty:\n",
    "            csvFilePath = \"output/csv/loyalty/\"+dimensionsSpecialSpecial[n]+\"/\"\n",
    "            if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "            fileName = dimensionsSpecialSpecial[n]+\"_date\"+\"_loyalty\"+\".csv\"\n",
    "            appendDFToCSV(dfs_ssd_l_fillNA[n], csvFilePath, fileName, sep)\n",
    "    \n",
    "    for n in range(len(dfs_ssc_l_fillNA)):\n",
    "        if not dfs_ssc_l_fillNA[n].empty:\n",
    "            csvFilePath = \"output/csv/loyalty/\"+dimensionsSpecialSpecial[n]+\"/\"\n",
    "            if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "            fileName = dimensionsSpecialSpecial[n]+\"_countryId\"+\"_loyalty\"+\".csv\"\n",
    "            appendDFToCSV(dfs_ssc_l_fillNA[n], csvFilePath, fileName, sep)\n",
    "    \n",
    "    for n in range(len(dfs_a_fillNA)):\n",
    "        if not dfs_a_fillNA[n].empty:\n",
    "            csvFilePath = \"output/csv/activity/\"+dimensions[n]+\"/\"\n",
    "            if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "            fileName = dimensions[n]+\"_dateHour\"+\"_cityId\"+\"_activity\"+\".csv\"\n",
    "            appendDFToCSV(dfs_a_fillNA[n], csvFilePath, fileName, sep)\n",
    "    \n",
    "    for n in range(len(dfs_s_a_fillNA)):\n",
    "        if not dfs_s_a_fillNA[n].empty:\n",
    "            csvFilePath = \"output/csv/activity/\"+dimensionsSpecial[n]+\"/\"\n",
    "            if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "            fileName = dimensionsSpecial[n]+\"_date\"+\"_countryId\"+\"_activity\"+\".csv\"\n",
    "            appendDFToCSV(dfs_s_a_fillNA[n], csvFilePath, fileName, sep)\n",
    "    \n",
    "    for n in range(len(dfs_ssd_a_fillNA)):\n",
    "        if not dfs_ssd_a_fillNA[n].empty:\n",
    "            csvFilePath = \"output/csv/activity/\"+dimensionsSpecialSpecial[n]+\"/\"\n",
    "            if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "            fileName = dimensionsSpecialSpecial[n]+\"_date\"+\"_activity\"+\".csv\"\n",
    "            appendDFToCSV(dfs_ssd_a_fillNA[n], csvFilePath, fileName, sep)\n",
    "    \n",
    "    for n in range(len(dfs_ssc_a_fillNA)):\n",
    "        if not dfs_ssc_a_fillNA[n].empty:\n",
    "            csvFilePath = \"output/csv/activity/\"+dimensionsSpecialSpecial[n]+\"/\"\n",
    "            if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "            fileName = dimensionsSpecialSpecial[n]+\"_countryId\"+\"_activity\"+\".csv\"\n",
    "            appendDFToCSV(dfs_ssc_a_fillNA[n], csvFilePath, fileName, sep)\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "    ## Date and Geography tables\n",
    "    \n",
    "    # Geography List\n",
    "    geoList = ['continentId','continent','countryId','country','region','cityId','city']\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    \n",
    "    report_request = gp.RunReportRequest(\n",
    "        property=f\"properties/{property_id}\",\n",
    "         dimensions=[\n",
    "             gp.Dimension(name=geoList[0]),\n",
    "             gp.Dimension(name=geoList[1]),\n",
    "             gp.Dimension(name=geoList[2]),\n",
    "             gp.Dimension(name=geoList[3]),\n",
    "             gp.Dimension(name=geoList[4]),\n",
    "             gp.Dimension(name=geoList[5]),\n",
    "             gp.Dimension(name=geoList[6]),\n",
    "           ],\n",
    "           metrics=[\n",
    "            ],\n",
    "            date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "        )\n",
    "        \n",
    "    # Perform query and append to list\n",
    "    df_geo = gp.query(service_account, report_request, report_type=\"report\")\n",
    "\n",
    "    # Sort columns\n",
    "    sortOrder = True\n",
    "    df_geo = df_geo.sort_values(list(df_geo.columns.values), ascending=sortOrder)\n",
    "\n",
    "    \n",
    "    # Date List\n",
    "    dateList = [\"dateHour\"]\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    \n",
    "    report_request = gp.RunReportRequest(\n",
    "        property=f\"properties/{property_id}\",\n",
    "         dimensions=[\n",
    "             gp.Dimension(name=dateList[0]),\n",
    "           ],\n",
    "           metrics=[\n",
    "            ],\n",
    "            date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "        )\n",
    "    \n",
    "    # Perform query and append to list\n",
    "    df_dateHour = gp.query(service_account, report_request, report_type=\"report\")\n",
    "\n",
    "    # Sort out dates function\n",
    "    df_dateHour = dates_funcs.sortOut_dateHour(df_dateHour)\n",
    "\n",
    "    \n",
    "    df_geoTest = df_geo.replace('', np.nan)\n",
    "    df_geoTest = df_geoTest.replace('(not set)', np.nan)\n",
    "    print(df_geoTest.shape)\n",
    "    \n",
    "    df_dateHourTest = df_dateHour.replace('', np.nan)\n",
    "    df_dateHourTest = df_dateHourTest.replace('(not set)', np.nan)\n",
    "    print(df_dateHourTest.shape)\n",
    "\n",
    "\n",
    "    csvFilePath = \"output/csv/reference/\"\n",
    "    if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "    fileName = \"geographyInfo.csv\"\n",
    "    if not df_geoTest.empty:\n",
    "        appendDFToCSV(df_geoTest, csvFilePath, fileName, sep)\n",
    "    \n",
    "    csvFilePath = \"output/csv/reference/\"\n",
    "    if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "    fileName = \"dateInfo.csv\"\n",
    "    if not df_dateHourTest.empty:\n",
    "        appendDFToCSV(df_dateHourTest, csvFilePath, fileName, sep)\n",
    "\n",
    "    # -------------------------------------------\n",
    "    \n",
    "    # Read in and merge geographyInfo with geographyCountryCodes \n",
    "    pathReferenceGeography=\"/Users/emil/miniforge3/envs/googleapi/Notebooks/Analytics/output/csv/reference/\"\n",
    "    #https://github.com/stefangabos/world_countries/blob/master/data/countries/en/world.csv\n",
    "    fileNameGeoMaster = \"countryCodes.csv\"\n",
    "    fileNameGeoInfo = \"geographyInfo.csv\"\n",
    "    \n",
    "    countryCodes = pathReferenceGeography+fileNameGeoMaster\n",
    "    geoInfo = pathReferenceGeography+fileNameGeoInfo\n",
    "    \n",
    "    dfCountryCodes = pd.read_csv(countryCodes, dtype='object')\n",
    "    dfGeoInfo = pd.read_csv(geoInfo, dtype='object')\n",
    "    \n",
    "    dfCountryCodes['alpha2'] = dfCountryCodes['alpha2'].str.upper()\n",
    "    dfCountryCodes['alpha3'] = dfCountryCodes['alpha3'].str.upper()\n",
    "    \n",
    "    dfCountryCodes.rename(columns={'alpha2': 'countryId'}, inplace=True)\n",
    "    dfCountryCodes.rename(columns={'alpha3': 'countryIdLong'}, inplace=True)\n",
    "    dfCountryCodes.rename(columns={'name': 'countryRef'}, inplace=True)\n",
    "    \n",
    "    dfCountryCodes = dfCountryCodes.drop(columns=['id'])\n",
    "    \n",
    "    dfMerged = dfCountryCodes.merge(dfGeoInfo, how='outer', on='countryId')\n",
    "    \n",
    "    # Rearrange columns\n",
    "    dfMerged = dfMerged[dfMerged.columns[[3,4,0,1,2,5,6,7,8]]]\n",
    "    \n",
    "    # Replace with NaN\n",
    "    dfMerged = dfMerged.replace('', np.nan)\n",
    "    dfMerged = dfMerged.replace('(not set)', np.nan)\n",
    "    \n",
    "    # Sort columns\n",
    "    sortOrder = True\n",
    "    dfSorted = dfMerged.sort_values(list(dfMerged.columns.values), ascending=sortOrder)\n",
    "    \n",
    "    \n",
    "    # Write to and overwrite file\n",
    "    sep = ','\n",
    "    csvFilePath = \"output/csv/reference/\"\n",
    "    if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "    fileName = \"geographyReference.csv\"\n",
    "    fileNameFull = csvFilePath + fileName\n",
    "    \n",
    "    if not dfSorted.empty:\n",
    "        dfSorted.to_csv(fileNameFull, mode='w', index=0, sep=sep)        \n",
    "    \n",
    "    # -------------------------------------------\n",
    "    \n",
    "    ## Table with all dates, not just those with fetched data, as reference\n",
    "    \n",
    "    # Date range\n",
    "    dates = pd.date_range('2022-01-01', '2030-01-01', freq=\"H\",inclusive='left')\n",
    "    \n",
    "    # List to Dataframe\n",
    "    dateReference = pd.DataFrame(dates, columns=['dateHour'])\n",
    "    \n",
    "    # Running function sortOut_dateHour\n",
    "    dateReference = dates_funcs.sortOut_dateHour(dateReference)\n",
    "    \n",
    "    csvFilePath = \"output/csv/reference/\"\n",
    "    fileName = \"dateReference.csv\"\n",
    "    fileNameFull = csvFilePath + fileName\n",
    "    \n",
    "    # If file does not alreay exist, write to file\n",
    "    if not os.path.isfile(fileNameFull):\n",
    "        dateReference.to_csv(fileNameFull, mode='w', index=0, sep=sep)\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    ## Max time to latestDatefile\n",
    "    \n",
    "    # Find largest date in dataframes, looking at df eventName, which should have all dates. (?)\n",
    "    maxTimestamp = dfs_merged_fillNA[14][\"dateFull\"].max()\n",
    "    maxTimestampString = datetime.datetime.strftime(maxTimestamp,'%Y-%m-%d')\n",
    "    maxDate = datetime.datetime.strptime(maxTimestampString,'%Y-%m-%d').date()\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"Fetched data until \"+maxTimestampString)\n",
    "    \n",
    "    # If date is larger than priviously max date.\n",
    "    if maxDate > maxSavedDateDT:\n",
    "    # Write largest date to file\n",
    "        f = open(\"latestDate.txt\", 'w')\n",
    "        f.write(maxTimestampString)\n",
    "        f.close()\n",
    "    \n",
    "    print(\"Written to .csv files\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR\")\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Footnote 1\n",
    "print(\n",
    "'''The following License applies to gapandas4 ONLY:\n",
    "\n",
    "MIT License\n",
    "Copyright (c) 2018\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \\\"Software\\\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \\\"AS IS\\\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "\n",
    "'''\n",
    ")\n",
    "# Now to print to log when script completed\n",
    "\n",
    "nowDT = datetime.datetime.now()\n",
    "now = datetime.datetime.strftime(nowDT,'%Y-%m-%d_%H:%M:%S')\n",
    "print(\"Script finished: \"+now)\n",
    "print(\"\\n\")\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9e8da910-43bb-4d2d-bebb-984325ed2801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "dfs_merged_fillNA2= dfs_merged_fillNA.copy()\n",
    "\n",
    "dfs_merged_by_Year = pd.DataFrame()\n",
    "dfs_merged_by_Month = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "219a0b22-cf2a-41fe-964f-f2bb879739b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by years\n",
    "for n in range(len(dimensions)):\n",
    "    if not dfs_merged_fillNA2[n].empty:\n",
    "        dfs_merged_by_Year[n] = dates_funcs.split_years(dfs_merged_fillNA2[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e03838d6-47aa-4e46-a2af-e33c839a0b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add for earch year add list to monthlist\n",
    "for n in range(len(dimensions)):\n",
    "    if not dfs_merged_by_Year[n].empty:\n",
    "        # List of lists\n",
    "        dfs_merged_by_Month[n] = [[] for _ in range(len(dfs_merged_by_Year[n]))]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c3984fbf-547a-4af1-9a4c-6785dbfcbbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by month\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "for n in range(len(dimensions)):\n",
    "    for x in range(len(dfs_merged_by_Year)):\n",
    "        dfs_merged_by_Month[n][x] = dates_funcs.split_months(dfs_merged_by_Year[n][x])\n",
    "\n",
    "pd.options.mode.chained_assignment = 'warn' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7c14f061-3d2a-4209-b608-f8117a4b66ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten df_by_months[x][y] (two levels) structure to array of dataframes (one level)\n",
    "for n in range(len(dimensions)):\n",
    "    dfs_array[n] = list(itertools.chain.from_iterable(dfs_merged_by_Month[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9e0f7359-29d4-47e9-93d8-d2110e92f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First INDEX is Dimension, second INDEX is month,\n",
    "#dfs_array[16][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "df3a406f-7022-4672-bd50-9ae23393d0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFilePath = \"output/csv/\"\n",
    "fileNameList = [[] for _ in range(len(dimensions))]   \n",
    "sep = \",\"\n",
    "    \n",
    "for n in range(len(dimensions)):\n",
    "    for x in range(len(dfs_array)):\n",
    "        if not dfs_array[n][x].empty:\n",
    "            csvFilePath = \"output/csv/metrics/\"+dimensions[n]+\"/\"\n",
    "            if not os.path.exists(csvFilePath):\n",
    "                os.makedirs(csvFilePath)\n",
    "            dfs_array[n][x] = dfs_array[n][x].astype(str)\n",
    "            fileNameList[n].append(dfs_array[n][x]['year'].min()+'_'+dfs_array[n][x]['month'].min())\n",
    "            dfs_array[n][x].drop(columns=['year'],inplace=True)\n",
    "            dfs_array[n][x].drop(columns=['month'],inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ee880d3b-7715-4f88-926c-8206f312549d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/csv/metrics/language/language_2022_12.csv\n",
      "output/csv/metrics/language/language_2023_1.csv\n",
      "output/csv/metrics/languageCode/languageCode_2022_12.csv\n",
      "output/csv/metrics/languageCode/languageCode_2023_1.csv\n",
      "output/csv/metrics/browser/browser_2022_12.csv\n",
      "output/csv/metrics/browser/browser_2023_1.csv\n",
      "output/csv/metrics/deviceCategory/deviceCategory_2022_12.csv\n",
      "output/csv/metrics/deviceCategory/deviceCategory_2023_1.csv\n",
      "output/csv/metrics/mobileDeviceBranding/mobileDeviceBranding_2022_12.csv\n",
      "output/csv/metrics/mobileDeviceBranding/mobileDeviceBranding_2023_1.csv\n",
      "output/csv/metrics/mobileDeviceMarketingName/mobileDeviceMarketingName_2022_12.csv\n",
      "output/csv/metrics/mobileDeviceMarketingName/mobileDeviceMarketingName_2023_1.csv\n",
      "output/csv/metrics/mobileDeviceModel/mobileDeviceModel_2022_12.csv\n",
      "output/csv/metrics/mobileDeviceModel/mobileDeviceModel_2023_1.csv\n",
      "output/csv/metrics/operatingSystemWithVersion/operatingSystemWithVersion_2022_12.csv\n",
      "output/csv/metrics/operatingSystemWithVersion/operatingSystemWithVersion_2023_1.csv\n",
      "output/csv/metrics/platform/platform_2022_12.csv\n",
      "output/csv/metrics/platform/platform_2023_1.csv\n",
      "output/csv/metrics/screenResolution/screenResolution_2022_12.csv\n",
      "output/csv/metrics/screenResolution/screenResolution_2023_1.csv\n",
      "output/csv/metrics/firstUserDefaultChannelGroup/firstUserDefaultChannelGroup_2022_12.csv\n",
      "output/csv/metrics/firstUserDefaultChannelGroup/firstUserDefaultChannelGroup_2023_1.csv\n",
      "output/csv/metrics/firstUserSource/firstUserSource_2022_12.csv\n",
      "output/csv/metrics/firstUserSource/firstUserSource_2023_1.csv\n",
      "output/csv/metrics/sessionDefaultChannelGroup/sessionDefaultChannelGroup_2022_12.csv\n",
      "output/csv/metrics/sessionDefaultChannelGroup/sessionDefaultChannelGroup_2023_1.csv\n",
      "output/csv/metrics/sessionSource/sessionSource_2022_12.csv\n",
      "output/csv/metrics/sessionSource/sessionSource_2023_1.csv\n",
      "output/csv/metrics/eventName/eventName_2022_12.csv\n",
      "output/csv/metrics/eventName/eventName_2023_1.csv\n",
      "output/csv/metrics/pagePath/pagePath_2022_12.csv\n",
      "output/csv/metrics/pagePath/pagePath_2023_1.csv\n",
      "output/csv/metrics/linkUrl/linkUrl_2022_12.csv\n",
      "output/csv/metrics/linkUrl/linkUrl_2023_1.csv\n",
      "output/csv/metrics/landingPage/landingPage_2022_12.csv\n",
      "output/csv/metrics/landingPage/landingPage_2023_1.csv\n",
      "output/csv/metrics/pageReferrer/pageReferrer_2022_12.csv\n",
      "output/csv/metrics/pageReferrer/pageReferrer_2023_1.csv\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(dimensions)):\n",
    "    for x, y in zip(range(len(dfs_array[n])),fileNameList[n]):\n",
    "        if not dfs_array[n][x].empty:\n",
    "            csvFilePath = \"output/csv/metrics/\"+dimensions[n]+\"/\"\n",
    "            fileName = dimensions[n]+f'_{y}.csv'\n",
    "            appendDFToCSV(dfs_array[n][x], csvFilePath, fileName, sep)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "54ec59f1-3e33-47e9-80c5-f7a3e4ac6e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dfs_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed7ebc0-697c-4c7b-91bc-813cd83531da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

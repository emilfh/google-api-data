{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f24eb0a0-83c0-49cd-b1aa-8019b7f95db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script started: 2023-06-16_01:17:10\n",
      "\n",
      "\n",
      "Last date saved: 2023-06-14\n",
      "\n",
      "\n",
      "Fetching new data, starting: 2023-06-15\n",
      "up to and including: 2023-06-15\n",
      "(83, 22)\n",
      "(89, 22)\n",
      "(85, 22)\n",
      "(82, 22)\n",
      "(87, 22)\n",
      "(80, 22)\n",
      "(92, 22)\n",
      "(97, 22)\n",
      "(74, 22)\n",
      "(100, 22)\n",
      "(103, 22)\n",
      "(106, 22)\n",
      "(97, 22)\n",
      "(100, 22)\n",
      "(253, 22)\n",
      "(137, 22)\n",
      "(78, 22)\n",
      "(118, 22)\n",
      "(125, 22)\n",
      "(0, 20)\n",
      "(0, 20)\n",
      "(0, 19)\n",
      "(0, 19)\n",
      "(227, 6)\n",
      "(279, 6)\n",
      "(277, 6)\n",
      "(243, 6)\n",
      "(315, 6)\n",
      "(386, 6)\n",
      "(473, 6)\n",
      "(556, 6)\n",
      "(179, 6)\n",
      "(623, 6)\n",
      "(241, 6)\n",
      "(270, 6)\n",
      "(231, 6)\n",
      "(261, 6)\n",
      "(815, 6)\n",
      "(709, 6)\n",
      "(245, 6)\n",
      "(595, 6)\n",
      "(502, 6)\n",
      "(0, 6)\n",
      "(0, 6)\n",
      "(0, 5)\n",
      "(0, 5)\n",
      "(489, 6)\n",
      "(650, 6)\n",
      "(624, 6)\n",
      "(482, 6)\n",
      "(764, 6)\n",
      "(1028, 6)\n",
      "(1379, 6)\n",
      "(1525, 6)\n",
      "(333, 6)\n",
      "(1653, 6)\n",
      "(467, 6)\n",
      "(545, 6)\n",
      "(457, 6)\n",
      "(538, 6)\n",
      "(1529, 6)\n",
      "(1791, 6)\n",
      "(510, 6)\n",
      "(1495, 6)\n",
      "(1246, 6)\n",
      "(0, 6)\n",
      "(0, 6)\n",
      "(0, 5)\n",
      "(0, 5)\n",
      "output/csv/language_dateHour_cityId.csv\n",
      "output/csv/languageCode_dateHour_cityId.csv\n",
      "output/csv/browser_dateHour_cityId.csv\n",
      "output/csv/deviceCategory_dateHour_cityId.csv\n",
      "output/csv/mobileDeviceBranding_dateHour_cityId.csv\n",
      "output/csv/mobileDeviceMarketingName_dateHour_cityId.csv\n",
      "output/csv/mobileDeviceModel_dateHour_cityId.csv\n",
      "output/csv/operatingSystemWithVersion_dateHour_cityId.csv\n",
      "output/csv/platform_dateHour_cityId.csv\n",
      "output/csv/screenResolution_dateHour_cityId.csv\n",
      "output/csv/firstUserDefaultChannelGroup_dateHour_cityId.csv\n",
      "output/csv/firstUserSource_dateHour_cityId.csv\n",
      "output/csv/sessionDefaultChannelGroup_dateHour_cityId.csv\n",
      "output/csv/sessionSource_dateHour_cityId.csv\n",
      "output/csv/eventName_dateHour_cityId.csv\n",
      "output/csv/pagePath_dateHour_cityId.csv\n",
      "output/csv/linkUrl_dateHour_cityId.csv\n",
      "output/csv/landingPage_dateHour_cityId.csv\n",
      "output/csv/pageReferrer_dateHour_cityId.csv\n",
      "output/csv/userGender_date_countryId.csv\n",
      "output/csv/brandingInterest_date_countryId.csv\n",
      "output/csv/userAgeBracket_date.csv\n",
      "output/csv/userAgeBracket_countryId.csv\n",
      "output/csv/loyalty/language_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/languageCode_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/browser_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/deviceCategory_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/mobileDeviceBranding_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/mobileDeviceMarketingName_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/mobileDeviceModel_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/operatingSystemWithVersion_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/platform_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/screenResolution_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/firstUserDefaultChannelGroup_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/firstUserSource_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/sessionDefaultChannelGroup_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/sessionSource_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/eventName_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/pagePath_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/linkUrl_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/landingPage_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/pageReferrer_dateHour_cityId_loyalty.csv\n",
      "output/csv/loyalty/userGender_date_countryId_loyalty.csv\n",
      "output/csv/loyalty/brandingInterest_date_countryId_loyalty.csv\n",
      "output/csv/loyalty/userAgeBracket_date_loyalty.csv\n",
      "output/csv/loyalty/userAgeBracket_countryId_loyalty.csv\n",
      "output/csv/activity/language_dateHour_cityId_activity.csv\n",
      "output/csv/activity/languageCode_dateHour_cityId_activity.csv\n",
      "output/csv/activity/browser_dateHour_cityId_activity.csv\n",
      "output/csv/activity/deviceCategory_dateHour_cityId_activity.csv\n",
      "output/csv/activity/mobileDeviceBranding_dateHour_cityId_activity.csv\n",
      "output/csv/activity/mobileDeviceMarketingName_dateHour_cityId_activity.csv\n",
      "output/csv/activity/mobileDeviceModel_dateHour_cityId_activity.csv\n",
      "output/csv/activity/operatingSystemWithVersion_dateHour_cityId_activity.csv\n",
      "output/csv/activity/platform_dateHour_cityId_activity.csv\n",
      "output/csv/activity/screenResolution_dateHour_cityId_activity.csv\n",
      "output/csv/activity/firstUserDefaultChannelGroup_dateHour_cityId_activity.csv\n",
      "output/csv/activity/firstUserSource_dateHour_cityId_activity.csv\n",
      "output/csv/activity/sessionDefaultChannelGroup_dateHour_cityId_activity.csv\n",
      "output/csv/activity/sessionSource_dateHour_cityId_activity.csv\n",
      "output/csv/activity/eventName_dateHour_cityId_activity.csv\n",
      "output/csv/activity/pagePath_dateHour_cityId_activity.csv\n",
      "output/csv/activity/linkUrl_dateHour_cityId_activity.csv\n",
      "output/csv/activity/landingPage_dateHour_cityId_activity.csv\n",
      "output/csv/activity/pageReferrer_dateHour_cityId_activity.csv\n",
      "output/csv/activity/userGender_date_countryId_activity.csv\n",
      "output/csv/activity/brandingInterest_date_countryId_activity.csv\n",
      "output/csv/activity/userAgeBracket_date_activity.csv\n",
      "output/csv/activity/userAgeBracket_countryId_activity.csv\n",
      "(54, 7)\n",
      "(14, 11)\n",
      "output/csv/reference/geographyInfo.csv\n",
      "output/csv/reference/dateInfo.csv\n",
      "Fetched data until 2023-06-15\n",
      "Written to .csv files\n",
      "\n",
      "\n",
      "The following License applies to gapandas4 ONLY:\n",
      "\n",
      "MIT License\n",
      "Copyright (c) 2018\n",
      "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
      "of this software and associated documentation files (the \"Software\"), to deal\n",
      "in the Software without restriction, including without limitation the rights\n",
      "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
      "copies of the Software, and to permit persons to whom the Software is\n",
      "furnished to do so, subject to the following conditions:\n",
      "The above copyright notice and this permission notice shall be included in all\n",
      "copies or substantial portions of the Software.\n",
      "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
      "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
      "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
      "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
      "SOFTWARE.\n",
      "\n",
      "\n",
      "Script finished: 2023-06-16_01:18:58\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "CPU times: user 12.6 s, sys: 742 ms, total: 13.3 s\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Import Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gapandas4 as gp # See Footnote 1 bottom of page\n",
    "import os\n",
    "import datetime\n",
    "from datetime import date\n",
    "# Written functions\n",
    "\n",
    "import sys\n",
    "# adding Notebooksfolder to the system path\n",
    "sys.path.insert(0, '/Users/emil/miniforge3/envs/googleapi/Notebooks')\n",
    "\n",
    "import importlib\n",
    "import dates_funcs\n",
    "importlib.reload(dates_funcs)\n",
    "from dates_funcs import appendDFToCSV\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Now to print to log when program running \n",
    "nowDT = datetime.datetime.now()\n",
    "now = datetime.datetime.strftime(nowDT,'%Y-%m-%d_%H:%M:%S')\n",
    "print(\"Script started: \"+now)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Check of Dates\n",
    "f = open(\"latestDate.txt\")\n",
    "maxSavedDate = f.read()\n",
    "f.close()\n",
    "#print(\"Previously fetched data up to and including: \"+ maxSavedDate)\n",
    "maxSavedDateDT = datetime.datetime.strptime(maxSavedDate,'%Y-%m-%d').date()\n",
    "print(\"Last date saved: \"+maxSavedDate)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Todays date\n",
    "todayDT = date.today()\n",
    "today = datetime.datetime.strftime(todayDT,'%Y-%m-%d')\n",
    "\n",
    "# start_date as the next day as maxSavedDate\n",
    "start_dateDT = maxSavedDateDT + datetime.timedelta(days=1)\n",
    "start_date = datetime.datetime.strftime(start_dateDT,'%Y-%m-%d')\n",
    "print(\"Fetching new data, starting: \"+start_date)\n",
    "\n",
    "# # end_date as the same day as start_date, to get data for one days\n",
    "#end_dateDT = start_dateDT + datetime.timedelta(days=1)\n",
    "end_dateDT = start_dateDT\n",
    "end_date = datetime.datetime.strftime(end_dateDT,'%Y-%m-%d')\n",
    "print(\"up to and including: \"+end_date)\n",
    "print(\"\\n\")\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#MASTER IF\n",
    "\n",
    "if start_dateDT > maxSavedDateDT and todayDT > end_dateDT and todayDT > maxSavedDateDT:\n",
    "    \n",
    "    # START BACKUP or previous .csv files to an archive\n",
    "    # DISABLED because taking up lots of space, will enable manually once in a while instead\n",
    "    '''\n",
    "    import pathlib\n",
    "    import zipfile\n",
    "    from zipfile import ZipFile, ZIP_LZMA\n",
    "    \n",
    "    directory = pathlib.Path(\"output/\")\n",
    "\n",
    "    try:\n",
    "        with ZipFile(\"backup_date:_\"+maxSavedDate+\"_written:_\"+now+\".zip\", mode=\"w\",compression=ZIP_LZMA, allowZip64=True) as archive:\n",
    "            for file_path in directory.rglob(\"*\"):\n",
    "                archive.write(\n",
    "                    file_path,\n",
    "                    arcname=file_path.relative_to(directory)\n",
    "                )\n",
    "            print(\"Previous .csv-files backed up to: backup_until:_\"+maxSavedDate+\"_written:_\"+now+\".zip\")\n",
    "    except BadZipFile as error:\n",
    "        print(error)\n",
    "    '''\n",
    "    # END BACKUP    \n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # CREDENTIALS\n",
    "    property_id = \"298727788\"\n",
    "    credentials_json_path=\"/Users/emil/miniforge3/envs/googleapi/Notebooks/Analytics/client_secrets.json\"\n",
    "    service_account = credentials_json_path\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # LISTS DIMENSIONS AND METRICS\n",
    "    # Dimensions\n",
    "    dimensions = [\"language\",\"languageCode\",\"browser\",\"deviceCategory\",\"mobileDeviceBranding\",\n",
    "                  \"mobileDeviceMarketingName\",\"mobileDeviceModel\",\"operatingSystemWithVersion\",\n",
    "                  \"platform\",\"screenResolution\",\"firstUserDefaultChannelGroup\",\"firstUserSource\",\n",
    "                  \"sessionDefaultChannelGroup\",\"sessionSource\",\"eventName\",\"pagePath\",\"linkUrl\",\n",
    "                  \"landingPage\",\"pageReferrer\"]\n",
    "    \n",
    "    # Does not work with cityId, only countryId\n",
    "    # does not work with dateHour only date.\n",
    "    dimensionsSpecial = [\"userGender\",\"brandingInterest\"]\n",
    "    \n",
    "    # Does not work with cityId, only countryId,\n",
    "    # does not work with dateHour only date:\n",
    "    # also cant request cityId and date in same request.\n",
    "    dimensionsSpecialSpecial = [\"userAgeBracket\"]\n",
    "    \n",
    "    dateHourList = [\"dateHour\"]\n",
    "    dateList = [\"date\"]\n",
    "    cityIdList = [\"cityId\"]\n",
    "    countryIdList = [\"countryId\"]\n",
    "    \n",
    "    # Metrics with empty first place and repeat last to count 1-18\n",
    "    metricList = [\"\",\"totalUsers\",\"newUsers\",\"activeUsers\",\"userEngagementDuration\",\"scrolledUsers\",\n",
    "                  \"averageSessionDuration\",\"bounceRate\",\"engagedSessions\",\"engagementRate\",\"sessions\",\n",
    "                  \"sessionsPerUser\",\"eventCount\",\"eventCountPerUser\",\"eventsPerSession\",\"screenPageViews\",\n",
    "                  \"screenPageViewsPerSession\",\"screenPageViewsPerUser\",\"totalUsers\"]\n",
    "        \n",
    "    loyaltyList = [\"wauPerMau\",\"dauPerMau\",\"dauPerWau\"]\n",
    "        \n",
    "    activityList = [\"active28DayUsers\",\"active7DayUsers\",\"activeUsers\"]\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # REQUESTS\n",
    "    \n",
    "    ### dimensions\n",
    "    \n",
    "    #### metricList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs = [[],[],[],]\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensions)):\n",
    "        for y, z in zip([1,7,13],[0,1,2]): \n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=dateHourList[0]),\n",
    "                    gp.Dimension(name=cityIdList[0]),\n",
    "                    gp.Dimension(name=dimensions[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=metricList[y]),\n",
    "                    gp.Metric(name=metricList[y+1]),\n",
    "                    gp.Metric(name=metricList[y+2]),\n",
    "                    gp.Metric(name=metricList[y+3]),\n",
    "                    gp.Metric(name=metricList[y+4]),\n",
    "                    gp.Metric(name=metricList[y+5]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs[z].append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "            \n",
    "    #### loyaltyList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_l = []\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensions)):\n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=dateList[0]),\n",
    "                    gp.Dimension(name=cityIdList[0]),\n",
    "                    gp.Dimension(name=dimensions[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=loyaltyList[0]),\n",
    "                    gp.Metric(name=loyaltyList[1]),\n",
    "                    gp.Metric(name=loyaltyList[2]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_l.append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "        \n",
    "    #### activityList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_a = []\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensions)):\n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=dateList[0]),\n",
    "                    gp.Dimension(name=cityIdList[0]),\n",
    "                    gp.Dimension(name=dimensions[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=activityList[0]),\n",
    "                    gp.Metric(name=activityList[1]),\n",
    "                    gp.Metric(name=activityList[2]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_a.append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "    \n",
    "        \n",
    "    ### dimensionsSpecial\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_s = [[],[],[],]\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensionsSpecial)):\n",
    "        for y, z in zip([1,7,13],[0,1,2]): \n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=dateList[0]),\n",
    "                    gp.Dimension(name=countryIdList[0]),\n",
    "                    gp.Dimension(name=dimensionsSpecial[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=metricList[y]),\n",
    "                    gp.Metric(name=metricList[y+1]),\n",
    "                    gp.Metric(name=metricList[y+2]),\n",
    "                    gp.Metric(name=metricList[y+3]),\n",
    "                    gp.Metric(name=metricList[y+4]),\n",
    "                    gp.Metric(name=metricList[y+5]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_s[z].append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "            \n",
    "    #### loyaltyList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_s_l = []\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensionsSpecial)):\n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=dateList[0]),\n",
    "                    gp.Dimension(name=countryIdList[0]),\n",
    "                    gp.Dimension(name=dimensionsSpecial[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=loyaltyList[0]),\n",
    "                    gp.Metric(name=loyaltyList[1]),\n",
    "                    gp.Metric(name=loyaltyList[2]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_s_l.append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "        \n",
    "    #### activityList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_s_a = []\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensionsSpecial)):\n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=dateList[0]),\n",
    "                    gp.Dimension(name=countryIdList[0]),\n",
    "                    gp.Dimension(name=dimensionsSpecial[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=activityList[0]),\n",
    "                    gp.Metric(name=activityList[1]),\n",
    "                    gp.Metric(name=activityList[2]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_s_a.append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "    \n",
    "        \n",
    "    ### dimensionsSpecialSpecial date\n",
    "    \n",
    "    #### metricList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_ssd = [[],[],[],]\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        for y, z in zip([1,7,13],[0,1,2]): \n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=dateList[0]),\n",
    "                    gp.Dimension(name=dimensionsSpecialSpecial[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=metricList[y]),\n",
    "                    gp.Metric(name=metricList[y+1]),\n",
    "                    gp.Metric(name=metricList[y+2]),\n",
    "                    gp.Metric(name=metricList[y+3]),\n",
    "                    gp.Metric(name=metricList[y+4]),\n",
    "                    gp.Metric(name=metricList[y+5]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_ssd[z].append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "            \n",
    "    #### loyaltyList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_ssd_l = []\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=dateList[0]),\n",
    "                    gp.Dimension(name=dimensionsSpecialSpecial[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=loyaltyList[0]),\n",
    "                    gp.Metric(name=loyaltyList[1]),\n",
    "                    gp.Metric(name=loyaltyList[2]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_ssd_l.append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "        \n",
    "    #### activityList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_ssd_a = []\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=dateList[0]),\n",
    "                    gp.Dimension(name=dimensionsSpecialSpecial[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=activityList[0]),\n",
    "                    gp.Metric(name=activityList[1]),\n",
    "                    gp.Metric(name=activityList[2]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_ssd_a.append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "    \n",
    "        \n",
    "    ### dimensionsSpecialSpecial countryId\n",
    "    \n",
    "    #### metricList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_ssc = [[],[],[],]\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        for y, z in zip([1,7,13],[0,1,2]): \n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=countryIdList[0]),\n",
    "                    gp.Dimension(name=dimensionsSpecialSpecial[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=metricList[y]),\n",
    "                    gp.Metric(name=metricList[y+1]),\n",
    "                    gp.Metric(name=metricList[y+2]),\n",
    "                    gp.Metric(name=metricList[y+3]),\n",
    "                    gp.Metric(name=metricList[y+4]),\n",
    "                    gp.Metric(name=metricList[y+5]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_ssc[z].append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "            \n",
    "    #### loyaltyList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_ssc_l = []\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=countryIdList[0]),\n",
    "                    gp.Dimension(name=dimensionsSpecialSpecial[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=loyaltyList[0]),\n",
    "                    gp.Metric(name=loyaltyList[1]),\n",
    "                    gp.Metric(name=loyaltyList[2]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_ssc_l.append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "        \n",
    "    #### activityList\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    dfs_ssc_a = []\n",
    "    \n",
    "    # Request for all dimensions in list\n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "    \n",
    "            report_request = gp.RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                dimensions=[\n",
    "                    gp.Dimension(name=countryIdList[0]),\n",
    "                    gp.Dimension(name=dimensionsSpecialSpecial[x])\n",
    "                ],\n",
    "                metrics=[\n",
    "                    gp.Metric(name=activityList[0]),\n",
    "                    gp.Metric(name=activityList[1]),\n",
    "                    gp.Metric(name=activityList[2]),\n",
    "                ],\n",
    "                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "            )\n",
    "            \n",
    "            # Perform query and append to list\n",
    "            dfs_ssc_a.append(gp.query(service_account, report_request, report_type=\"report\"))\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "    ## Formatting\n",
    "    \n",
    "    #### metricList\n",
    "    \n",
    "    # Drop the extra Totalusers column, since it exists twice in the tables to merge\n",
    "    # since metricList contained it twice.\n",
    "    \n",
    "    for x in range(len(dimensions)):\n",
    "        for z in [2]:\n",
    "            dfs[z][x].drop(columns=['totalUsers'],inplace=True)\n",
    "    \n",
    "    for x in range(len(dimensionsSpecial)):\n",
    "        for z in [2]:\n",
    "            dfs_s[z][x].drop(columns=['totalUsers'],inplace=True)\n",
    "    \n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        for z in [2]:\n",
    "            dfs_ssd[z][x].drop(columns=['totalUsers'],inplace=True)\n",
    "    \n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        for z in [2]:\n",
    "            dfs_ssc[z][x].drop(columns=['totalUsers'],inplace=True)\n",
    "            \n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "    ## Merge\n",
    "    \n",
    "    dfs_merged = list(range(len(dimensions)))\n",
    "    \n",
    "    for x in range(len(dimensions)):\n",
    "        mergeOn = ['dateHour','cityId'] \n",
    "        mergeDim = [dimensions[x]]\n",
    "        mergeOn =  mergeOn + mergeDim\n",
    "        dfs_merged[x] = dfs[0][x].merge(dfs[1][x], how='outer', on=mergeOn)\n",
    "    \n",
    "    for n in range(len(dimensions)):\n",
    "        mergeOn = ['dateHour','cityId'] \n",
    "        mergeDim = [dimensions[n]]\n",
    "        mergeOn =  mergeOn + mergeDim\n",
    "        dfs_merged[n] = dfs_merged[n].merge(dfs[2][n], how='outer', on=mergeOn)\n",
    "    \n",
    "    \n",
    "    dfs_s_merged = list(range(len(dimensionsSpecial)))\n",
    "    \n",
    "    for x in range(len(dimensionsSpecial)):\n",
    "        mergeOn = ['date','countryId'] \n",
    "        mergeDim = [dimensionsSpecial[x]]\n",
    "        mergeOn =  mergeOn + mergeDim\n",
    "        dfs_s_merged[x] = dfs_s[0][x].merge(dfs_s[1][x], how='outer', on=mergeOn)\n",
    "    \n",
    "    for n in range(len(dimensionsSpecial)):\n",
    "        mergeOn = ['date','countryId'] \n",
    "        mergeDim = [dimensionsSpecial[n]]\n",
    "        mergeOn =  mergeOn + mergeDim\n",
    "        dfs_s_merged[n] = dfs_s_merged[n].merge(dfs_s[2][n], how='outer', on=mergeOn)\n",
    "    \n",
    "    \n",
    "    dfs_ssd_merged = list(range(len(dimensionsSpecialSpecial)))\n",
    "    \n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        mergeOn = ['date'] \n",
    "        mergeDim = [dimensionsSpecialSpecial[x]]\n",
    "        mergeOn =  mergeOn + mergeDim\n",
    "        dfs_ssd_merged[x] = dfs_ssd[0][x].merge(dfs_ssd[1][x], how='outer', on=mergeOn)\n",
    "    \n",
    "    for n in range(len(dimensionsSpecialSpecial)):\n",
    "        mergeOn = ['date'] \n",
    "        mergeDim = [dimensionsSpecialSpecial[n]]\n",
    "        mergeOn =  mergeOn + mergeDim\n",
    "        dfs_ssd_merged[n] = dfs_ssd_merged[n].merge(dfs_ssd[2][n], how='outer', on=mergeOn)\n",
    "    \n",
    "    \n",
    "    dfs_ssc_merged = list(range(len(dimensionsSpecialSpecial)))\n",
    "    \n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        mergeOn = ['countryId'] \n",
    "        mergeDim = [dimensionsSpecialSpecial[x]]\n",
    "        mergeOn =  mergeOn + mergeDim\n",
    "        dfs_ssc_merged[x] = dfs_ssc[0][x].merge(dfs_ssc[1][x], how='outer', on=mergeOn)\n",
    "    \n",
    "    for n in range(len(dimensionsSpecialSpecial)):\n",
    "        mergeOn = ['countryId'] \n",
    "        mergeDim = [dimensionsSpecialSpecial[n]]\n",
    "        mergeOn =  mergeOn + mergeDim\n",
    "        dfs_ssc_merged[n] = dfs_ssc_merged[n].merge(dfs_ssc[2][n], how='outer', on=mergeOn)\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Sort out dates and sort columns.\n",
    "        \n",
    "    #### metricList\n",
    "    \n",
    "    for x in range(len(dimensions)):\n",
    "        for z in [0,1,2]: \n",
    "            dfs[z][x] = dates_funcs.sortOut_dateHour_short(dfs[z][x])\n",
    "    \n",
    "    for x in range(len(dimensionsSpecial)):\n",
    "        for z in [0,1,2]: \n",
    "            dfs_s[z][x] = dates_funcs.sortOut_date_short(dfs_s[z][x])\n",
    "            \n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        for z in [0,1,2]: \n",
    "            dfs_ssd[z][x] = dates_funcs.sortOut_date_short(dfs_ssd[z][x])\n",
    "    \n",
    "    #### loyaltyList\n",
    "    \n",
    "    for x in range(len(dimensions)):\n",
    "        dfs_l[x] = dates_funcs.sortOut_date_short(dfs_l[x])\n",
    "    \n",
    "    for x in range(len(dimensionsSpecial)):\n",
    "        dfs_s_l[x] = dates_funcs.sortOut_date_short(dfs_s_l[x])\n",
    "            \n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        dfs_ssd_l[x] = dates_funcs.sortOut_date_short(dfs_ssd_l[x])\n",
    "    \n",
    "    #### activityList\n",
    "    \n",
    "    for x in range(len(dimensions)):\n",
    "        dfs_a[x] = dates_funcs.sortOut_date_short(dfs_a[x])\n",
    "    \n",
    "    for x in range(len(dimensionsSpecial)):\n",
    "        dfs_s_a[x] = dates_funcs.sortOut_date_short(dfs_s_a[x])\n",
    "            \n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        dfs_ssd_a[x] = dates_funcs.sortOut_date_short(dfs_ssd_a[x])\n",
    "    \n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        for z in [0,1,2]: \n",
    "            sortOrder = True\n",
    "            dfs_ssc[z][x] = dfs_ssc[z][x].sort_values(list(dfs_ssc[z][x].columns.values), ascending=sortOrder)\n",
    "            \n",
    "    for x in range(len(dimensionsSpecialSpecial)):\n",
    "        sortOrder = True\n",
    "        dfs_ssc_l[x] = dfs_ssc_l[x].sort_values(list(dfs_ssc_l[x].columns.values), ascending=sortOrder)\n",
    "\n",
    "            for x in range(len(dimensionsSpecialSpecial)):\n",
    "        sortOrder = True\n",
    "        dfs_ssc_a[x] = dfs_ssc_a[x].sort_values(list(dfs_ssc_a[x].columns.values), ascending=sortOrder)\n",
    "\n",
    "        \n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "    ## Sort out NA\n",
    "    \n",
    "    #### metricList\n",
    "    \n",
    "    dfs_merged_fillNA = list(range(len(dimensions)))\n",
    "    \n",
    "    for n in range(len(dimensions)):\n",
    "        dfs_merged_fillNA[n] = dfs_merged[n].replace('', np.nan)\n",
    "        dfs_merged_fillNA[n] = dfs_merged_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_merged_fillNA[n].shape)\n",
    "    \n",
    "    \n",
    "    dfs_s_merged_fillNA = list(range(len(dimensionsSpecial)))\n",
    "    \n",
    "    for n in range(len(dimensionsSpecial)):\n",
    "        dfs_s_merged_fillNA[n] = dfs_s_merged[n].replace('', np.nan)\n",
    "        dfs_s_merged_fillNA[n] = dfs_s_merged_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_s_merged_fillNA[n].shape)\n",
    "    \n",
    "    \n",
    "    dfs_ssd_merged_fillNA = list(range(len(dimensionsSpecialSpecial)))\n",
    "    \n",
    "    for n in range(len(dimensionsSpecialSpecial)):\n",
    "        dfs_ssd_merged_fillNA[n] = dfs_ssd_merged[n].replace('', np.nan)\n",
    "        dfs_ssd_merged_fillNA[n] = dfs_ssd_merged_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_ssd_merged_fillNA[n].shape)\n",
    "    \n",
    "    \n",
    "    dfs_ssc_merged_fillNA = list(range(len(dimensionsSpecialSpecial)))\n",
    "    \n",
    "    for n in range(len(dimensionsSpecialSpecial)):\n",
    "        dfs_ssc_merged_fillNA[n] = dfs_ssc_merged[n].replace('', np.nan)\n",
    "        dfs_ssc_merged_fillNA[n] = dfs_ssc_merged_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_ssc_merged_fillNA[n].shape)\n",
    "    \n",
    "    #### loyaltyList\n",
    "    \n",
    "    dfs_l_fillNA = list(range(len(dimensions)))\n",
    "    \n",
    "    for n in range(len(dimensions)):\n",
    "        dfs_l_fillNA[n] = dfs_l[n].replace('', np.nan)\n",
    "        dfs_l_fillNA[n] = dfs_l_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_l_fillNA[n].shape)\n",
    "    \n",
    "    \n",
    "    dfs_s_l_fillNA = list(range(len(dimensionsSpecial)))\n",
    "    \n",
    "    for n in range(len(dimensionsSpecial)):\n",
    "        dfs_s_l_fillNA[n] = dfs_s_l[n].replace('', np.nan)\n",
    "        dfs_s_l_fillNA[n] = dfs_s_l_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_s_l_fillNA[n].shape)\n",
    "    \n",
    "    \n",
    "    dfs_ssd_l_fillNA = list(range(len(dimensionsSpecialSpecial)))\n",
    "    \n",
    "    for n in range(len(dimensionsSpecialSpecial)):\n",
    "        dfs_ssd_l_fillNA[n] = dfs_ssd_l[n].replace('', np.nan)\n",
    "        dfs_ssd_l_fillNA[n] = dfs_ssd_l_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_ssd_l_fillNA[n].shape)\n",
    "    \n",
    "    \n",
    "    dfs_ssc_l_fillNA = list(range(len(dimensionsSpecialSpecial)))\n",
    "    \n",
    "    for n in range(len(dimensionsSpecialSpecial)):\n",
    "        dfs_ssc_l_fillNA[n] = dfs_ssc_l[n].replace('', np.nan)\n",
    "        dfs_ssc_l_fillNA[n] = dfs_ssc_l_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_ssc_l_fillNA[n].shape)\n",
    "    \n",
    "    #### activityList\n",
    "    \n",
    "    \n",
    "    dfs_a_fillNA = list(range(len(dimensions)))\n",
    "    \n",
    "    for n in range(len(dimensions)):\n",
    "        dfs_a_fillNA[n] = dfs_a[n].replace('', np.nan)\n",
    "        dfs_a_fillNA[n] = dfs_a_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_a_fillNA[n].shape)\n",
    "    \n",
    "    \n",
    "    dfs_s_a_fillNA = list(range(len(dimensionsSpecial)))\n",
    "    \n",
    "    for n in range(len(dimensionsSpecial)):\n",
    "        dfs_s_a_fillNA[n] = dfs_s_a[n].replace('', np.nan)\n",
    "        dfs_s_a_fillNA[n] = dfs_s_a_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_s_a_fillNA[n].shape)\n",
    "    \n",
    "    \n",
    "    dfs_ssd_a_fillNA = list(range(len(dimensionsSpecialSpecial)))\n",
    "    \n",
    "    for n in range(len(dimensionsSpecialSpecial)):\n",
    "        dfs_ssd_a_fillNA[n] = dfs_ssd_a[n].replace('', np.nan)\n",
    "        dfs_ssd_a_fillNA[n] = dfs_ssd_a_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_ssd_a_fillNA[n].shape)\n",
    "    \n",
    "    \n",
    "    dfs_ssc_a_fillNA = list(range(len(dimensionsSpecialSpecial)))\n",
    "    \n",
    "    for n in range(len(dimensionsSpecialSpecial)):\n",
    "        dfs_ssc_a_fillNA[n] = dfs_ssc_a[n].replace('', np.nan)\n",
    "        dfs_ssc_a_fillNA[n] = dfs_ssc_a_fillNA[n].replace('(not set)', np.nan)\n",
    "        print(dfs_ssc_a_fillNA[n].shape)\n",
    "    \n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "    ## Dataframes to .csv\n",
    "    \n",
    "    csvFilePath = \"output/csv/\"\n",
    "    fileName = dimensions[n]\n",
    "    sep = \",\"\n",
    "    \n",
    "    for n in range(len(dfs_merged_fillNA)):\n",
    "        csvFilePath = \"output/csv/\"\n",
    "        fileName = dimensions[n]+\"_dateHour\"+\"_cityId\"+\".csv\" \n",
    "        appendDFToCSV(dfs_merged_fillNA[n], csvFilePath, dimensions[n]+\"_dateHour\"+\"_cityId\"+\".csv\", sep)\n",
    "    \n",
    "    for n in range(len(dfs_s_merged_fillNA)):\n",
    "        csvFilePath = \"output/csv/\"\n",
    "        fileName = dimensionsSpecial[n]+\"_date\"+\"_countryId\"+\".csv\"\n",
    "        appendDFToCSV(dfs_s_merged_fillNA[n], csvFilePath, dimensionsSpecial[n]+\"_date\"+\"_countryId\"+\".csv\", sep)\n",
    "    \n",
    "    for n in range(len(dfs_ssd_merged_fillNA)):\n",
    "        csvFilePath = \"output/csv/\"\n",
    "        fileName = dimensionsSpecialSpecial[n]+\"_date\"\".csv\"\n",
    "        appendDFToCSV(dfs_ssd_merged_fillNA[n], csvFilePath, dimensionsSpecialSpecial[n]+\"_date\"+\".csv\", sep)\n",
    "    \n",
    "    for n in range(len(dfs_ssc_merged_fillNA)):\n",
    "        csvFilePath = \"output/csv/\"\n",
    "        fileName = dimensionsSpecialSpecial[n]+\"_countryId\"+\".csv\"\n",
    "        appendDFToCSV(dfs_ssc_merged_fillNA[n], csvFilePath, dimensionsSpecialSpecial[n]+\"_countryId\"+\".csv\", sep)\n",
    "    \n",
    "    for n in range(len(dfs_l_fillNA)):\n",
    "        csvFilePath = \"output/csv/loyalty/\"\n",
    "        fileName = dimensions[n]+\"_dateHour\"+\"_cityId\"+\"_loyalty\"+\".csv\"\n",
    "        appendDFToCSV(dfs_l_fillNA[n], csvFilePath, dimensions[n]+\"_dateHour\"+\"_cityId\"+\"_loyalty\"+\".csv\", sep)\n",
    "    \n",
    "    for n in range(len(dfs_s_l_fillNA)):\n",
    "        csvFilePath = \"output/csv/loyalty/\"\n",
    "        fileName = dimensionsSpecial[n]+\"_date\"+\"_countryId\"+\"_loyalty\"+\".csv\"\n",
    "        appendDFToCSV(dfs_s_l_fillNA[n], csvFilePath, dimensionsSpecial[n]+\"_date\"+\"_countryId\"+\"_loyalty\"+\".csv\", sep)\n",
    "    \n",
    "    for n in range(len(dfs_ssd_l_fillNA)):\n",
    "        csvFilePath = \"output/csv/loyalty/\"\n",
    "        fileName = dimensionsSpecialSpecial[n]+\"_date\"+\"_loyalty\"+\".csv\"\n",
    "        appendDFToCSV(dfs_ssd_l_fillNA[n], csvFilePath, dimensionsSpecialSpecial[n]+\"_date\"+\"_loyalty\"+\".csv\", sep)\n",
    "    \n",
    "    for n in range(len(dfs_ssc_l_fillNA)):\n",
    "        csvFilePath = \"output/csv/loyalty/\"\n",
    "        fileName = dimensionsSpecialSpecial[n]+\"_countryId\"+\"_loyalty\"+\".csv\"\n",
    "        appendDFToCSV(dfs_ssc_l_fillNA[n], csvFilePath, dimensionsSpecialSpecial[n]+\"_countryId\"+\"_loyalty\"+\".csv\", sep)\n",
    "    \n",
    "    for n in range(len(dfs_a_fillNA)):\n",
    "        csvFilePath = \"output/csv/activity/\"\n",
    "        fileName = dimensions[n]+\"_dateHour\"+\"_cityId\"+\"_activity\"+\".csv\"\n",
    "        appendDFToCSV(dfs_a_fillNA[n], csvFilePath, dimensions[n]+\"_dateHour\"+\"_cityId\"+\"_activity\"+\".csv\", sep)\n",
    "    \n",
    "    for n in range(len(dfs_s_a_fillNA)):\n",
    "        csvFilePath = \"output/csv/activity/\"\n",
    "        fileName = dimensionsSpecial[n]+\"_date\"+\"_countryId\"+\"_activity\"+\".csv\"\n",
    "        appendDFToCSV(dfs_s_a_fillNA[n], csvFilePath, dimensionsSpecial[n]+\"_date\"+\"_countryId\"+\"_activity\"+\".csv\", sep)\n",
    "    \n",
    "    for n in range(len(dfs_ssd_a_fillNA)):\n",
    "        csvFilePath = \"output/csv/activity/\"\n",
    "        fileName = dimensionsSpecialSpecial[n]+\"_date\"+\"_activity\"+\".csv\"\n",
    "        appendDFToCSV(dfs_ssd_a_fillNA[n], csvFilePath, dimensionsSpecialSpecial[n]+\"_date\"+\"_activity\"+\".csv\", sep)\n",
    "    \n",
    "    for n in range(len(dfs_ssc_a_fillNA)):\n",
    "        csvFilePath = \"output/csv/activity/\"\n",
    "        fileName = dimensionsSpecialSpecial[n]+\"_countryId\"+\"_activity\"+\".csv\"\n",
    "        appendDFToCSV(dfs_ssc_a_fillNA[n], csvFilePath, dimensionsSpecialSpecial[n]+\"_countryId\"+\"_activity\"+\".csv\", sep)\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "    ## Date and Geography tables\n",
    "    \n",
    "    # Geography List\n",
    "    geoList = ['continentId','continent','countryId','country','region','cityId','city']\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    \n",
    "    report_request = gp.RunReportRequest(\n",
    "        property=f\"properties/{property_id}\",\n",
    "         dimensions=[\n",
    "             gp.Dimension(name=geoList[0]),\n",
    "             gp.Dimension(name=geoList[1]),\n",
    "             gp.Dimension(name=geoList[2]),\n",
    "             gp.Dimension(name=geoList[3]),\n",
    "             gp.Dimension(name=geoList[4]),\n",
    "             gp.Dimension(name=geoList[5]),\n",
    "             gp.Dimension(name=geoList[6]),\n",
    "           ],\n",
    "           metrics=[\n",
    "            ],\n",
    "            date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "        )\n",
    "        \n",
    "    # Perform query and append to list\n",
    "    df_geo = gp.query(service_account, report_request, report_type=\"report\")\n",
    "\n",
    "    # Sort columns\n",
    "    sortOrder = True\n",
    "    df_geo = df_geo.sort_values(list(df_geo.columns.values), ascending=sortOrder)\n",
    "\n",
    "    \n",
    "    # Date List\n",
    "    dateList = [\"dateHour\"]\n",
    "    \n",
    "    # Empty list to fill with dataframes\n",
    "    \n",
    "    report_request = gp.RunReportRequest(\n",
    "        property=f\"properties/{property_id}\",\n",
    "         dimensions=[\n",
    "             gp.Dimension(name=dateList[0]),\n",
    "           ],\n",
    "           metrics=[\n",
    "            ],\n",
    "            date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n",
    "        )\n",
    "    \n",
    "    # Perform query and append to list\n",
    "    df_dateHour = gp.query(service_account, report_request, report_type=\"report\")\n",
    "\n",
    "    # Sort out dates function\n",
    "    df_dateHour = dates_funcs.sortOut_dateHour(df_dateHour)\n",
    "\n",
    "    \n",
    "    df_geoTest = df_geo.replace('', np.nan)\n",
    "    df_geoTest = df_geoTest.replace('(not set)', np.nan)\n",
    "    print(df_geoTest.shape)\n",
    "    \n",
    "    df_dateHourTest = df_dateHour.replace('', np.nan)\n",
    "    df_dateHourTest = df_dateHourTest.replace('(not set)', np.nan)\n",
    "    print(df_dateHourTest.shape)\n",
    "\n",
    "\n",
    "    csvFilePath = \"output/csv/reference/\"\n",
    "    fileName = \"geographyInfo.csv\"\n",
    "    appendDFToCSV(df_geoTest, csvFilePath, fileName, sep)\n",
    "    \n",
    "    csvFilePath = \"output/csv/reference/\"\n",
    "    fileName = \"dateInfo.csv\"\n",
    "    appendDFToCSV(df_dateHourTest, csvFilePath, fileName, sep)\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # Read in and merge geographyInfo with geographyCountryCodes \n",
    "    pathReferenceGeography=\"/Users/emil/miniforge3/envs/googleapi/Notebooks/Analytics/output/csv/reference/\"\n",
    "    #https://github.com/stefangabos/world_countries/blob/master/data/countries/en/world.csv\n",
    "    fileNameGeoMaster = \"countryCodes.csv\"\n",
    "    fileNameGeoInfo = \"geographyInfo.csv\"\n",
    "    \n",
    "    countryCodes = pathReferenceGeography+fileNameGeoMaster\n",
    "    geoInfo = pathReferenceGeography+fileNameGeoInfo\n",
    "    \n",
    "    dfCountryCodes = pd.read_csv(countryCodes, dtype='object')\n",
    "    dfGeoInfo = pd.read_csv(geoInfo, dtype='object')\n",
    "    \n",
    "    dfCountryCodes['alpha2'] = dfCountryCodes['alpha2'].str.upper()\n",
    "    dfCountryCodes['alpha3'] = dfCountryCodes['alpha3'].str.upper()\n",
    "    \n",
    "    dfCountryCodes.rename(columns={'alpha2': 'countryId'}, inplace=True)\n",
    "    dfCountryCodes.rename(columns={'alpha3': 'countryIdLong'}, inplace=True)\n",
    "    dfCountryCodes.rename(columns={'name': 'countryRef'}, inplace=True)\n",
    "    \n",
    "    dfCountryCodes = dfCountryCodes.drop(columns=['id'])\n",
    "    \n",
    "    dfMerged = dfCountryCodes.merge(dfGeoInfo, how='outer', on='countryId')\n",
    "    \n",
    "    # Rearrange columns\n",
    "    dfMerged = dfMerged[dfMerged.columns[[3,4,0,1,2,5,6,7,8]]]\n",
    "    \n",
    "    # Replace with NaN\n",
    "    dfMerged = dfMerged.replace('', np.nan)\n",
    "    dfMerged = dfMerged.replace('(not set)', np.nan)\n",
    "    \n",
    "    # Sort columns\n",
    "    sortOrder = True\n",
    "    dfSorted = dfMerged.sort_values(list(dfMerged.columns.values), ascending=sortOrder)\n",
    "    \n",
    "    sep = ','\n",
    "    csvFilePath = \"output/csv/reference/\"\n",
    "    fileName = \"geographyReference.csv\"\n",
    "    fileNameFull = csvFilePath + fileName\n",
    "    \n",
    "    # Write to and overwrite file\n",
    "    dfSorted.to_csv(fileNameFull, mode='w', index=0, sep=sep)        \n",
    "    # -------------------------------------------\n",
    "\n",
    "    \n",
    "    ## Table with all dates, not just those with fetched data, as reference\n",
    "    \n",
    "    # Date range\n",
    "    dates = pd.date_range('2022-01-01', '2030-01-01', freq=\"H\",inclusive='left')\n",
    "    \n",
    "    # List to Dataframe\n",
    "    dateReference = pd.DataFrame(dates, columns=['dateHour'])\n",
    "    \n",
    "    # Running function sortOut_dateHour\n",
    "    dateReference = dates_funcs.sortOut_dateHour(dateReference)\n",
    "    \n",
    "    csvFilePath = \"output/csv/reference/\"\n",
    "    fileName = \"dateReference.csv\"\n",
    "    fileNameFull = csvFilePath + fileName\n",
    "    \n",
    "    # If file does not alreay exist, write to file\n",
    "    if not os.path.isfile(fileNameFull):\n",
    "        dateReference.to_csv(fileNameFull, mode='w', index=0, sep=sep)\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    ## Max time to latestDatefile\n",
    "    \n",
    "    # Find largest date in dataframes, looking at df eventName, which should have all dates. (?)\n",
    "    maxTimestamp = dfs_merged_fillNA[14][\"dateFull\"].max()\n",
    "    maxTimestampString = datetime.datetime.strftime(maxTimestamp,'%Y-%m-%d')\n",
    "    maxDate = datetime.datetime.strptime(maxTimestampString,'%Y-%m-%d').date()\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"Fetched data until \"+maxTimestampString)\n",
    "    \n",
    "    # If date is larger than priviously max date.\n",
    "    if maxDate > maxSavedDateDT:\n",
    "    # Write largest date to file\n",
    "        f = open(\"latestDate.txt\", 'w')\n",
    "        f.write(maxTimestampString)\n",
    "        f.close()\n",
    "    \n",
    "    print(\"Written to .csv files\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR\")\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Footnote 1\n",
    "print(\n",
    "'''The following License applies to gapandas4 ONLY:\n",
    "\n",
    "MIT License\n",
    "Copyright (c) 2018\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \\\"Software\\\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \\\"AS IS\\\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "\n",
    "'''\n",
    ")\n",
    "# Now to print to log when script completed\n",
    "\n",
    "nowDT = datetime.datetime.now()\n",
    "now = datetime.datetime.strftime(nowDT,'%Y-%m-%d_%H:%M:%S')\n",
    "print(\"Script finished: \"+now)\n",
    "print(\"\\n\")\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de83e48c-df7b-48a5-bb35-3fe7f3277e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

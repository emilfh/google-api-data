#!/usr/bin/env python
# coding: utf-8

# In[1]:


get_ipython().run_cell_magic('time', '', '\n# Import Packages\nimport pandas as pd\nimport numpy as np\n\nimport gapandas4 as gp # See Footnote 1 bottom of page\nimport os\nimport datetime\nfrom datetime import date\n# Written functions\n\nimport itertools\nimport sys\n# adding Notebooksfolder to the system path\nsys.path.insert(0, \'/Users/emil/miniforge3/envs/googleapi/Notebooks\')\n\nimport importlib\nimport dates_funcs\nimportlib.reload(dates_funcs)\nfrom dates_funcs import appendDFToCSV\n\n# ---------------------------------------------------------------------------------------------------------------------\n\n# Now to print to log when program running \nnowDT = datetime.datetime.now()\nnow = datetime.datetime.strftime(nowDT,\'%Y-%m-%d_%H:%M:%S\')\nprint("Script started: "+now)\nprint("\\n")\n\n# Check of Dates\nf = open("latestDate.txt")\nmaxSavedDate = f.read()\nf.close()\n#print("Previously fetched data up to and including: "+ maxSavedDate)\nmaxSavedDateDT = datetime.datetime.strptime(maxSavedDate,\'%Y-%m-%d\').date()\nprint("Last date saved: "+maxSavedDate)\nprint("\\n")\n\n# Todays date\ntodayDT = date.today()\ntoday = datetime.datetime.strftime(todayDT,\'%Y-%m-%d\')\n\n# start_date as the next day as maxSavedDate\nstart_dateDT = maxSavedDateDT + datetime.timedelta(days=1)\nstart_date = datetime.datetime.strftime(start_dateDT,\'%Y-%m-%d\')\nprint("Fetching new data, starting: "+start_date)\n\n# # end_date as the same day as start_date, to get data for one days\nend_dateDT = start_dateDT + datetime.timedelta(days=3)\n#end_dateDT = start_dateDT \nend_date = datetime.datetime.strftime(end_dateDT,\'%Y-%m-%d\')\nprint("up to and including: "+end_date)\nprint("\\n")\n\n# ---------------------------------------------------------------------------------------------------------------------\n\n#MASTER IF\n\nif start_dateDT > maxSavedDateDT and todayDT > end_dateDT and todayDT > maxSavedDateDT:\n    \n    # START BACKUP or previous .csv files to an archive\n    # DISABLED because taking up lots of space, will enable manually once in a while instead\n    \'\'\'\n    import pathlib\n    import zipfile\n    from zipfile import ZipFile, ZIP_LZMA\n    \n    directory = pathlib.Path("output/")\n\n    try:\n        with ZipFile("backup_date:_"+maxSavedDate+"_written:_"+now+".zip", mode="w",compression=ZIP_LZMA, allowZip64=True) as archive:\n            for file_path in directory.rglob("*"):\n                archive.write(\n                    file_path,\n                    arcname=file_path.relative_to(directory)\n                )\n            print("Previous .csv-files backed up to: backup_until:_"+maxSavedDate+"_written:_"+now+".zip")\n    except BadZipFile as error:\n        print(error)\n    \'\'\'\n    # END BACKUP    \n\n    # ---------------------------------------------------------------------------------------------------------------------\n    \n    # CREDENTIALS\n    property_id = "298727788"\n    credentials_json_path="/Users/emil/miniforge3/envs/googleapi/Notebooks/Analytics/client_secrets.json"\n    service_account = credentials_json_path\n    \n    # ---------------------------------------------------------------------------------------------------------------------\n    \n    # LISTS DIMENSIONS AND METRICS\n    # Dimensions\n    dimensions = ["language","languageCode","browser","deviceCategory","mobileDeviceBranding",\n                  "mobileDeviceMarketingName","mobileDeviceModel","operatingSystemWithVersion",\n                  "platform","screenResolution","firstUserDefaultChannelGroup","firstUserSource",\n                  "sessionDefaultChannelGroup","sessionSource","eventName","pagePath","linkUrl",\n                  "landingPage","pageReferrer"]\n    \n    # Does not work with cityId, only countryId\n    # does not work with dateHour only date.\n    dimensionsSpecial = ["userGender","brandingInterest"]\n    \n    # Does not work with cityId, only countryId,\n    # does not work with dateHour only date:\n    # also cant request cityId and date in same request.\n    dimensionsSpecialSpecial = ["userAgeBracket"]\n    \n    dateHourList = ["dateHour"]\n    dateList = ["date"]\n    cityIdList = ["cityId"]\n    countryIdList = ["countryId"]\n    \n    # Metrics with empty first place and repeat last to count 1-18\n    metricList = ["","totalUsers","newUsers","activeUsers","userEngagementDuration","scrolledUsers",\n                  "averageSessionDuration","bounceRate","engagedSessions","engagementRate","sessions",\n                  "sessionsPerUser","eventCount","eventCountPerUser","eventsPerSession","screenPageViews",\n                  "screenPageViewsPerSession","screenPageViewsPerUser","totalUsers"]\n        \n    loyaltyList = ["wauPerMau","dauPerMau","dauPerWau"]\n        \n    activityList = ["active28DayUsers","active7DayUsers","activeUsers"]\n    \n    # ---------------------------------------------------------------------------------------------------------------------\n    \n    # REQUESTS\n    \n    ### dimensions\n    \n    #### metricList\n    \n    # Empty list to fill with dataframes\n    dfs = [[],[],[],]\n    \n    # Request for all dimensions in list\n    for x in range(len(dimensions)):\n        for y, z in zip([1,7,13],[0,1,2]): \n    \n            report_request = gp.RunReportRequest(\n                property=f"properties/{property_id}",\n                dimensions=[\n                    gp.Dimension(name=dateHourList[0]),\n                    gp.Dimension(name=cityIdList[0]),\n                    gp.Dimension(name=dimensions[x])\n                ],\n                metrics=[\n                    gp.Metric(name=metricList[y]),\n                    gp.Metric(name=metricList[y+1]),\n                    gp.Metric(name=metricList[y+2]),\n                    gp.Metric(name=metricList[y+3]),\n                    gp.Metric(name=metricList[y+4]),\n                    gp.Metric(name=metricList[y+5]),\n                ],\n                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n            )\n            \n            # Perform query and append to list\n            dfs[z].append(gp.query(service_account, report_request, report_type="report"))\n    \n            \n    #### loyaltyList\n    \n    # Empty list to fill with dataframes\n    dfs_l = []\n    \n    # Request for all dimensions in list\n    for x in range(len(dimensions)):\n    \n            report_request = gp.RunReportRequest(\n                property=f"properties/{property_id}",\n                dimensions=[\n                    gp.Dimension(name=dateList[0]),\n                    gp.Dimension(name=cityIdList[0]),\n                    gp.Dimension(name=dimensions[x])\n                ],\n                metrics=[\n                    gp.Metric(name=loyaltyList[0]),\n                    gp.Metric(name=loyaltyList[1]),\n                    gp.Metric(name=loyaltyList[2]),\n                ],\n                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n            )\n            \n            # Perform query and append to list\n            dfs_l.append(gp.query(service_account, report_request, report_type="report"))\n    \n        \n    #### activityList\n    \n    # Empty list to fill with dataframes\n    dfs_a = []\n    \n    # Request for all dimensions in list\n    for x in range(len(dimensions)):\n    \n            report_request = gp.RunReportRequest(\n                property=f"properties/{property_id}",\n                dimensions=[\n                    gp.Dimension(name=dateList[0]),\n                    gp.Dimension(name=cityIdList[0]),\n                    gp.Dimension(name=dimensions[x])\n                ],\n                metrics=[\n                    gp.Metric(name=activityList[0]),\n                    gp.Metric(name=activityList[1]),\n                    gp.Metric(name=activityList[2]),\n                ],\n                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n            )\n            \n            # Perform query and append to list\n            dfs_a.append(gp.query(service_account, report_request, report_type="report"))\n    \n    \n        \n    ### dimensionsSpecial\n    \n    # Empty list to fill with dataframes\n    dfs_s = [[],[],[],]\n    \n    # Request for all dimensions in list\n    for x in range(len(dimensionsSpecial)):\n        for y, z in zip([1,7,13],[0,1,2]): \n    \n            report_request = gp.RunReportRequest(\n                property=f"properties/{property_id}",\n                dimensions=[\n                    gp.Dimension(name=dateList[0]),\n                    gp.Dimension(name=countryIdList[0]),\n                    gp.Dimension(name=dimensionsSpecial[x])\n                ],\n                metrics=[\n                    gp.Metric(name=metricList[y]),\n                    gp.Metric(name=metricList[y+1]),\n                    gp.Metric(name=metricList[y+2]),\n                    gp.Metric(name=metricList[y+3]),\n                    gp.Metric(name=metricList[y+4]),\n                    gp.Metric(name=metricList[y+5]),\n                ],\n                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n            )\n            \n            # Perform query and append to list\n            dfs_s[z].append(gp.query(service_account, report_request, report_type="report"))\n    \n            \n    #### loyaltyList\n    \n    # Empty list to fill with dataframes\n    dfs_s_l = []\n    \n    # Request for all dimensions in list\n    for x in range(len(dimensionsSpecial)):\n    \n            report_request = gp.RunReportRequest(\n                property=f"properties/{property_id}",\n                dimensions=[\n                    gp.Dimension(name=dateList[0]),\n                    gp.Dimension(name=countryIdList[0]),\n                    gp.Dimension(name=dimensionsSpecial[x])\n                ],\n                metrics=[\n                    gp.Metric(name=loyaltyList[0]),\n                    gp.Metric(name=loyaltyList[1]),\n                    gp.Metric(name=loyaltyList[2]),\n                ],\n                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n            )\n            \n            # Perform query and append to list\n            dfs_s_l.append(gp.query(service_account, report_request, report_type="report"))\n    \n        \n    #### activityList\n    \n    # Empty list to fill with dataframes\n    dfs_s_a = []\n    \n    # Request for all dimensions in list\n    for x in range(len(dimensionsSpecial)):\n    \n            report_request = gp.RunReportRequest(\n                property=f"properties/{property_id}",\n                dimensions=[\n                    gp.Dimension(name=dateList[0]),\n                    gp.Dimension(name=countryIdList[0]),\n                    gp.Dimension(name=dimensionsSpecial[x])\n                ],\n                metrics=[\n                    gp.Metric(name=activityList[0]),\n                    gp.Metric(name=activityList[1]),\n                    gp.Metric(name=activityList[2]),\n                ],\n                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n            )\n            \n            # Perform query and append to list\n            dfs_s_a.append(gp.query(service_account, report_request, report_type="report"))\n    \n    \n        \n    ### dimensionsSpecialSpecial date\n    \n    #### metricList\n    \n    # Empty list to fill with dataframes\n    dfs_ssd = [[],[],[],]\n    \n    # Request for all dimensions in list\n    for x in range(len(dimensionsSpecialSpecial)):\n        for y, z in zip([1,7,13],[0,1,2]): \n    \n            report_request = gp.RunReportRequest(\n                property=f"properties/{property_id}",\n                dimensions=[\n                    gp.Dimension(name=dateList[0]),\n                    gp.Dimension(name=dimensionsSpecialSpecial[x])\n                ],\n                metrics=[\n                    gp.Metric(name=metricList[y]),\n                    gp.Metric(name=metricList[y+1]),\n                    gp.Metric(name=metricList[y+2]),\n                    gp.Metric(name=metricList[y+3]),\n                    gp.Metric(name=metricList[y+4]),\n                    gp.Metric(name=metricList[y+5]),\n                ],\n                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n            )\n            \n            # Perform query and append to list\n            dfs_ssd[z].append(gp.query(service_account, report_request, report_type="report"))\n    \n            \n    #### loyaltyList\n    \n    # Empty list to fill with dataframes\n    dfs_ssd_l = []\n    \n    # Request for all dimensions in list\n    for x in range(len(dimensionsSpecialSpecial)):\n    \n            report_request = gp.RunReportRequest(\n                property=f"properties/{property_id}",\n                dimensions=[\n                    gp.Dimension(name=dateList[0]),\n                    gp.Dimension(name=dimensionsSpecialSpecial[x])\n                ],\n                metrics=[\n                    gp.Metric(name=loyaltyList[0]),\n                    gp.Metric(name=loyaltyList[1]),\n                    gp.Metric(name=loyaltyList[2]),\n                ],\n                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n            )\n            \n            # Perform query and append to list\n            dfs_ssd_l.append(gp.query(service_account, report_request, report_type="report"))\n    \n        \n    #### activityList\n    \n    # Empty list to fill with dataframes\n    dfs_ssd_a = []\n    \n    # Request for all dimensions in list\n    for x in range(len(dimensionsSpecialSpecial)):\n    \n            report_request = gp.RunReportRequest(\n                property=f"properties/{property_id}",\n                dimensions=[\n                    gp.Dimension(name=dateList[0]),\n                    gp.Dimension(name=dimensionsSpecialSpecial[x])\n                ],\n                metrics=[\n                    gp.Metric(name=activityList[0]),\n                    gp.Metric(name=activityList[1]),\n                    gp.Metric(name=activityList[2]),\n                ],\n                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n            )\n            \n            # Perform query and append to list\n            dfs_ssd_a.append(gp.query(service_account, report_request, report_type="report"))\n    \n    \n        \n    ### dimensionsSpecialSpecial countryId\n    \n    #### metricList\n    \n    # Empty list to fill with dataframes\n    dfs_ssc = [[],[],[],]\n    \n    # Request for all dimensions in list\n    for x in range(len(dimensionsSpecialSpecial)):\n        for y, z in zip([1,7,13],[0,1,2]): \n    \n            report_request = gp.RunReportRequest(\n                property=f"properties/{property_id}",\n                dimensions=[\n                    gp.Dimension(name=countryIdList[0]),\n                    gp.Dimension(name=dimensionsSpecialSpecial[x])\n                ],\n                metrics=[\n                    gp.Metric(name=metricList[y]),\n                    gp.Metric(name=metricList[y+1]),\n                    gp.Metric(name=metricList[y+2]),\n                    gp.Metric(name=metricList[y+3]),\n                    gp.Metric(name=metricList[y+4]),\n                    gp.Metric(name=metricList[y+5]),\n                ],\n                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n            )\n            \n            # Perform query and append to list\n            dfs_ssc[z].append(gp.query(service_account, report_request, report_type="report"))\n    \n            \n    #### loyaltyList\n    \n    # Empty list to fill with dataframes\n    dfs_ssc_l = []\n    \n    # Request for all dimensions in list\n    for x in range(len(dimensionsSpecialSpecial)):\n    \n            report_request = gp.RunReportRequest(\n                property=f"properties/{property_id}",\n                dimensions=[\n                    gp.Dimension(name=countryIdList[0]),\n                    gp.Dimension(name=dimensionsSpecialSpecial[x])\n                ],\n                metrics=[\n                    gp.Metric(name=loyaltyList[0]),\n                    gp.Metric(name=loyaltyList[1]),\n                    gp.Metric(name=loyaltyList[2]),\n                ],\n                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n            )\n            \n            # Perform query and append to list\n            dfs_ssc_l.append(gp.query(service_account, report_request, report_type="report"))\n    \n        \n    #### activityList\n    \n    # Empty list to fill with dataframes\n    dfs_ssc_a = []\n    \n    # Request for all dimensions in list\n    for x in range(len(dimensionsSpecialSpecial)):\n    \n            report_request = gp.RunReportRequest(\n                property=f"properties/{property_id}",\n                dimensions=[\n                    gp.Dimension(name=countryIdList[0]),\n                    gp.Dimension(name=dimensionsSpecialSpecial[x])\n                ],\n                metrics=[\n                    gp.Metric(name=activityList[0]),\n                    gp.Metric(name=activityList[1]),\n                    gp.Metric(name=activityList[2]),\n                ],\n                date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n            )\n            \n            # Perform query and append to list\n            dfs_ssc_a.append(gp.query(service_account, report_request, report_type="report"))\n    \n    # ---------------------------------------------------------------------------------------------------------------------\n      \n    ## Formatting\n    \n    #### metricList\n    \n    # Drop the extra Totalusers column, since it exists twice in the tables to merge\n    # since metricList contained it twice.\n    \n    for x in range(len(dimensions)):\n        for z in [2]:\n            dfs[z][x].drop(columns=[\'totalUsers\'],inplace=True)\n    \n    for x in range(len(dimensionsSpecial)):\n        for z in [2]:\n            dfs_s[z][x].drop(columns=[\'totalUsers\'],inplace=True)\n    \n    for x in range(len(dimensionsSpecialSpecial)):\n        for z in [2]:\n            dfs_ssd[z][x].drop(columns=[\'totalUsers\'],inplace=True)\n    \n    for x in range(len(dimensionsSpecialSpecial)):\n        for z in [2]:\n            dfs_ssc[z][x].drop(columns=[\'totalUsers\'],inplace=True)\n            \n    # ---------------------------------------------------------------------------------------------------------------------\n        \n    ## Merge\n    \n    dfs_merged = list(range(len(dimensions)))\n    \n    for x in range(len(dimensions)):\n        mergeOn = [\'dateHour\',\'cityId\'] \n        mergeDim = [dimensions[x]]\n        mergeOn =  mergeOn + mergeDim\n        dfs_merged[x] = dfs[0][x].merge(dfs[1][x], how=\'outer\', on=mergeOn)\n    \n    for n in range(len(dimensions)):\n        mergeOn = [\'dateHour\',\'cityId\'] \n        mergeDim = [dimensions[n]]\n        mergeOn =  mergeOn + mergeDim\n        dfs_merged[n] = dfs_merged[n].merge(dfs[2][n], how=\'outer\', on=mergeOn)\n    \n    \n    dfs_s_merged = list(range(len(dimensionsSpecial)))\n    \n    for x in range(len(dimensionsSpecial)):\n        mergeOn = [\'date\',\'countryId\'] \n        mergeDim = [dimensionsSpecial[x]]\n        mergeOn =  mergeOn + mergeDim\n        dfs_s_merged[x] = dfs_s[0][x].merge(dfs_s[1][x], how=\'outer\', on=mergeOn)\n    \n    for n in range(len(dimensionsSpecial)):\n        mergeOn = [\'date\',\'countryId\'] \n        mergeDim = [dimensionsSpecial[n]]\n        mergeOn =  mergeOn + mergeDim\n        dfs_s_merged[n] = dfs_s_merged[n].merge(dfs_s[2][n], how=\'outer\', on=mergeOn)\n    \n    \n    dfs_ssd_merged = list(range(len(dimensionsSpecialSpecial)))\n    \n    for x in range(len(dimensionsSpecialSpecial)):\n        mergeOn = [\'date\'] \n        mergeDim = [dimensionsSpecialSpecial[x]]\n        mergeOn =  mergeOn + mergeDim\n        dfs_ssd_merged[x] = dfs_ssd[0][x].merge(dfs_ssd[1][x], how=\'outer\', on=mergeOn)\n    \n    for n in range(len(dimensionsSpecialSpecial)):\n        mergeOn = [\'date\'] \n        mergeDim = [dimensionsSpecialSpecial[n]]\n        mergeOn =  mergeOn + mergeDim\n        dfs_ssd_merged[n] = dfs_ssd_merged[n].merge(dfs_ssd[2][n], how=\'outer\', on=mergeOn)\n    \n    \n    dfs_ssc_merged = list(range(len(dimensionsSpecialSpecial)))\n    \n    for x in range(len(dimensionsSpecialSpecial)):\n        mergeOn = [\'countryId\'] \n        mergeDim = [dimensionsSpecialSpecial[x]]\n        mergeOn =  mergeOn + mergeDim\n        dfs_ssc_merged[x] = dfs_ssc[0][x].merge(dfs_ssc[1][x], how=\'outer\', on=mergeOn)\n    \n    for n in range(len(dimensionsSpecialSpecial)):\n        mergeOn = [\'countryId\'] \n        mergeDim = [dimensionsSpecialSpecial[n]]\n        mergeOn =  mergeOn + mergeDim\n        dfs_ssc_merged[n] = dfs_ssc_merged[n].merge(dfs_ssc[2][n], how=\'outer\', on=mergeOn)\n\n    # ---------------------------------------------------------------------------------------------------------------------\n\n    # Sort out dates and sort columns.\n        \n    #### metricList\n    \n    for x in range(len(dimensions)):\n        if not dfs_merged[x].empty:\n            dfs_merged[x] = dates_funcs.sortOut_dateHour_short(dfs_merged[x])\n    \n    for x in range(len(dimensionsSpecial)):\n        if not  dfs_s_merged[x].empty:\n            dfs_s_merged[x] = dates_funcs.sortOut_date_short(dfs_s_merged[x])\n            \n    for x in range(len(dimensionsSpecialSpecial)):\n        if not dfs_ssd_merged[x].empty:\n            dfs_ssd_merged[x] = dates_funcs.sortOut_date_short(dfs_ssd_merged[x])\n    \n    for x in range(len(dimensionsSpecialSpecial)):\n        if not dfs_ssc_merged[x].empty:\n            sortOrder = True\n            dfs_ssc_merged[x] = dfs_ssc_merged[x].sort_values(list(dfs_ssc_merged[x].columns.values), ascending=sortOrder)\n    \n    #### loyaltyList\n    \n    for x in range(len(dimensions)):\n        if not dfs_l[x].empty:\n            dfs_l[x] = dates_funcs.sortOut_date_short(dfs_l[x])\n    \n    for x in range(len(dimensionsSpecial)):\n        if not dfs_s_l[x].empty:\n            dfs_s_l[x] = dates_funcs.sortOut_date_short(dfs_s_l[x])\n            \n    for x in range(len(dimensionsSpecialSpecial)):\n        if not dfs_ssd_l[x].empty:\n            dfs_ssd_l[x] = dates_funcs.sortOut_date_short(dfs_ssd_l[x])\n\n    for x in range(len(dimensionsSpecialSpecial)):\n        if not dfs_ssc_l[x].empty:\n            sortOrder = True\n            dfs_ssc_l[x] = dfs_ssc_l[x].sort_values(list(dfs_ssc_l[x].columns.values), ascending=sortOrder)\n    \n    #### activityList\n    \n    for x in range(len(dimensions)):\n        if not dfs_a[x].empty:\n            dfs_a[x] = dates_funcs.sortOut_date_short(dfs_a[x])\n    \n    for x in range(len(dimensionsSpecial)):\n        if not dfs_s_a[x].empty:\n            dfs_s_a[x] = dates_funcs.sortOut_date_short(dfs_s_a[x])\n            \n    for x in range(len(dimensionsSpecialSpecial)):\n        if not dfs_ssd_a[x].empty:\n            dfs_ssd_a[x] = dates_funcs.sortOut_date_short(dfs_ssd_a[x])\n    \n    for x in range(len(dimensionsSpecialSpecial)):\n        if not dfs_ssc_a[x].empty:\n            sortOrder = True\n            dfs_ssc_a[x] = dfs_ssc_a[x].sort_values(list(dfs_ssc_a[x].columns.values), ascending=sortOrder)\n        \n    # ---------------------------------------------------------------------------------------------------------------------\n    \n    # Add baseURL advokatfamiljforsvar.se to fullUrl\n\n    pos_landingPage = dimensions.index("landingPage") \n    mainAdress = \'https://advokatfamiljforsvar.se\'  \n            \n    dfs_merged[pos_landingPage][\'landingPage\'] = dfs_merged[pos_landingPage][\'landingPage\'].where(dfs_merged[pos_landingPage][\'landingPage\'].values == \'(not set)\', mainAdress + dfs_merged[pos_landingPage][\'landingPage\'])\n\n    dfs_l[pos_landingPage][\'landingPage\'] = dfs_l[pos_landingPage][\'landingPage\'].where(dfs_l[pos_landingPage][\'landingPage\'].values == \'(not set)\', mainAdress + dfs_l[pos_landingPage][\'landingPage\'])\n\n    dfs_a[pos_landingPage][\'landingPage\'] = dfs_a[pos_landingPage][\'landingPage\'].where(dfs_a[pos_landingPage][\'landingPage\'].values == \'(not set)\', mainAdress + dfs_a[pos_landingPage][\'landingPage\'])\n    \n    #---------------------------------------------------------------------------------------------\n\n    ## Sort out NA\n    \n    #### metricList\n    \n    dfs_merged_fillNA = list(range(len(dimensions)))\n    \n    for n in range(len(dimensions)):\n        dfs_merged_fillNA[n] = dfs_merged[n].replace(\'\', np.nan)\n        dfs_merged_fillNA[n] = dfs_merged_fillNA[n].replace(\'(not set)\', np.nan)\n        print(dfs_merged_fillNA[n].shape)\n    \n    \n    dfs_s_merged_fillNA = list(range(len(dimensionsSpecial)))\n    \n    for n in range(len(dimensionsSpecial)):\n        dfs_s_merged_fillNA[n] = dfs_s_merged[n].replace(\'\', np.nan)\n        dfs_s_merged_fillNA[n] = dfs_s_merged_fillNA[n].replace(\'(not set)\', np.nan)\n        print(dfs_s_merged_fillNA[n].shape)\n    \n    \n    dfs_ssd_merged_fillNA = list(range(len(dimensionsSpecialSpecial)))\n    \n    for n in range(len(dimensionsSpecialSpecial)):\n        dfs_ssd_merged_fillNA[n] = dfs_ssd_merged[n].replace(\'\', np.nan)\n        dfs_ssd_merged_fillNA[n] = dfs_ssd_merged_fillNA[n].replace(\'(not set)\', np.nan)\n        print(dfs_ssd_merged_fillNA[n].shape)\n    \n    \n    dfs_ssc_merged_fillNA = list(range(len(dimensionsSpecialSpecial)))\n    \n    for n in range(len(dimensionsSpecialSpecial)):\n        dfs_ssc_merged_fillNA[n] = dfs_ssc_merged[n].replace(\'\', np.nan)\n        dfs_ssc_merged_fillNA[n] = dfs_ssc_merged_fillNA[n].replace(\'(not set)\', np.nan)\n        print(dfs_ssc_merged_fillNA[n].shape)\n    \n    #### loyaltyList\n    \n    dfs_l_fillNA = list(range(len(dimensions)))\n    \n    for n in range(len(dimensions)):\n        dfs_l_fillNA[n] = dfs_l[n].replace(\'\', np.nan)\n        dfs_l_fillNA[n] = dfs_l_fillNA[n].replace(\'(not set)\', np.nan)\n        print(dfs_l_fillNA[n].shape)\n    \n    \n    dfs_s_l_fillNA = list(range(len(dimensionsSpecial)))\n    \n    for n in range(len(dimensionsSpecial)):\n        dfs_s_l_fillNA[n] = dfs_s_l[n].replace(\'\', np.nan)\n        dfs_s_l_fillNA[n] = dfs_s_l_fillNA[n].replace(\'(not set)\', np.nan)\n        print(dfs_s_l_fillNA[n].shape)\n    \n    \n    dfs_ssd_l_fillNA = list(range(len(dimensionsSpecialSpecial)))\n    \n    for n in range(len(dimensionsSpecialSpecial)):\n        dfs_ssd_l_fillNA[n] = dfs_ssd_l[n].replace(\'\', np.nan)\n        dfs_ssd_l_fillNA[n] = dfs_ssd_l_fillNA[n].replace(\'(not set)\', np.nan)\n        print(dfs_ssd_l_fillNA[n].shape)\n    \n    \n    dfs_ssc_l_fillNA = list(range(len(dimensionsSpecialSpecial)))\n    \n    for n in range(len(dimensionsSpecialSpecial)):\n        dfs_ssc_l_fillNA[n] = dfs_ssc_l[n].replace(\'\', np.nan)\n        dfs_ssc_l_fillNA[n] = dfs_ssc_l_fillNA[n].replace(\'(not set)\', np.nan)\n        print(dfs_ssc_l_fillNA[n].shape)\n    \n    #### activityList\n    \n    \n    dfs_a_fillNA = list(range(len(dimensions)))\n    \n    for n in range(len(dimensions)):\n        dfs_a_fillNA[n] = dfs_a[n].replace(\'\', np.nan)\n        dfs_a_fillNA[n] = dfs_a_fillNA[n].replace(\'(not set)\', np.nan)\n        print(dfs_a_fillNA[n].shape)\n    \n    \n    dfs_s_a_fillNA = list(range(len(dimensionsSpecial)))\n    \n    for n in range(len(dimensionsSpecial)):\n        dfs_s_a_fillNA[n] = dfs_s_a[n].replace(\'\', np.nan)\n        dfs_s_a_fillNA[n] = dfs_s_a_fillNA[n].replace(\'(not set)\', np.nan)\n        print(dfs_s_a_fillNA[n].shape)\n    \n    \n    dfs_ssd_a_fillNA = list(range(len(dimensionsSpecialSpecial)))\n    \n    for n in range(len(dimensionsSpecialSpecial)):\n        dfs_ssd_a_fillNA[n] = dfs_ssd_a[n].replace(\'\', np.nan)\n        dfs_ssd_a_fillNA[n] = dfs_ssd_a_fillNA[n].replace(\'(not set)\', np.nan)\n        print(dfs_ssd_a_fillNA[n].shape)\n    \n    \n    dfs_ssc_a_fillNA = list(range(len(dimensionsSpecialSpecial)))\n    \n    for n in range(len(dimensionsSpecialSpecial)):\n        dfs_ssc_a_fillNA[n] = dfs_ssc_a[n].replace(\'\', np.nan)\n        dfs_ssc_a_fillNA[n] = dfs_ssc_a_fillNA[n].replace(\'(not set)\', np.nan)\n        print(dfs_ssc_a_fillNA[n].shape)\n    \n        \n    # ---------------------------------------------------------------------------------------------------------------------\n\n    # SPLIT DATA BY YEAR AND THEN MONTH print to .csv, all dfs which contains dateinfo\n\n    dfs_out = pd.DataFrame()\n\n    # Empty dataframes for years\n    dfs_merged_fillNA_by_Year = pd.DataFrame()\n    dfs_s_merged_fillNA_by_Year = pd.DataFrame()\n    dfs_ssd_merged_fillNA_by_Year = pd.DataFrame()\n    #dfs_ssc_merged_fillNA_by_Year = pd.DataFrame()\n    dfs_l_fillNA_by_Year = pd.DataFrame()\n    dfs_s_l_fillNA_by_Year = pd.DataFrame()\n    dfs_ssd_l_fillNA_by_Year = pd.DataFrame()\n    #dfs_ssc_l_fillNA_by_Year = pd.DataFrame()\n    dfs_a_fillNA_by_Year = pd.DataFrame()\n    dfs_s_a_fillNA_by_Year = pd.DataFrame()\n    dfs_ssd_a_fillNA_by_Year = pd.DataFrame()\n    #dfs_ssc_a_fillNA_by_Year = pd.DataFrame()\n\n    # Empty dataframes for years\n    dfs_merged_fillNA_by_Month = pd.DataFrame()\n    dfs_s_merged_fillNA_by_Month = pd.DataFrame()\n    dfs_ssd_merged_fillNA_by_Month = pd.DataFrame()\n    #dfs_ssc_merged_fillNA_by_Month = pd.DataFrame()\n    dfs_l_fillNA_by_Month = pd.DataFrame()\n    dfs_s_l_fillNA_by_Month = pd.DataFrame()\n    dfs_ssd_l_fillNA_by_Month = pd.DataFrame()\n    #dfs_ssc_l_fillNA_by_Month = pd.DataFrame()\n    dfs_a_fillNA_by_Month = pd.DataFrame()\n    dfs_s_a_fillNA_by_Month = pd.DataFrame()\n    dfs_ssd_a_fillNA_by_Month = pd.DataFrame()\n    #dfs_ssc_a_fillNA_by_Month = pd.DataFrame()\n\n    # All lists to loop over using zip:\n    list_of_dimensions = [dimensions, dimensionsSpecial, dimensionsSpecialSpecial,\n                          dimensions, dimensionsSpecial, dimensionsSpecialSpecial,\n                          dimensions, dimensionsSpecial, dimensionsSpecialSpecial]\n\n    list_of_input = [dfs_merged_fillNA, dfs_s_merged_fillNA, dfs_ssd_merged_fillNA,\n                     dfs_l_fillNA, dfs_s_l_fillNA, dfs_ssd_l_fillNA,\n                     dfs_a_fillNA, dfs_s_a_fillNA, dfs_ssd_a_fillNA]\n\n    list_of_type = [\'metrics\', \'metrics\', \'metrics\',\n                    \'loyalty\', \'loyalty\', \'loyalty\',\n                    \'activity\', \'activity\', \'activity\']\n\n    list_of_df_for_year = [dfs_merged_fillNA_by_Year, dfs_s_merged_fillNA_by_Year, dfs_ssd_merged_fillNA_by_Year,\n                           dfs_l_fillNA_by_Year, dfs_s_l_fillNA_by_Year, dfs_ssd_l_fillNA_by_Year,\n                           dfs_a_fillNA_by_Year, dfs_s_a_fillNA_by_Year, dfs_ssd_a_fillNA_by_Year]\n\n    list_of_df_for_month = [dfs_merged_fillNA_by_Month, dfs_s_merged_fillNA_by_Month, dfs_ssd_merged_fillNA_by_Month,\n                           dfs_l_fillNA_by_Month, dfs_s_l_fillNA_by_Month, dfs_ssd_l_fillNA_by_Month, \n                           dfs_a_fillNA_by_Month, dfs_s_a_fillNA_by_Month, dfs_ssd_a_fillNA_by_Month]\n\n    pd.options.mode.chained_assignment = None \n\n\n    \n    # Looping over all data\n    for l_dim, l_input, l_type, l_year, l_month in zip(list_of_dimensions, list_of_input, list_of_type, list_of_df_for_year, list_of_df_for_month):\n\n        if l_dim and l_input and l_type:\n            \n            # Split by years\n            for n in range(len(l_dim)):\n                if not l_input[n].empty:\n                    l_year[n] = dates_funcs.split_years(l_input[n])\n        \n            # add for earch year add list to monthlist\n            for n in range(len(l_dim)):\n                if not l_input[n].empty:\n                    if not l_year[n].empty:\n                        # List of lists\n                        l_month[n] = [[] for _ in range(len(l_year[n]))]    \n        \n            # Split by month\n            for n in range(len(l_dim)):\n                for x in range(len(l_year)):\n                    if not l_input[n].empty:\n                        if not l_year[n][x].empty:\n                            l_month[n][x] = dates_funcs.split_months(l_year[n][x])\n            \n            # Flatten df_by_months[x][y] (two levels) structure to array of dataframes (one level)\n            for n in range(len(l_dim)):\n                if not l_input[n].empty:\n                    if not l_month[n].empty:\n                        dfs_out[n] = list(itertools.chain.from_iterable(l_month[n]))\n          \n\n            #print to .csv, create folder if needed\n            for n in range(len(l_dim)):\n                for x in range(len(dfs_out)):\n                #for x, y in zip(range(len(dfs_out[n])),fileNameList[n]):\n                    if not l_input[n].empty:\n                        if not dfs_out[n][x].empty:\n                            csvFilePath = "output/csv/"+l_type+"/"+l_dim[n]+"/"\n                            if not os.path.exists(csvFilePath):\n                                os.makedirs(csvFilePath)\n                            #dfs_out[n][x] = dfs_out[n][x].astype(str)\n                            #dfs_out[n][x][\'year\'].min()+\'_\'+dfs_out[n][x][\'month\'].min()\n                            fileName = l_dim[n]+"_"+l_type+"_"+dfs_out[n][x][\'year\'].min().astype(str)+\'_\'+dfs_out[n][x][\'month\'].min().astype(str)+".csv"\n                            dfs_out[n][x].drop(columns=[\'year\'],inplace=True)\n                            dfs_out[n][x].drop(columns=[\'month\'],inplace=True)\n                            appendDFToCSV(dfs_out[n][x], csvFilePath, fileName, \',\')       \n    # End Loop\n        \n    pd.options.mode.chained_assignment = \'warn\' \n\n    # ---------------------------------------------------------------------------------------------------------------------\n        \n    ## Dataframes which does not contain dateFull to .csv\n\n    \'\'\'\n    for n in range(len(dfs_merged_fillNA)):\n        if not dfs_merged_fillNA[n].empty:\n            csvFilePath = "output/csv/metrics/"+dimensions[n]+"/"\n            if not os.path.exists(csvFilePath):\n                os.makedirs(csvFilePath)\n            fileName = dimensions[n]+"_dateHour"+"_cityId"+".csv" \n            appendDFToCSV(dfs_merged_fillNA[n], csvFilePath, fileName, sep)\n    \n    for n in range(len(dfs_s_merged_fillNA)):\n        if not dfs_s_merged_fillNA[n].empty:\n            csvFilePath = "output/csv/metrics/"+dimensionsSpecial[n]+"/"\n            if not os.path.exists(csvFilePath):\n                os.makedirs(csvFilePath)\n            fileName = dimensionsSpecial[n]+"_date"+"_countryId"+".csv"\n            appendDFToCSV(dfs_s_merged_fillNA[n], csvFilePath, fileName, sep)\n    \n    for n in range(len(dfs_ssd_merged_fillNA)):\n        if not dfs_ssd_merged_fillNA[n].empty:\n            csvFilePath = "output/csv/metrics/"+dimensionsSpecialSpecial[n]+"/"\n            if not os.path.exists(csvFilePath):\n                os.makedirs(csvFilePath)\n            fileName = dimensionsSpecialSpecial[n]+"_date"+".csv"\n            appendDFToCSV(dfs_ssd_merged_fillNA[n], csvFilePath, fileName, sep)\n    \'\'\'\n    for n in range(len(dfs_ssc_merged_fillNA)):\n        if not dfs_ssc_merged_fillNA[n].empty:\n            csvFilePath = "output/csv/metrics/"+dimensionsSpecialSpecial[n]+"/"\n            if not os.path.exists(csvFilePath):\n                os.makedirs(csvFilePath)\n            fileName = dimensionsSpecialSpecial[n]+"_metrics_countryId.csv"\n            appendDFToCSV(dfs_ssc_merged_fillNA[n], csvFilePath, fileName, \',\')\n    \'\'\'\n    for n in range(len(dfs_l_fillNA)):\n        if not dfs_l_fillNA[n].empty:\n            csvFilePath = "output/csv/loyalty/"+dimensions[n]+"/"\n            if not os.path.exists(csvFilePath):\n                os.makedirs(csvFilePath)\n            fileName = dimensions[n]+"_dateHour"+"_cityId"+"_loyalty"+".csv"\n            appendDFToCSV(dfs_l_fillNA[n], csvFilePath, fileName, sep)\n    \n    for n in range(len(dfs_s_l_fillNA)):\n        if not dfs_s_l_fillNA[n].empty:\n            csvFilePath = "output/csv/loyalty/"+dimensionsSpecial[n]+"/"\n            if not os.path.exists(csvFilePath):\n                os.makedirs(csvFilePath)\n            fileName = dimensionsSpecial[n]+"_date"+"_countryId"+"_loyalty"+".csv"\n            appendDFToCSV(dfs_s_l_fillNA[n], csvFilePath, fileName, sep)\n    \n    for n in range(len(dfs_ssd_l_fillNA)):\n        if not dfs_ssd_l_fillNA[n].empty:\n            csvFilePath = "output/csv/loyalty/"+dimensionsSpecialSpecial[n]+"/"\n            if not os.path.exists(csvFilePath):\n                os.makedirs(csvFilePath)\n            fileName = dimensionsSpecialSpecial[n]+"_date"+"_loyalty"+".csv"\n            appendDFToCSV(dfs_ssd_l_fillNA[n], csvFilePath, fileName, sep)\n    \'\'\'\n    for n in range(len(dfs_ssc_l_fillNA)):\n        if not dfs_ssc_l_fillNA[n].empty:\n            csvFilePath = "output/csv/loyalty/"+dimensionsSpecialSpecial[n]+"/"\n            if not os.path.exists(csvFilePath):\n                os.makedirs(csvFilePath)\n            fileName = dimensionsSpecialSpecial[n]+"_loyalty_countryId.csv"\n            appendDFToCSV(dfs_ssc_l_fillNA[n], csvFilePath, fileName, \',\')\n    \'\'\'\n    for n in range(len(dfs_a_fillNA)):\n        if not dfs_a_fillNA[n].empty:\n            csvFilePath = "output/csv/activity/"+dimensions[n]+"/"\n            if not os.path.exists(csvFilePath):\n                os.makedirs(csvFilePath)\n            fileName = dimensions[n]+"_dateHour"+"_cityId"+"_activity"+".csv"\n            appendDFToCSV(dfs_a_fillNA[n], csvFilePath, fileName, sep)\n    \n    for n in range(len(dfs_s_a_fillNA)):\n        if not dfs_s_a_fillNA[n].empty:\n            csvFilePath = "output/csv/activity/"+dimensionsSpecial[n]+"/"\n            if not os.path.exists(csvFilePath):\n                os.makedirs(csvFilePath)\n            fileName = dimensionsSpecial[n]+"_date"+"_countryId"+"_activity"+".csv"\n            appendDFToCSV(dfs_s_a_fillNA[n], csvFilePath, fileName, sep)\n    \n    for n in range(len(dfs_ssd_a_fillNA)):\n        if not dfs_ssd_a_fillNA[n].empty:\n            csvFilePath = "output/csv/activity/"+dimensionsSpecialSpecial[n]+"/"\n            if not os.path.exists(csvFilePath):\n                os.makedirs(csvFilePath)\n            fileName = dimensionsSpecialSpecial[n]+"_date"+"_activity"+".csv"\n            appendDFToCSV(dfs_ssd_a_fillNA[n], csvFilePath, fileName, sep)\n    \'\'\'\n    for n in range(len(dfs_ssc_a_fillNA)):\n        if not dfs_ssc_a_fillNA[n].empty:\n            csvFilePath = "output/csv/activity/"+dimensionsSpecialSpecial[n]+"/"\n            if not os.path.exists(csvFilePath):\n                os.makedirs(csvFilePath)\n            fileName = dimensionsSpecialSpecial[n]+"_activity_countryId.csv"\n            appendDFToCSV(dfs_ssc_a_fillNA[n], csvFilePath, fileName, \',\')\n    \n    # ---------------------------------------------------------------------------------------------------------------------\n        \n    ## Date and Geography tables\n    \n    # Geography List\n    geoList = [\'continentId\',\'continent\',\'countryId\',\'country\',\'region\',\'cityId\',\'city\']\n    \n    # Empty list to fill with dataframes\n    \n    report_request = gp.RunReportRequest(\n        property=f"properties/{property_id}",\n         dimensions=[\n             gp.Dimension(name=geoList[0]),\n             gp.Dimension(name=geoList[1]),\n             gp.Dimension(name=geoList[2]),\n             gp.Dimension(name=geoList[3]),\n             gp.Dimension(name=geoList[4]),\n             gp.Dimension(name=geoList[5]),\n             gp.Dimension(name=geoList[6]),\n           ],\n           metrics=[\n            ],\n            date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n        )\n        \n    # Perform query and append to list\n    df_geo = gp.query(service_account, report_request, report_type="report")\n\n    # Sort columns\n    sortOrder = True\n    df_geo = df_geo.sort_values(list(df_geo.columns.values), ascending=sortOrder)\n\n    \n    # Date List\n    dateList = ["dateHour"]\n    \n    # Empty list to fill with dataframes\n    \n    report_request = gp.RunReportRequest(\n        property=f"properties/{property_id}",\n         dimensions=[\n             gp.Dimension(name=dateList[0]),\n           ],\n           metrics=[\n            ],\n            date_ranges=[gp.DateRange(start_date=start_date, end_date=end_date)],\n        )\n    \n    # Perform query and append to list\n    df_dateHour = gp.query(service_account, report_request, report_type="report")\n\n    # Sort out dates function\n    df_dateHour = dates_funcs.sortOut_dateHour(df_dateHour)\n\n    \n    df_geoTest = df_geo.replace(\'\', np.nan)\n    df_geoTest = df_geoTest.replace(\'(not set)\', np.nan)\n    print(df_geoTest.shape)\n    \n    df_dateHourTest = df_dateHour.replace(\'\', np.nan)\n    df_dateHourTest = df_dateHourTest.replace(\'(not set)\', np.nan)\n    print(df_dateHourTest.shape)\n\n\n    csvFilePath = "output/csv/reference/"\n    if not os.path.exists(csvFilePath):\n                os.makedirs(csvFilePath)\n    fileName = "geographyInfo.csv"\n    if not df_geoTest.empty:\n        appendDFToCSV(df_geoTest, csvFilePath, fileName, \',\')\n    \n    csvFilePath = "output/csv/reference/"\n    if not os.path.exists(csvFilePath):\n                os.makedirs(csvFilePath)\n    fileName = "dateInfo.csv"\n    if not df_dateHourTest.empty:\n        appendDFToCSV(df_dateHourTest, csvFilePath, fileName, \',\')\n\n    # -------------------------------------------\n    \n    # Read in and merge geographyInfo with geographyCountryCodes \n    pathReferenceGeography="/Users/emil/miniforge3/envs/googleapi/Notebooks/Analytics/output/csv/reference/"\n    #https://github.com/stefangabos/world_countries/blob/master/data/countries/en/world.csv\n    fileNameGeoMaster = "countryCodes.csv"\n    fileNameGeoInfo = "geographyInfo.csv"\n    \n    countryCodes = pathReferenceGeography+fileNameGeoMaster\n    geoInfo = pathReferenceGeography+fileNameGeoInfo\n    \n    dfCountryCodes = pd.read_csv(countryCodes, dtype=\'object\')\n    dfGeoInfo = pd.read_csv(geoInfo, dtype=\'object\')\n    \n    dfCountryCodes[\'alpha2\'] = dfCountryCodes[\'alpha2\'].str.upper()\n    dfCountryCodes[\'alpha3\'] = dfCountryCodes[\'alpha3\'].str.upper()\n    \n    dfCountryCodes.rename(columns={\'alpha2\': \'countryId\'}, inplace=True)\n    dfCountryCodes.rename(columns={\'alpha3\': \'countryIdLong\'}, inplace=True)\n    dfCountryCodes.rename(columns={\'name\': \'countryRef\'}, inplace=True)\n    \n    dfCountryCodes = dfCountryCodes.drop(columns=[\'id\'])\n    \n    dfMerged = dfCountryCodes.merge(dfGeoInfo, how=\'outer\', on=\'countryId\')\n    \n    # Rearrange columns\n    dfMerged = dfMerged[dfMerged.columns[[3,4,0,1,2,5,6,7,8]]]\n    \n    # Replace with NaN\n    dfMerged = dfMerged.replace(\'\', np.nan)\n    dfMerged = dfMerged.replace(\'(not set)\', np.nan)\n    \n    # Sort columns\n    sortOrder = True\n    dfSorted = dfMerged.sort_values(list(dfMerged.columns.values), ascending=sortOrder)\n    \n    \n    # Write to and overwrite file\n    sep = \',\'\n    csvFilePath = "output/csv/reference/"\n    if not os.path.exists(csvFilePath):\n                os.makedirs(csvFilePath)\n    fileName = "geographyReference.csv"\n    fileNameFull = csvFilePath + fileName\n    \n    if not dfSorted.empty:\n        dfSorted.to_csv(fileNameFull, mode=\'w\', index=0, sep=\',\')        \n    \n    # -------------------------------------------\n    \n    ## Table with all dates, not just those with fetched data, as reference\n    \n    # Date range\n    dates = pd.date_range(\'2022-01-01\', \'2030-01-01\', freq="H",inclusive=\'left\')\n    \n    # List to Dataframe\n    dateReference = pd.DataFrame(dates, columns=[\'dateHour\'])\n    \n    # Running function sortOut_dateHour\n    dateReference = dates_funcs.sortOut_dateHour(dateReference)\n    \n    csvFilePath = "output/csv/reference/"\n    fileName = "dateReference.csv"\n    fileNameFull = csvFilePath + fileName\n    \n    # If file does not alreay exist, write to file\n    if not os.path.isfile(fileNameFull):\n        dateReference.to_csv(fileNameFull, mode=\'w\', index=0, sep=\',\')\n    \n    # ---------------------------------------------------------------------------------------------------------------------\n    \n    ## Max time to latestDatefile\n    \n    # Find largest date in dataframes, looking at df eventName, which should have all dates. (?)\n    maxTimestamp = dfs_merged_fillNA[14]["dateFull"].max()\n    maxTimestampString = datetime.datetime.strftime(maxTimestamp,\'%Y-%m-%d\')\n    maxDate = datetime.datetime.strptime(maxTimestampString,\'%Y-%m-%d\').date()\n\n    print("\\n")\n    print("Fetched data until "+maxTimestampString)\n    \n    # If date is larger than priviously max date.\n    if maxDate > maxSavedDateDT:\n    # Write largest date to file\n        f = open("latestDate.txt", \'w\')\n        f.write(maxTimestampString)\n        f.close()\n    \n    print("Written to .csv files")\n    print("\\n")\n\nelse:\n    print("ERROR")\n\n# ---------------------------------------------------------------------------------------------------------------------\n\n# Footnote 1\nprint(\n\'\'\'The following License applies to gapandas4 ONLY:\n\nMIT License\nCopyright (c) 2018\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \\"Software\\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \\"AS IS\\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n\'\'\'\n)\n# Now to print to log when script completed\n\nnowDT = datetime.datetime.now()\nnow = datetime.datetime.strftime(nowDT,\'%Y-%m-%d_%H:%M:%S\')\nprint("Script finished: "+now)\nprint("\\n")\nprint("--------------------------------------------------------------------------------")\nprint("\\n")\n')


# In[ ]:




